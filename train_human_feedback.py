#!/usr/bin/env python3
"""
ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ
ì‚¬ëŒì´ ëª¨ë¸ì˜ ì£¼í–‰ì„ í‰ê°€í•˜ì—¬ ê°•í™”í•™ìŠµ ì§„í–‰

ì‚¬ìš©ë²•:
    python train_human_feedback.py --model ppo_model.pth --port /dev/ttyACM0
"""

import argparse
import numpy as np
import torch
import time
import sys
import os
from datetime import datetime
from collections import deque

# í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„í¬íŠ¸
from rc_car_env import RCCarEnv
from ppo_agent import PPOAgent
from rc_car_controller import RCCarController

# TensorBoard ì§€ì›
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False


class HumanFeedbackTrainer:
    """
    ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ í´ë˜ìŠ¤
    """
    
    def __init__(
        self,
        agent: PPOAgent,
        port: str = '/dev/ttyACM0',
        max_steps: int = 1000,
        action_delay: float = 0.1
    ):
        """
        Args:
            agent: PPO ì—ì´ì „íŠ¸
            port: ì‹œë¦¬ì–¼ í¬íŠ¸
            max_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜
            action_delay: ì•¡ì…˜ ê°„ ì§€ì—° ì‹œê°„
        """
        self.agent = agent
        self.port = port
        self.max_steps = max_steps
        self.action_delay = action_delay
        
        # ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ ë° ì œì–´ê¸°
        self.env = RCCarEnv(
            max_steps=max_steps,
            use_extended_actions=True,
            use_discrete_actions=True
        )
        
        self.controller = RCCarController(port=port, delay=action_delay)
        
        # í‰ê°€ ë°ì´í„° ì €ì¥ì†Œ
        self.evaluation_buffer = {
            'states': [],
            'actions': [],
            'rewards': [],  # ì‚¬ëŒì´ ì¤€ í‰ê°€ ì ìˆ˜
            'dones': [],
            'log_probs': [],
            'values': [],
            'latents': []
        }
    
    def run_episode_for_evaluation(self, verbose: bool = True):
        """
        í‰ê°€ë¥¼ ìœ„í•œ ì—í”¼ì†Œë“œ ì‹¤í–‰
        
        Returns:
            episode_data: ì—í”¼ì†Œë“œ ë°ì´í„°
        """
        # í™˜ê²½ ë¦¬ì…‹
        state = self.env.reset()
        
        # TRM-PPO: ì ì¬ ìƒíƒœ ì´ˆê¸°í™”
        if hasattr(self.agent, 'use_recurrent') and self.agent.use_recurrent:
            self.agent.reset_carry()
        
        episode_data = {
            'states': [],
            'actions': [],
            'log_probs': [],
            'values': [],
            'latents': []
        }
        
        if verbose:
            print("\n" + "="*60)
            print("ëª¨ë¸ ì£¼í–‰ í‰ê°€ ì¤‘...")
            print("="*60)
        
        try:
            for step in range(self.max_steps):
                # ìƒíƒœ ì •ê·œí™”
                state_normalized = state.astype(np.float32) / 255.0
                state_tensor = torch.FloatTensor(state_normalized).unsqueeze(0).to(self.agent.device)
                
                # ì•¡ì…˜ ì„ íƒ
                if hasattr(self.agent, 'use_recurrent') and self.agent.use_recurrent:
                    action, log_prob, value, latent_np = self.agent.get_action_with_carry(
                        state_tensor, deterministic=False
                    )
                else:
                    action, log_prob, value = self.agent.actor_critic.get_action(state_tensor)
                    latent_np = None
                
                # ì•¡ì…˜ ë³€í™˜
                action_np = int(action.squeeze(0).cpu().detach().numpy().item())
                log_prob_np = log_prob.squeeze(0).cpu().item() if log_prob is not None else 0.0
                value_np = value.squeeze(0).cpu().item()
                
                # ì‹¤ì œ í•˜ë“œì›¨ì–´ ì œì–´
                self.controller.execute_discrete_action(action_np)
                
                # í™˜ê²½ ìŠ¤í…
                next_state, _, done, info = self.env.step(action_np)
                
                # ë°ì´í„° ì €ì¥
                episode_data['states'].append(state_normalized.copy())
                episode_data['actions'].append(action_np)
                episode_data['log_probs'].append(log_prob_np)
                episode_data['values'].append(value_np)
                if latent_np is not None:
                    episode_data['latents'].append(latent_np)
                
                if verbose and (step + 1) % 50 == 0:
                    action_name = {
                        0: "Stop", 1: "Right+Gas", 2: "Left+Gas",
                        3: "Gas", 4: "Brake"
                    }.get(action_np, f"Action {action_np}")
                    print(f"[Step {step+1:4d}] Action: {action_name}")
                
                time.sleep(self.action_delay)
                
                if done:
                    break
                
                state = next_state
        
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì •ì§€
        self.controller.stop()
        
        return episode_data
    
    def get_human_feedback(self, episode_data: dict):
        """
        ì‚¬ëŒìœ¼ë¡œë¶€í„° í‰ê°€ ì ìˆ˜ ë°›ê¸°
        
        Args:
            episode_data: ì—í”¼ì†Œë“œ ë°ì´í„°
        
        Returns:
            feedback_score: í‰ê°€ ì ìˆ˜ (0.0 ~ 1.0)
        """
        print("\n" + "="*60)
        print("ì£¼í–‰ í‰ê°€")
        print("="*60)
        print("ì—í”¼ì†Œë“œ ê¸¸ì´:", len(episode_data['states']), "ìŠ¤í…")
        print("\ní‰ê°€ ì ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0.0 ~ 1.0):")
        print("  0.0: ë§¤ìš° ë‚˜ì¨")
        print("  0.5: ë³´í†µ")
        print("  1.0: ë§¤ìš° ì¢‹ìŒ")
        print("="*60)
        
        while True:
            try:
                score = float(input("ì ìˆ˜ (0.0-1.0): "))
                if 0.0 <= score <= 1.0:
                    return score
                else:
                    print("âš ï¸  0.0ê³¼ 1.0 ì‚¬ì´ì˜ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”.")
            except ValueError:
                print("âš ï¸  ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            except KeyboardInterrupt:
                print("\nâš ï¸  í‰ê°€ ì·¨ì†Œ")
                return None
    
    def train_with_feedback(
        self,
        num_episodes: int = 10,
        feedback_weight: float = 1.0,
        save_path: str = 'ppo_model_feedback.pth',
        log_dir: str = 'runs'
    ):
        """
        ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ
        
        Args:
            num_episodes: í‰ê°€í•  ì—í”¼ì†Œë“œ ìˆ˜
            feedback_weight: í”¼ë“œë°± ê°€ì¤‘ì¹˜
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
        """
        print(f"\n{'='*60}")
        print("ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ì—í”¼ì†Œë“œ ìˆ˜: {num_episodes}")
        print(f"í”¼ë“œë°± ê°€ì¤‘ì¹˜: {feedback_weight}")
        print(f"{'='*60}\n")
        
        # TensorBoard ì„¤ì •
        writer = None
        if HAS_TENSORBOARD:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_path = os.path.join(log_dir, f"human_feedback_{timestamp}")
            writer = SummaryWriter(log_path)
            print(f"ğŸ“Š TensorBoard ë¡œê·¸: {log_path}\n")
        
        episode_scores = []
        
        for episode in range(num_episodes):
            print(f"\n>>> ì—í”¼ì†Œë“œ {episode + 1}/{num_episodes} <<<")
            
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            episode_data = self.run_episode_for_evaluation(verbose=True)
            
            # ì‚¬ëŒ í‰ê°€ ë°›ê¸°
            feedback_score = self.get_human_feedback(episode_data)
            
            if feedback_score is None:
                print("âš ï¸  í‰ê°€ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì—í”¼ì†Œë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            episode_scores.append(feedback_score)
            
            # í”¼ë“œë°±ì„ ë¦¬ì›Œë“œë¡œ ë³€í™˜ (ì •ê·œí™”)
            # feedback_scoreë¥¼ -1.0 ~ 1.0 ë²”ìœ„ë¡œ ë³€í™˜
            normalized_reward = (feedback_score - 0.5) * 2.0  # 0.0 -> -1.0, 1.0 -> 1.0
            
            # ëª¨ë“  ìŠ¤í…ì— ë™ì¼í•œ í”¼ë“œë°± ë¦¬ì›Œë“œ ì ìš©
            rewards = [normalized_reward * feedback_weight] * len(episode_data['states'])
            dones = [False] * (len(episode_data['states']) - 1) + [True]
            
            # ë²„í¼ì— ì €ì¥
            for i in range(len(episode_data['states'])):
                self.evaluation_buffer['states'].append(episode_data['states'][i])
                self.evaluation_buffer['actions'].append(episode_data['actions'][i])
                self.evaluation_buffer['rewards'].append(rewards[i])
                self.evaluation_buffer['dones'].append(dones[i])
                self.evaluation_buffer['log_probs'].append(episode_data['log_probs'][i])
                self.evaluation_buffer['values'].append(episode_data['values'][i])
                if i < len(episode_data.get('latents', [])):
                    self.evaluation_buffer['latents'].append(episode_data['latents'][i])
            
            # ì¼ì •ëŸ‰ ìŒ“ì´ë©´ ì—…ë°ì´íŠ¸
            if len(self.evaluation_buffer['states']) >= 512:  # ì‘ì€ ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸
                print(f"\nì—…ë°ì´íŠ¸ ì¤‘... (ë²„í¼ í¬ê¸°: {len(self.evaluation_buffer['states'])})")
                loss_info = self._update_from_buffer()
                
                if writer:
                    writer.add_scalar('Train/Loss', loss_info.get('loss', 0), episode)
                    writer.add_scalar('Train/PolicyLoss', loss_info.get('policy_loss', 0), episode)
                    writer.add_scalar('Train/ValueLoss', loss_info.get('value_loss', 0), episode)
                    writer.add_scalar('Feedback/Score', feedback_score, episode)
                    writer.add_scalar('Feedback/AvgScore', np.mean(episode_scores), episode)
                
                # ë²„í¼ ì´ˆê¸°í™”
                self.evaluation_buffer = {
                    'states': [],
                    'actions': [],
                    'rewards': [],
                    'dones': [],
                    'log_probs': [],
                    'values': [],
                    'latents': []
                }
            
            # ëª¨ë¸ ì €ì¥
            if (episode + 1) % 5 == 0:
                self.agent.save(save_path)
                print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {save_path}")
            
            # í†µê³„ ì¶œë ¥
            print(f"\ní‰ê°€ ì ìˆ˜: {feedback_score:.3f}")
            print(f"í‰ê·  ì ìˆ˜: {np.mean(episode_scores):.3f} Â± {np.std(episode_scores):.3f}")
            
            # ë‹¤ìŒ ì—í”¼ì†Œë“œ ì¤€ë¹„
            if episode < num_episodes - 1:
                print("\në‹¤ìŒ ì—í”¼ì†Œë“œë¥¼ ì¤€ë¹„í•˜ì„¸ìš”... (3ì´ˆ í›„ ì‹œì‘)")
                time.sleep(3)
        
        # ìµœì¢… ì—…ë°ì´íŠ¸
        if len(self.evaluation_buffer['states']) > 0:
            print(f"\nìµœì¢… ì—…ë°ì´íŠ¸ ì¤‘... (ë²„í¼ í¬ê¸°: {len(self.evaluation_buffer['states'])})")
            self._update_from_buffer()
        
        # ìµœì¢… ì €ì¥
        self.agent.save(save_path)
        
        if writer:
            writer.close()
        
        print(f"\n{'='*60}")
        print("ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"í‰ê·  í‰ê°€ ì ìˆ˜: {np.mean(episode_scores):.3f} Â± {np.std(episode_scores):.3f}")
        print(f"ìµœê³  ì ìˆ˜: {np.max(episode_scores):.3f}")
        print(f"ìµœì € ì ìˆ˜: {np.min(episode_scores):.3f}")
        print(f"ëª¨ë¸ ì €ì¥: {save_path}")
        print(f"{'='*60}\n")
    
    def _update_from_buffer(self):
        """ë²„í¼ ë°ì´í„°ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        if len(self.evaluation_buffer['states']) == 0:
            return {}
        
        # ë²„í¼ë¥¼ í…ì„œë¡œ ë³€í™˜
        states = torch.FloatTensor(np.array(self.evaluation_buffer['states'])).to(self.agent.device)
        actions = torch.LongTensor(np.array(self.evaluation_buffer['actions'])).to(self.agent.device)
        old_log_probs = torch.FloatTensor(np.array(self.evaluation_buffer['log_probs'])).to(self.agent.device)
        old_values = torch.FloatTensor(np.array(self.evaluation_buffer['values'])).to(self.agent.device)
        rewards = np.array(self.evaluation_buffer['rewards'])
        dones = np.array(self.evaluation_buffer['dones'])
        
        # ì ì¬ ìƒíƒœ í…ì„œ
        latents = None
        if hasattr(self.agent, 'use_recurrent') and self.agent.use_recurrent:
            if len(self.evaluation_buffer['latents']) > 0:
                latents = torch.FloatTensor(np.array(self.evaluation_buffer['latents'])).to(self.agent.device)
                if latents.dim() == 3 and latents.shape[1] == 1:
                    latents = latents.squeeze(1)
        
        # ë¦¬í„´ ê³„ì‚° (Monte Carlo)
        returns = []
        running_return = 0
        for step in reversed(range(len(rewards))):
            if dones[step]:
                running_return = 0
            running_return = rewards[step] + self.agent.gamma * running_return
            returns.insert(0, running_return)
        
        advantages = [r - v for r, v in zip(returns, old_values.cpu().numpy())]
        advantages = torch.FloatTensor(advantages).to(self.agent.device)
        returns = torch.FloatTensor(returns).to(self.agent.device)
        
        # ì •ê·œí™”
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ ë²„ì „)
        epochs = 3
        total_loss = 0
        
        for epoch in range(epochs):
            # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í‰ê°€ (ì´ì‚° ì•¡ì…˜)
            if hasattr(self.agent, 'use_recurrent') and self.agent.use_recurrent:
                log_probs, values, entropy = self.agent.actor_critic.evaluate(
                    states, actions, latent=latents, n_cycles=self.agent.n_cycles
                )
            else:
                log_probs, values, entropy = self.agent.actor_critic.evaluate(states, actions)
            
            # ì •ì±… ë¹„ìœ¨
            ratio = torch.exp(log_probs - old_log_probs)
            
            # PPO í´ë¦¬í•‘ ì†ì‹¤
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.agent.clip_epsilon, 1 + self.agent.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤
            value_loss = torch.nn.functional.mse_loss(values.squeeze(-1), returns)
            
            # ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
            entropy_loss = -entropy.mean()
            
            # ì´ ì†ì‹¤
            loss = policy_loss + self.agent.value_coef * value_loss + self.agent.entropy_coef * entropy_loss
            
            # ì—­ì „íŒŒ
            self.agent.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
            self.agent.optimizer.step()
            
            total_loss += loss.item()
        
        return {
            'loss': total_loss / epochs,
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.mean().item()
        }
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.env:
            self.env.close()
        if self.controller:
            self.controller.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python train_human_feedback.py --model ppo_model.pth --port /dev/ttyACM0 --episodes 10
        """
    )
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument('--model', type=str, required=True,
                        help='í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--port', type=str, default='/dev/ttyACM0',
                        help='ì‹œë¦¬ì–¼ í¬íŠ¸ (ê¸°ë³¸: /dev/ttyACM0)')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--episodes', type=int, default=10,
                        help='í‰ê°€í•  ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 10)')
    parser.add_argument('--max-steps', type=int, default=1000,
                        help='ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸: 1000)')
    parser.add_argument('--delay', type=float, default=0.1,
                        help='ì•¡ì…˜ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 0.1)')
    parser.add_argument('--feedback-weight', type=float, default=1.0,
                        help='í”¼ë“œë°± ê°€ì¤‘ì¹˜ (ê¸°ë³¸: 1.0)')
    
    # ì €ì¥ ì„¤ì •
    parser.add_argument('--save', type=str, default='ppo_model_feedback.pth',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ppo_model_feedback.pth)')
    
    # ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°
    parser.add_argument('--hidden-dim', type=int, default=256,
                        help='íˆë“  ë ˆì´ì–´ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--latent-dim', type=int, default=256,
                        help='TRM-PPO ì ì¬ ìƒíƒœ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--n-cycles', type=int, default=4,
                        help='TRM-PPO ì¬ê·€ ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 4)')
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None,
                        help='ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™ ì„ íƒ)')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = PPOAgent(
        state_dim=256,
        action_dim=5,  # ì´ì‚° ì•¡ì…˜ë§Œ
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=True,
        device=device,
        discrete_action=True,  # ì´ì‚° ì•¡ì…˜ë§Œ
        num_discrete_actions=5,
        use_recurrent=True
    )
    
    # ëª¨ë¸ ë¡œë“œ
    if os.path.exists(args.model):
        agent.load(args.model)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {args.model}")
    else:
        print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        print("ëœë¤ ì •ì±…ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = HumanFeedbackTrainer(
        agent=agent,
        port=args.port,
        max_steps=args.max_steps,
        action_delay=args.delay
    )
    
    try:
        # ì‚¬ëŒ í‰ê°€ ê¸°ë°˜ ê°•í™”í•™ìŠµ
        trainer.train_with_feedback(
            num_episodes=args.episodes,
            feedback_weight=args.feedback_weight,
            save_path=args.save
        )
    finally:
        trainer.close()


if __name__ == "__main__":
    main()

