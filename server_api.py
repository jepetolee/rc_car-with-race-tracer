#!/usr/bin/env python3
"""
ì„œë²„ API: ë¼ì¦ˆë² ë¦¬ íŒŒì´ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë°›ì•„ í•™ìŠµ ìˆ˜í–‰
Flask ê¸°ë°˜ REST API ì„œë²„
"""

import os
import pickle
import argparse
import uuid
import numpy as np
import traceback
from datetime import datetime
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import torch

# í•™ìŠµ ê´€ë ¨ ì„í¬íŠ¸
from train_with_teacher_forcing import TeacherForcingTrainer
from train_ppo import train as train_dqn_env
from train_imitation_rl import ImitationRLTrainer
from ppo_agent import DQNAgent

app = Flask(__name__)
CORS(app)  # CORS í—ˆìš© (ë¼ì¦ˆë² ë¦¬ íŒŒì´ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)

# ì „ì—­ ë³€ìˆ˜
UPLOAD_FOLDER = 'uploaded_data'
MODEL_FOLDER = 'trained_models'
TEMP_FOLDER = 'temp_uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(MODEL_FOLDER, exist_ok=True)
os.makedirs(TEMP_FOLDER, exist_ok=True)

# ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ ì„¸ì…˜ ê´€ë¦¬
upload_sessions = {}  # {session_id: {'filename': str, 'file_size': int, 'chunks': {}, 'total_chunks': int}}


@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat()
    })


@app.route('/api/upload_data/init', methods=['POST'])
def upload_data_init():
    """
    ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ ì´ˆê¸°í™”
    
    ìš”ì²­:
    - filename: íŒŒì¼ëª…
    - file_size: íŒŒì¼ í¬ê¸°
    - chunk_size: ì²­í¬ í¬ê¸°
    - total_chunks: ì´ ì²­í¬ ìˆ˜
    
    ì‘ë‹µ:
    - session_id: ì„¸ì…˜ ID
    """
    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Type must be application/json'}), 400
        
        data = request.json
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        filename = data.get('filename')
        file_size = data.get('file_size')
        chunk_size = data.get('chunk_size')
        total_chunks = data.get('total_chunks')
        
        if not all([filename, file_size is not None, chunk_size is not None, total_chunks is not None]):
            return jsonify({
                'error': 'Missing required fields',
                'received': {
                    'filename': filename,
                    'file_size': file_size,
                    'chunk_size': chunk_size,
                    'total_chunks': total_chunks
                }
            }), 400
        
        # ì„¸ì…˜ ID ìƒì„±
        session_id = str(uuid.uuid4())
        
        # ì„¸ì…˜ ì •ë³´ ì €ì¥
        upload_sessions[session_id] = {
            'filename': filename,
            'file_size': file_size,
            'chunk_size': chunk_size,
            'total_chunks': total_chunks,
            'chunks': {},  # {chunk_index: chunk_path}
            'received_chunks': set()
        }
        
        print(f"ğŸ“¥ ì—…ë¡œë“œ ì„¸ì…˜ ì‹œì‘: {session_id} ({filename}, {file_size / (1024*1024):.2f} MB)")
        
        return jsonify({
            'status': 'success',
            'session_id': session_id
        })
    
    except Exception as e:
        import traceback
        error_msg = str(e)
        traceback.print_exc()
        return jsonify({'error': f'Server error: {error_msg}'}), 500


@app.route('/api/upload_data/chunk', methods=['POST'])
def upload_data_chunk():
    """
    ì²­í¬ ì—…ë¡œë“œ
    
    ìš”ì²­:
    - session_id: ì„¸ì…˜ ID
    - chunk_index: ì²­í¬ ì¸ë±ìŠ¤
    - chunk: ì²­í¬ ë°ì´í„°
    
    ì‘ë‹µ:
    - status: success
    - received_chunks: ë°›ì€ ì²­í¬ ìˆ˜
    """
    try:
        if 'chunk' not in request.files:
            return jsonify({'error': 'No chunk provided'}), 400
        
        session_id = request.form.get('session_id')
        chunk_index = int(request.form.get('chunk_index'))
        
        if session_id not in upload_sessions:
            return jsonify({'error': 'Invalid session_id'}), 400
        
        session = upload_sessions[session_id]
        chunk = request.files['chunk']
        
        # ì²­í¬ ì €ì¥
        chunk_path = os.path.join(TEMP_FOLDER, f"{session_id}_chunk_{chunk_index}")
        chunk.save(chunk_path)
        
        session['chunks'][chunk_index] = chunk_path
        session['received_chunks'].add(chunk_index)
        
        received = len(session['received_chunks'])
        total = session['total_chunks']
        
        return jsonify({
            'status': 'success',
            'received_chunks': received,
            'total_chunks': total
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/upload_data/finish', methods=['POST'])
def upload_data_finish():
    """
    ì—…ë¡œë“œ ì™„ë£Œ ë° íŒŒì¼ ì¡°ë¦½
    
    ìš”ì²­:
    - session_id: ì„¸ì…˜ ID
    
    ì‘ë‹µ:
    - status: success
    - file_path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    try:
        data = request.json
        session_id = data.get('session_id')
        
        if session_id not in upload_sessions:
            return jsonify({'error': 'Invalid session_id'}), 400
        
        session = upload_sessions[session_id]
        received = len(session['received_chunks'])
        total = session['total_chunks']
        
        if received != total:
            return jsonify({'error': f'Missing chunks: {received}/{total}'}), 400
        
        # íŒŒì¼ ì¡°ë¦½
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"demos_{timestamp}.pkl"
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        
        print(f"ğŸ”¨ íŒŒì¼ ì¡°ë¦½ ì¤‘: {session['filename']} â†’ {filename}")
        
        with open(filepath, 'wb') as f:
            for i in range(total):
                chunk_path = session['chunks'][i]
                with open(chunk_path, 'rb') as chunk_file:
                    f.write(chunk_file.read())
                # ì„ì‹œ ì²­í¬ íŒŒì¼ ì‚­ì œ
                os.remove(chunk_path)
        
        # ì„¸ì…˜ ì‚­ì œ
        del upload_sessions[session_id]
        
        # ë°ì´í„° ê²€ì¦
        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
            num_episodes = len(data.get('demonstrations', []))
            total_steps = sum(len(ep.get('states', [])) for ep in data.get('demonstrations', []))
        except Exception as e:
            return jsonify({'error': f'Invalid pickle file: {str(e)}'}), 400
        
        print(f"âœ… íŒŒì¼ ì¡°ë¦½ ì™„ë£Œ: {filename} ({num_episodes} ì—í”¼ì†Œë“œ, {total_steps} ìŠ¤í…)")
        
        return jsonify({
            'status': 'success',
            'file_path': filepath,
            'filename': filename,
            'num_episodes': num_episodes,
            'total_steps': total_steps
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/upload_data', methods=['POST'])
def upload_data():
    """
    ë¼ì¦ˆë² ë¦¬ íŒŒì´ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„° ì—…ë¡œë“œ
    
    ìš”ì²­:
    - Content-Type: multipart/form-data
    - file: pickle íŒŒì¼ (human_demos.pkl)
    
    ì‘ë‹µ:
    - status: success/error
    - file_path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file provided'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file.seek(0, os.SEEK_END)
        file_size = file.tell()
        file_size_mb = file_size / (1024 * 1024)
        file.seek(0)  # ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ
        
        print(f"ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘: {file.filename} ({file_size_mb:.2f} MB)")
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"demos_{timestamp}.pkl"
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì €ì¥ (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì›)
        chunk_size = 1024 * 1024  # 1MB ì²­í¬
        with open(filepath, 'wb') as f:
            while True:
                chunk = file.read(chunk_size)
                if not chunk:
                    break
                f.write(chunk)
        
        # ë°ì´í„° ê²€ì¦
        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
            num_episodes = len(data.get('demonstrations', []))
            total_steps = sum(len(ep.get('states', [])) for ep in data.get('demonstrations', []))
        except Exception as e:
            return jsonify({'error': f'Invalid pickle file: {str(e)}'}), 400
        
        return jsonify({
            'status': 'success',
            'file_path': filepath,
            'filename': filename,
            'num_episodes': num_episodes,
            'total_steps': total_steps
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# íŒ¨ì¹˜ ì—…ë¡œë“œë¥¼ ìœ„í•œ ì„ì‹œ ì €ì¥ì†Œ
PATCH_STORAGE = {}  # {session_id: {'patches': [...], 'metadata': {...}}}


@app.route('/api/upload_patch', methods=['POST'])
def upload_patch():
    """
    íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì—…ë¡œë“œ (16x16 ì´ë¯¸ì§€ íŒ¨ì¹˜)
    
    ìš”ì²­:
    - session_id: ì„¸ì…˜ ID (ê°™ì€ ì—…ë¡œë“œ ì„¸ì…˜)
    - patch_index: íŒ¨ì¹˜ ì¸ë±ìŠ¤
    - total_patches: ì´ íŒ¨ì¹˜ ìˆ˜
    - states: ì´ë¯¸ì§€ íŒ¨ì¹˜ ë°°ì—´ (16x16 ì´ë¯¸ì§€ë“¤ì˜ ë¦¬ìŠ¤íŠ¸)
    - actions: ì•¡ì…˜ ë°°ì—´
    - metadata: ë©”íƒ€ë°ì´í„° (ì²« íŒ¨ì¹˜ì—ë§Œ)
    
    ì‘ë‹µ:
    - status: success
    - patch_index: ë°›ì€ íŒ¨ì¹˜ ì¸ë±ìŠ¤
    """
    try:
        data = request.json
        session_id = data.get('session_id')
        patch_index = data.get('patch_index')
        total_patches = data.get('total_patches')
        states = data.get('states')
        actions = data.get('actions')
        metadata = data.get('metadata')
        
        if not session_id:
            return jsonify({'error': 'session_id required'}), 400
        
        # ì„¸ì…˜ ì´ˆê¸°í™”
        if session_id not in PATCH_STORAGE:
            PATCH_STORAGE[session_id] = {
                'patches': [],
                'metadata': None,
                'total_patches': total_patches
            }
        
        # íŒ¨ì¹˜ ì €ì¥
        PATCH_STORAGE[session_id]['patches'].append({
            'index': patch_index,
            'states': states,
            'actions': actions
        })
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥ (ì²« íŒ¨ì¹˜)
        if metadata and PATCH_STORAGE[session_id]['metadata'] is None:
            PATCH_STORAGE[session_id]['metadata'] = metadata
        
        return jsonify({
            'status': 'success',
            'patch_index': patch_index,
            'received_patches': len(PATCH_STORAGE[session_id]['patches']),
            'total_patches': total_patches
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/merge_patches', methods=['POST'])
def merge_patches():
    """
    ì—…ë¡œë“œëœ íŒ¨ì¹˜ë“¤ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©
    
    ìš”ì²­:
    - session_id: ì„¸ì…˜ ID
    
    ì‘ë‹µ:
    - status: success
    - file_path: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    - total_samples: ì´ ìƒ˜í”Œ ìˆ˜
    """
    try:
        data = request.json
        session_id = data.get('session_id')
        
        if not session_id or session_id not in PATCH_STORAGE:
            return jsonify({'error': 'Invalid session_id'}), 400
        
        session_data = PATCH_STORAGE[session_id]
        patches = session_data['patches']
        metadata = session_data['metadata']
        
        if len(patches) == 0:
            return jsonify({'error': 'No patches found'}), 400
        
        # íŒ¨ì¹˜ë“¤ì„ ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬
        patches.sort(key=lambda x: x['index'])
        
        # ëª¨ë“  íŒ¨ì¹˜ ë³‘í•©
        all_states = []
        all_actions = []
        
        for patch in patches:
            all_states.extend(patch['states'])
            all_actions.extend(patch['actions'])
        
        # ì—í”¼ì†Œë“œ í˜•íƒœë¡œ ë³€í™˜ (ë‹¨ì¼ ì—í”¼ì†Œë“œë¡œ)
        demonstrations = [{
            'states': all_states,
            'actions': all_actions,
            'rewards': [0.0] * len(all_states),  # ë¦¬ì›Œë“œëŠ” 0ìœ¼ë¡œ ì„¤ì •
            'dones': [False] * (len(all_states) - 1) + [True],
            'timestamps': []
        }]
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if metadata:
            metadata['num_episodes'] = 1
            metadata['total_steps'] = len(all_states)
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"demos_patched_{timestamp}.pkl"
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        
        with open(filepath, 'wb') as f:
            pickle.dump({
                'metadata': metadata or {},
                'demonstrations': demonstrations
            }, f)
        
        # ì„¸ì…˜ ë°ì´í„° ì‚­ì œ
        del PATCH_STORAGE[session_id]
        
        return jsonify({
            'status': 'success',
            'file_path': filepath,
            'filename': filename,
            'total_samples': len(all_states),
            'num_patches': len(patches)
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/train/supervised', methods=['POST'])
def train_supervised():
    """
    Supervised Learning (Teacher Forcing) í•™ìŠµ ì‹œì‘
    
    ìš”ì²­:
    - file_path: ì—…ë¡œë“œëœ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    - model_path: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒ, ì—†ìœ¼ë©´ ëœë¤ ì´ˆê¸°í™”)
    - epochs: í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)
    - batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)
    - learning_rate: í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)
    
    ì‘ë‹µ:
    - status: success
    - model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    """
    try:
        data = request.json
        file_path = data.get('file_path')
        model_path = data.get('model_path')
        epochs = data.get('epochs', 100)
        batch_size = data.get('batch_size', 64)
        learning_rate = data.get('learning_rate', 3e-4)
        
        print(f"ğŸ“š Teacher Forcing í•™ìŠµ ìš”ì²­:")
        print(f"   ë°›ì€ ë°ì´í„°: {data}")
        print(f"   íŒŒì¼: {file_path}")
        print(f"   ì—í­: {epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   í•™ìŠµë¥ : {learning_rate}")
        
        if not file_path:
            return jsonify({'error': 'file_path is required'}), 400
        
        # íŒŒì¼ ê²½ë¡œ í™•ì¸ (ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ)
        if not os.path.isabs(file_path):
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° UPLOAD_FOLDER ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
            file_path = os.path.join(UPLOAD_FOLDER, os.path.basename(file_path))
        
        print(f"   ì‹¤ì œ íŒŒì¼ ê²½ë¡œ: {file_path}")
        print(f"   íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(file_path)}")
        
        if not os.path.exists(file_path):
            available_files = []
            if os.path.exists(UPLOAD_FOLDER):
                available_files = [f for f in os.listdir(UPLOAD_FOLDER) if f.endswith('.pkl')]
            return jsonify({
                'error': f'File not found: {file_path}',
                'upload_folder': UPLOAD_FOLDER,
                'available_files': available_files[:10]
            }), 400
        
        # ë°ì´í„° ë¡œë“œ
        with open(file_path, 'rb') as f:
            demo_data = pickle.load(f)
        
        demonstrations = demo_data.get('demonstrations', [])
        if len(demonstrations) == 0:
            return jsonify({'error': 'No demonstrations found'}), 400
        
        # ìƒíƒœ ì°¨ì› ìë™ ê°ì§€
        state_dim = None
        if len(demonstrations) > 0:
            first_episode = demonstrations[0]
            states = first_episode.get('states', [])
            if len(states) > 0:
                first_state = np.array(states[0])
                if len(first_state.shape) == 1:
                    state_dim = first_state.shape[0]
                else:
                    state_dim = first_state.size
                print(f"ğŸ“ ìƒíƒœ ì°¨ì› ìë™ ê°ì§€: {state_dim}")
        
        if state_dim is None:
            return jsonify({'error': 'Could not determine state_dim from demonstrations'}), 400
        
        first_episode = demonstrations[0]
        actions = first_episode.get('actions', [])
        if len(actions) > 0:
            action_dim = int(np.max(actions)) + 1
        else:
            return jsonify({'error': 'Could not determine action_dim from demonstrations'}), 400
        print(f"ğŸ“ ì•¡ì…˜ ì°¨ì›: {action_dim}")
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        agent = DQNAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            device=device,
            hidden_dim=256,
            latent_dim=256,
        )
        loaded_model_path = None
        if model_path and os.path.exists(model_path):
            try:
                agent.load(model_path, strict=False)
                loaded_model_path = model_path
                print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            except Exception as e:
                print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ëœë¤ ì´ˆê¸°í™”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                model_path = None
        else:
            if model_path:
                print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            model_path = None
        
        trainer = TeacherForcingTrainer(agent, demonstrations, device=device, lr=learning_rate)
        model_path = os.path.join(MODEL_FOLDER, f"pretrained_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth")
        
        trainer.pretrain(
            epochs=epochs,
            batch_size=batch_size,
            save_path=model_path,
            verbose=True
        )
        
        return jsonify({
            'status': 'success',
            'model_path': model_path,
            'epochs': epochs,
            'state_dim': state_dim,
            'loaded_model': loaded_model_path
        })
    
    except Exception as e:
        import traceback
        error_msg = str(e)
        error_trace = traceback.format_exc()
        print(f"âŒ Teacher Forcing í•™ìŠµ ì‹¤íŒ¨:")
        print(error_trace)
        return jsonify({
            'error': error_msg,
            'traceback': error_trace
        }), 500


@app.route('/api/train/imitation_rl', methods=['POST'])
def train_imitation_rl_api():
    """
    Imitation Learning via Reinforcement Learning í•™ìŠµ ì‹œì‘
    
    ìš”ì²­:
    - file_path: ì—…ë¡œë“œëœ ë°ëª¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    - model_path: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒ)
    - epochs: í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)
    - batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)
    - learning_rate: í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)
    
    ì‘ë‹µ:
    - status: success
    - model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    - final_match_rate: ìµœì¢… ì¼ì¹˜ìœ¨
    """
    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Type must be application/json'}), 400
        
        data = request.json
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        file_path = data.get('file_path')
        model_path = data.get('model_path')
        epochs = data.get('epochs', 100)
        batch_size = data.get('batch_size', 64)
        learning_rate = data.get('learning_rate', 3e-4)
        
        print(f"ğŸ“š Imitation RL í•™ìŠµ ìš”ì²­:")
        print(f"   ë°›ì€ ë°ì´í„°: {data}")
        print(f"   íŒŒì¼: {file_path}")
        print(f"   ì—í­: {epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   í•™ìŠµë¥ : {learning_rate}")
        
        if not file_path:
            return jsonify({
                'error': 'file_path is required',
                'received_data': data
            }), 400
        
        # íŒŒì¼ ê²½ë¡œ í™•ì¸ (ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ)
        if not os.path.isabs(file_path):
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° UPLOAD_FOLDER ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
            file_path = os.path.join(UPLOAD_FOLDER, os.path.basename(file_path))
        
        print(f"   ì‹¤ì œ íŒŒì¼ ê²½ë¡œ: {file_path}")
        print(f"   íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(file_path)}")
        
        if not os.path.exists(file_path):
            # íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ê°€ëŠ¥í•œ íŒŒì¼ ëª©ë¡ í‘œì‹œ
            available_files = []
            if os.path.exists(UPLOAD_FOLDER):
                available_files = [f for f in os.listdir(UPLOAD_FOLDER) if f.endswith('.pkl')]
            return jsonify({
                'error': f'File not found: {file_path}',
                'upload_folder': UPLOAD_FOLDER,
                'available_files': available_files[:10]  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            }), 400
        
        # model_path ì²˜ë¦¬ (ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì—¬ëŸ¬ ìœ„ì¹˜ í™•ì¸)
        if model_path:
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í”„ë¡œì íŠ¸ ë£¨íŠ¸, MODEL_FOLDER, í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸
            if not os.path.isabs(model_path):
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ í™•ì¸
                possible_paths = [
                    model_path,  # í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€
                    os.path.join(MODEL_FOLDER, model_path),  # MODEL_FOLDER ê¸°ì¤€
                    os.path.join(MODEL_FOLDER, os.path.basename(model_path)),  # íŒŒì¼ëª…ë§Œ ì‚¬ìš©
                    os.path.join('.', model_path),  # í˜„ì¬ ë””ë ‰í† ë¦¬ ëª…ì‹œ
                ]
                
                found = False
                for candidate in possible_paths:
                    if os.path.exists(candidate):
                        model_path = candidate
                        found = True
                        break
                
                if not found:
                    print(f"âš ï¸  ì§€ì •ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                    print(f"   ì‹œë„í•œ ê²½ë¡œë“¤: {possible_paths}")
                    model_path = None
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"   ë””ë°”ì´ìŠ¤: {device}")
        trainer = ImitationRLTrainer(
            demos_path=file_path,
            model_path=model_path if model_path and os.path.exists(model_path) else None,
            device=device,
            learning_rate=learning_rate,
            batch_size=batch_size
        )
        updates_per_epoch = data.get('updates_per_epoch', 1000)
        
        try:
            print("ğŸš€ í•™ìŠµ ì‹œì‘...")
            model_path = trainer.train(
                epochs=epochs,
                updates_per_epoch=updates_per_epoch
            )
            print(f"âœ… í•™ìŠµ ì™„ë£Œ: {model_path}")
        except Exception as e:
            import traceback
            error_msg = f"í•™ìŠµ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            traceback.print_exc()
            return jsonify({'error': error_msg}), 500
        
        # ìµœì¢… í‰ê°€
        try:
            final_match_rate = trainer.evaluate()
            print(f"ğŸ“Š ìµœì¢… ì¼ì¹˜ìœ¨: {final_match_rate:.2%}")
        except Exception as e:
            print(f"âš ï¸  í‰ê°€ ì‹¤íŒ¨: {e}")
            final_match_rate = 0.0
        
        return jsonify({
            'status': 'success',
            'model_path': model_path,
            'final_match_rate': float(final_match_rate),
            'epochs': epochs
        })
    
    except Exception as e:
        import traceback
        error_msg = f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        return jsonify({'error': error_msg}), 500


@app.route('/api/train/ppo', methods=['POST'])
def train_ppo_api():
    """
    TRM-DQN ê°•í™”í•™ìŠµ ì‹œì‘
    
    ìš”ì²­:
    - env_type: í™˜ê²½ íƒ€ì… (carracing/sim/real)
    - epsilon/í•™ìŠµë¥  ë“± DQN í•˜ì´í¼íŒŒë¼ë¯¸í„°
    
    ì‘ë‹µ:
    - status: success
    - model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    """
    try:
        data = request.json or {}
        config = argparse.Namespace(
            env_type=data.get('env_type', 'carracing'),
            state_dim=data.get('state_dim', 784),
            action_dim=data.get('action_dim', 5),
            hidden_dim=data.get('hidden_dim', 256),
            latent_dim=data.get('latent_dim', 256),
            n_deep_loops=data.get('n_deep_loops', 2),
            n_latent_loops=data.get('n_latent_loops', 2),
            gamma=data.get('gamma', 0.99),
            learning_rate=data.get('learning_rate', 3e-4),
            batch_size=data.get('batch_size', 128),
            replay_buffer=data.get('replay_buffer', 200_000),
            target_update_interval=data.get('target_update_interval', 2000),
            max_grad_norm=data.get('max_grad_norm', 1.0),
            max_episodes=data.get('max_episodes', 1000),
            max_episode_steps=data.get('max_episode_steps', 1000),
            eps_start=data.get('eps_start', 1.0),
            eps_end=data.get('eps_end', 0.05),
            eps_decay=data.get('eps_decay', 300_000),
            save_dir=MODEL_FOLDER,
            save_interval=data.get('save_interval', 50),
            use_tensorboard=False,
        )
        train_dqn_env(config)
        latest_models = sorted(
            [f for f in os.listdir(MODEL_FOLDER) if f.endswith('.pth')]
        )
        model_path = os.path.join(MODEL_FOLDER, latest_models[-1]) if latest_models else None
        return jsonify({
            'status': 'success',
            'model_path': model_path
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/model/latest', methods=['GET'])
def get_latest_model():
    """
    ìµœì‹  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    
    ì‘ë‹µ:
    - ëª¨ë¸ íŒŒì¼ (.pth)
    """
    try:
        # MODEL_FOLDERì—ì„œ ìµœì‹  ëª¨ë¸ ì°¾ê¸°
        model_files = [f for f in os.listdir(MODEL_FOLDER) if f.endswith('.pth')]
        if not model_files:
            return jsonify({'error': 'No model found'}), 404
        
        # ìµœì‹  íŒŒì¼ ì„ íƒ (ì´ë¦„ ê¸°ì¤€)
        latest_model = sorted(model_files)[-1]
        model_path = os.path.join(MODEL_FOLDER, latest_model)
        
        return send_file(
            model_path,
            as_attachment=True,
            download_name=latest_model
        )
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/model/list', methods=['GET'])
def list_models():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    
    ì‘ë‹µ:
    - models: ëª¨ë¸ íŒŒì¼ ëª©ë¡
    """
    try:
        model_files = [f for f in os.listdir(MODEL_FOLDER) if f.endswith('.pth')]
        model_info = []
        
        for model_file in sorted(model_files, reverse=True):
            model_path = os.path.join(MODEL_FOLDER, model_file)
            stat = os.stat(model_path)
            model_info.append({
                'filename': model_file,
                'size': stat.st_size,
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
            })
        
        return jsonify({
            'models': model_info
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/model/download/<filename>', methods=['GET'])
def download_model(filename):
    """
    íŠ¹ì • ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    
    Args:
        filename: ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ íŒŒì¼ëª… (ì˜ˆ: pretrained_20251129_190816.pth)
    
    ì‘ë‹µ:
        - ëª¨ë¸ íŒŒì¼ (.pth)
    """
    try:
        # ë³´ì•ˆ: ê²½ë¡œ íƒìƒ‰ ë°©ì§€
        if '..' in filename or '/' in filename or '\\' in filename:
            return jsonify({'error': 'Invalid filename'}), 400
        
        model_path = os.path.join(MODEL_FOLDER, filename)
        
        if not os.path.exists(model_path):
            return jsonify({'error': f'Model not found: {filename}'}), 404
        
        if not filename.endswith('.pth'):
            return jsonify({'error': 'Invalid file type. Only .pth files are allowed'}), 400
        
        return send_file(
            model_path,
            as_attachment=True,
            download_name=filename
        )
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/inference', methods=['POST'])
def inference():
    """
    ì‹¤ì‹œê°„ ì¶”ë¡  (ì„ íƒ ì‚¬í•­)
    
    ìš”ì²­:
    - state: 256ì°¨ì› ìƒíƒœ ë²¡í„° (16x16 ì´ë¯¸ì§€)
    - model_path: ì‚¬ìš©í•  ëª¨ë¸ ê²½ë¡œ (ì„ íƒ, ê¸°ë³¸: ìµœì‹  ëª¨ë¸)
    
    ì‘ë‹µ:
    - action: ì¶”ë¡ ëœ ì•¡ì…˜ (0-4)
    - q_values: ê° ì•¡ì…˜ì˜ Qê°’
    """
    try:
        data = request.json
        state = data.get('state')
        model_path = data.get('model_path')
        
        if state is None:
            return jsonify({'error': 'No state provided'}), 400
        
        # ëª¨ë¸ ë¡œë“œ
        if model_path is None:
            # ìµœì‹  ëª¨ë¸ ì‚¬ìš©
            model_files = [f for f in os.listdir(MODEL_FOLDER) if f.endswith('.pth')]
            if not model_files:
                return jsonify({'error': 'No model found'}), 404
            model_path = os.path.join(MODEL_FOLDER, sorted(model_files)[-1])
        
        # ì—ì´ì „íŠ¸ ë¡œë“œ ë° ì¶”ë¡ 
        state_dim = len(state)
        agent = DQNAgent(
            state_dim=state_dim,
            action_dim=5,
            device='cpu',
        )
        agent.load(model_path, strict=False)
        
        state_vec = np.array(state, dtype=np.float32)
        action = agent.act_greedy(state_vec)
        q_values = agent.predict(np.expand_dims(state_vec, axis=0))[0]

        return jsonify({
            'action': int(action),
            'q_values': [float(q) for q in q_values]
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# QR ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
QR_DATA_FOLDER = 'qr_dataset'
os.makedirs(QR_DATA_FOLDER, exist_ok=True)
os.makedirs(os.path.join(QR_DATA_FOLDER, 'qr_present'), exist_ok=True)
os.makedirs(os.path.join(QR_DATA_FOLDER, 'qr_absent'), exist_ok=True)


@app.route('/api/upload_qr_data', methods=['POST'])
def upload_qr_data():
    """
    QR ë°ì´í„° ì—…ë¡œë“œ (ì´ë¯¸ì§€ ë°°ì¹˜)
    
    ìš”ì²­:
    - images: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    - labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ (0: QR ì—†ìŒ, 1: QR ìˆìŒ)
    - metadata: ë©”íƒ€ë°ì´í„° (ì„ íƒ)
    
    ì‘ë‹µ:
    - status: success
    - saved_count: ì €ì¥ëœ ì´ë¯¸ì§€ ìˆ˜
    """
    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Type must be application/json'}), 400
        
        data = request.json
        images_base64 = data.get('images', [])
        labels = data.get('labels', [])
        metadata = data.get('metadata', {})
        
        if len(images_base64) != len(labels):
            return jsonify({'error': 'Images and labels count mismatch'}), 400
        
        if len(images_base64) == 0:
            return jsonify({'error': 'No images provided'}), 400
        
        import base64
        import cv2
        
        saved_count = 0
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for i, (img_base64, label) in enumerate(zip(images_base64, labels)):
            try:
                # base64 ë””ì½”ë”©
                img_bytes = base64.b64decode(img_base64)
                nparr = np.frombuffer(img_bytes, np.uint8)
                img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)
                
                if img is None:
                    continue
                
                # íŒŒì¼ëª… ìƒì„±
                label_str = 'qr_present' if label == 1 else 'qr_absent'
                filename = f"{label_str}_{timestamp}_{i:04d}.png"
                filepath = os.path.join(QR_DATA_FOLDER, label_str, filename)
                
                # ì´ë¯¸ì§€ ì €ì¥
                cv2.imwrite(filepath, img)
                saved_count += 1
                
            except Exception as e:
                print(f"âš ï¸  ì´ë¯¸ì§€ {i} ì €ì¥ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"ğŸ“¥ QR ë°ì´í„° ì—…ë¡œë“œ: {saved_count}/{len(images_base64)}ì¥ ì €ì¥ ì™„ë£Œ")
        
        return jsonify({
            'status': 'success',
            'saved_count': saved_count,
            'total_count': len(images_base64),
            'metadata': metadata
        })
    
    except Exception as e:
        import traceback
        error_msg = str(e)
        traceback.print_exc()
        return jsonify({'error': f'Server error: {error_msg}'}), 500


def main():
    parser = argparse.ArgumentParser(description='RC Car í•™ìŠµ ì„œë²„ API')
    parser.add_argument('--host', type=str, default='0.0.0.0',
                        help='ì„œë²„ í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸: 0.0.0.0)')
    parser.add_argument('--port', type=int, default=5000,
                        help='ì„œë²„ í¬íŠ¸ (ê¸°ë³¸: 5000)')
    parser.add_argument('--debug', action='store_true',
                        help='ë””ë²„ê·¸ ëª¨ë“œ')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ ì„œë²„ ì‹œì‘: http://{args.host}:{args.port}")
    print(f"ğŸ“ ì—…ë¡œë“œ í´ë”: {UPLOAD_FOLDER}")
    print(f"ğŸ“ ëª¨ë¸ í´ë”: {MODEL_FOLDER}")
    
    app.run(host=args.host, port=args.port, debug=args.debug)


if __name__ == '__main__':
    main()

