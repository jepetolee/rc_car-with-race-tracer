#!/usr/bin/env python3
"""
ìˆ˜ë™ í”¼ë“œë°± ê¸°ë°˜ PPO ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
ë¼ì¦ˆë² ë¦¬ íŒŒì´ì—ì„œ ì‹¤í–‰í•˜ì—¬ Human-in-the-Loop í•™ìŠµ ìˆ˜í–‰

ì‚¬ìš©ë²•:
    python3 train_manual_feedback.py

í‚¤ë³´ë“œ ì¡°ì‘:
    - [SPACE] ë˜ëŠ” [+]: ê¸ì • í”¼ë“œë°± (ì„ ë¡œ ìœ ì§€ ì¤‘)
    - [-] ë˜ëŠ” [n]: ë¶€ì • í”¼ë“œë°± (ì„ ë¡œ ì´íƒˆ)
    - [r]: ì—í”¼ì†Œë“œ ë¦¬ì…‹ (ì°¨ëŸ‰ ìœ„ì¹˜ ì¬ì¡°ì • í›„)
    - [s]: ëª¨ë¸ ì €ì¥
    - [q]: í•™ìŠµ ì¢…ë£Œ
    - [p]: ì¼ì‹œì •ì§€/ì¬ê°œ
"""

import sys
import os
import time
import threading
import queue
import argparse
import json
from datetime import datetime
from collections import deque
import numpy as np
import torch

# í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
try:
    import termios
    import tty
    HAS_TERMIOS = True
except ImportError:
    HAS_TERMIOS = False

from ppo_agent import PPOAgent


class TrainingMetrics:
    """
    í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì  ë° ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
    """
    
    def __init__(self, window_size: int = 20, log_file: str = None):
        """
        Args:
            window_size: ì´ë™ í‰ê·  ê³„ì‚°ì„ ìœ„í•œ ìœˆë„ìš° í¬ê¸°
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        """
        self.window_size = window_size
        
        # ì—í”¼ì†Œë“œ ë©”íŠ¸ë¦­
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_positive_rates = []  # ê¸ì • í”¼ë“œë°± ë¹„ìœ¨
        
        # ì´ë™ í‰ê· ìš© deque
        self.recent_rewards = deque(maxlen=window_size)
        self.recent_lengths = deque(maxlen=window_size)
        self.recent_positive_rates = deque(maxlen=window_size)
        
        # í˜„ì¬ ì—í”¼ì†Œë“œ ì¶”ì 
        self.current_positive = 0
        self.current_negative = 0
        self.current_neutral = 0
        
        # PPO ì†ì‹¤ ì¶”ì 
        self.policy_losses = []
        self.value_losses = []
        self.entropies = []
        
        # ë² ìŠ¤íŠ¸ ê¸°ë¡
        self.best_reward = float('-inf')
        self.best_positive_rate = 0.0
        self.best_episode = 0
        
        # í•™ìŠµ ê°œì„  ì¶”ì 
        self.improvement_streak = 0  # ì—°ì† ê°œì„  íšŸìˆ˜
        self.no_improvement_count = 0  # ê°œì„  ì—†ëŠ” ì—í”¼ì†Œë“œ ìˆ˜
        
        # ë¡œê·¸ íŒŒì¼
        if log_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = f"training_log_{timestamp}.json"
        self.log_file = log_file
        self.log_data = []
    
    def reset_episode(self):
        """ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ë¦¬ì…‹"""
        self.current_positive = 0
        self.current_negative = 0
        self.current_neutral = 0
    
    def record_feedback(self, feedback_type: str):
        """
        í”¼ë“œë°± ê¸°ë¡
        
        Args:
            feedback_type: 'positive', 'negative', 'neutral'
        """
        if feedback_type == 'positive':
            self.current_positive += 1
        elif feedback_type == 'negative':
            self.current_negative += 1
        else:
            self.current_neutral += 1
    
    def end_episode(self, episode: int, reward: float, length: int):
        """
        ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        
        Args:
            episode: ì—í”¼ì†Œë“œ ë²ˆí˜¸
            reward: ì´ ë¦¬ì›Œë“œ
            length: ì—í”¼ì†Œë“œ ê¸¸ì´
        """
        # ê¸ì • í”¼ë“œë°± ë¹„ìœ¨ ê³„ì‚°
        total_feedback = self.current_positive + self.current_negative
        if total_feedback > 0:
            positive_rate = self.current_positive / total_feedback
        else:
            positive_rate = 0.5  # í”¼ë“œë°± ì—†ìœ¼ë©´ ì¤‘ë¦½
        
        # ê¸°ë¡ ì¶”ê°€
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
        self.episode_positive_rates.append(positive_rate)
        
        self.recent_rewards.append(reward)
        self.recent_lengths.append(length)
        self.recent_positive_rates.append(positive_rate)
        
        # ë² ìŠ¤íŠ¸ ê¸°ë¡ ì—…ë°ì´íŠ¸
        improved = False
        if reward > self.best_reward:
            self.best_reward = reward
            self.best_episode = episode
            improved = True
        if positive_rate > self.best_positive_rate:
            self.best_positive_rate = positive_rate
            improved = True
        
        # ê°œì„  ì¶”ì 
        if improved:
            self.improvement_streak += 1
            self.no_improvement_count = 0
        else:
            self.improvement_streak = 0
            self.no_improvement_count += 1
        
        # ë¡œê·¸ ì €ì¥
        log_entry = {
            'episode': episode,
            'reward': reward,
            'length': length,
            'positive_rate': positive_rate,
            'positive_count': self.current_positive,
            'negative_count': self.current_negative,
            'neutral_count': self.current_neutral,
            'avg_reward': self.get_avg_reward(),
            'avg_positive_rate': self.get_avg_positive_rate(),
            'timestamp': datetime.now().isoformat()
        }
        self.log_data.append(log_entry)
    
    def record_loss(self, policy_loss: float, value_loss: float, entropy: float):
        """PPO ì†ì‹¤ ê¸°ë¡"""
        self.policy_losses.append(policy_loss)
        self.value_losses.append(value_loss)
        self.entropies.append(entropy)
    
    def get_avg_reward(self) -> float:
        """ìµœê·¼ í‰ê·  ë¦¬ì›Œë“œ"""
        if not self.recent_rewards:
            return 0.0
        return np.mean(self.recent_rewards)
    
    def get_avg_positive_rate(self) -> float:
        """ìµœê·¼ í‰ê·  ê¸ì • í”¼ë“œë°± ë¹„ìœ¨"""
        if not self.recent_positive_rates:
            return 0.0
        return np.mean(self.recent_positive_rates)
    
    def get_learning_status(self) -> str:
        """
        í•™ìŠµ ìƒíƒœ íŒë‹¨
        
        Returns:
            ìƒíƒœ ë¬¸ìì—´: 'ğŸš€ ê¸‰ì„±ì¥', 'ğŸ“ˆ ê°œì„  ì¤‘', 'â¡ï¸ ì•ˆì •', 'ğŸ“‰ ì •ì²´', 'âš ï¸ ì•…í™”'
        """
        if len(self.recent_rewards) < 5:
            return "ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì¤‘"
        
        # ìµœê·¼ íŠ¸ë Œë“œ ë¶„ì„
        recent = list(self.recent_rewards)
        first_half = np.mean(recent[:len(recent)//2])
        second_half = np.mean(recent[len(recent)//2:])
        
        improvement = (second_half - first_half) / (abs(first_half) + 1e-8)
        
        # ê¸ì • í”¼ë“œë°± ë¹„ìœ¨ í™•ì¸
        avg_positive_rate = self.get_avg_positive_rate()
        
        if improvement > 0.2 and avg_positive_rate > 0.7:
            return "ğŸš€ ê¸‰ì„±ì¥"
        elif improvement > 0.05 or self.improvement_streak >= 3:
            return "ğŸ“ˆ ê°œì„  ì¤‘"
        elif abs(improvement) <= 0.05 and avg_positive_rate > 0.5:
            return "â¡ï¸ ì•ˆì •"
        elif improvement < -0.1 or self.no_improvement_count > 10:
            return "âš ï¸ ì•…í™”"
        else:
            return "ğŸ“‰ ì •ì²´"
    
    def get_summary(self) -> str:
        """í•™ìŠµ ìš”ì•½ ë¬¸ìì—´ ìƒì„±"""
        status = self.get_learning_status()
        avg_reward = self.get_avg_reward()
        avg_positive_rate = self.get_avg_positive_rate()
        
        lines = [
            f"â”Œ{'â”€'*56}â”",
            f"â”‚ í•™ìŠµ ìƒíƒœ: {status:40} â”‚",
            f"â”œ{'â”€'*56}â”¤",
            f"â”‚ í‰ê·  ë¦¬ì›Œë“œ (ìµœê·¼ {self.window_size}): {avg_reward:+8.2f}              â”‚",
            f"â”‚ ê¸ì • í”¼ë“œë°± ë¹„ìœ¨:         {avg_positive_rate*100:5.1f}%                â”‚",
            f"â”‚ ë² ìŠ¤íŠ¸ ë¦¬ì›Œë“œ:           {self.best_reward:+8.2f} (Ep {self.best_episode:3d})     â”‚",
            f"â”œ{'â”€'*56}â”¤",
        ]
        
        if self.policy_losses:
            recent_policy = np.mean(self.policy_losses[-5:])
            recent_value = np.mean(self.value_losses[-5:])
            recent_entropy = np.mean(self.entropies[-5:])
            lines.extend([
                f"â”‚ Policy Loss:    {recent_policy:8.4f}                        â”‚",
                f"â”‚ Value Loss:     {recent_value:8.4f}                        â”‚",
                f"â”‚ Entropy:        {recent_entropy:8.4f}                        â”‚",
            ])
        
        # í•™ìŠµ ì¡°ì–¸
        advice = self._get_advice()
        lines.extend([
            f"â”œ{'â”€'*56}â”¤",
            f"â”‚ ğŸ’¡ {advice:52} â”‚",
            f"â””{'â”€'*56}â”˜",
        ])
        
        return '\n'.join(lines)
    
    def _get_advice(self) -> str:
        """í•™ìŠµ ìƒí™©ì— ë§ëŠ” ì¡°ì–¸"""
        avg_positive_rate = self.get_avg_positive_rate()
        status = self.get_learning_status()
        
        if "ê¸‰ì„±ì¥" in status:
            return "í›Œë¥­í•©ë‹ˆë‹¤! í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”."
        elif avg_positive_rate < 0.3:
            return "ë¶€ì • í”¼ë“œë°±ì´ ë§ìŠµë‹ˆë‹¤. ì†ë„ë¥¼ ë‚®ì¶”ì„¸ìš”."
        elif avg_positive_rate > 0.8 and self.no_improvement_count > 5:
            return "ì‰¬ìš´ êµ¬ê°„ì…ë‹ˆë‹¤. ë” ì–´ë ¤ìš´ ì½”ìŠ¤ë¥¼ ì‹œë„í•˜ì„¸ìš”."
        elif "ì •ì²´" in status or "ì•…í™”" in status:
            return "í•™ìŠµì´ ë©ˆì·„ìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”."
        elif len(self.recent_rewards) < 10:
            return "ë” ë§ì€ ì—í”¼ì†Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤. ê³„ì†í•˜ì„¸ìš”."
        else:
            return "ê¾¸ì¤€íˆ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ì„¸ìš”."
    
    def save_log(self):
        """ë¡œê·¸ íŒŒì¼ ì €ì¥"""
        with open(self.log_file, 'w') as f:
            json.dump(self.log_data, f, indent=2)
    
    def print_progress_bar(self, current: int, total: int, width: int = 30):
        """ì§„í–‰ë¥  ë°” ì¶œë ¥"""
        progress = current / total
        filled = int(width * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return f"[{bar}] {progress*100:5.1f}%"


class KeyboardListener:
    """ë¹„ë™ê¸° í‚¤ë³´ë“œ ì…ë ¥ ë¦¬ìŠ¤ë„ˆ"""
    
    def __init__(self):
        self.input_queue = queue.Queue()
        self.running = False
        self.thread = None
        
    def start(self):
        """ë¦¬ìŠ¤ë„ˆ ì‹œì‘"""
        if not HAS_TERMIOS:
            print("ê²½ê³ : termios ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‚¤ë³´ë“œ ì…ë ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
            return
            
        self.running = True
        self.thread = threading.Thread(target=self._listen, daemon=True)
        self.thread.start()
        
    def stop(self):
        """ë¦¬ìŠ¤ë„ˆ ì¤‘ì§€"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=1.0)
    
    def _listen(self):
        """í‚¤ë³´ë“œ ì…ë ¥ ëŒ€ê¸° (ë³„ë„ ìŠ¤ë ˆë“œ)"""
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        
        try:
            tty.setraw(fd)
            while self.running:
                if sys.stdin in [sys.stdin]:
                    ch = sys.stdin.read(1)
                    if ch:
                        self.input_queue.put(ch)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
    
    def get_input(self):
        """íì—ì„œ ì…ë ¥ ê°€ì ¸ì˜¤ê¸° (non-blocking)"""
        try:
            return self.input_queue.get_nowait()
        except queue.Empty:
            return None


class ManualFeedbackTrainer:
    """
    ìˆ˜ë™ í”¼ë“œë°± ê¸°ë°˜ í›ˆë ¨ í´ë˜ìŠ¤
    Human-in-the-Loop ê°•í™”í•™ìŠµ
    """
    
    def __init__(
        self,
        agent: PPOAgent,
        save_path: str = 'ppo_manual.pth',
        positive_reward: float = 1.0,
        negative_reward: float = -2.0,
        neutral_reward: float = 0.1,
        feedback_timeout: float = 0.5,
        update_frequency: int = 64,
        update_epochs: int = 4
    ):
        """
        Args:
            agent: PPO ì—ì´ì „íŠ¸ (TRM-PPO ê¶Œì¥)
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            positive_reward: ê¸ì • í”¼ë“œë°± ë¦¬ì›Œë“œ (ì„ ë¡œ ìœ ì§€)
            negative_reward: ë¶€ì • í”¼ë“œë°± ë¦¬ì›Œë“œ (ì„ ë¡œ ì´íƒˆ)
            neutral_reward: ì¤‘ë¦½ ë¦¬ì›Œë“œ (í”¼ë“œë°± ì—†ìŒ - ê¸°ë³¸ ì „ì§„ ë³´ìƒ)
            feedback_timeout: í”¼ë“œë°± ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            update_frequency: PPO ì—…ë°ì´íŠ¸ ì£¼ê¸° (ìŠ¤í… ìˆ˜)
            update_epochs: PPO ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
        """
        self.agent = agent
        self.save_path = save_path
        self.positive_reward = positive_reward
        self.negative_reward = negative_reward
        self.neutral_reward = neutral_reward
        self.feedback_timeout = feedback_timeout
        self.update_frequency = update_frequency
        self.update_epochs = update_epochs
        
        # RC Car ì¸í„°í˜ì´ìŠ¤
        self.rc_car = None
        self._init_rc_car()
        
        # í‚¤ë³´ë“œ ë¦¬ìŠ¤ë„ˆ
        self.keyboard = KeyboardListener()
        
        # ìƒíƒœ ë³€ìˆ˜
        self.paused = False
        self.running = False
        self.episode_count = 0
        self.step_count = 0
        self.total_steps = 0
        
        # í†µê³„
        self.episode_rewards = []
        self.positive_count = 0
        self.negative_count = 0
        
        # í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì 
        self.metrics = TrainingMetrics(window_size=20)
        
    def _init_rc_car(self):
        """RC Car ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            from rc_car_interface import RC_Car_Interface
            self.rc_car = RC_Car_Interface()
            print("âœ“ RC Car ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError as e:
            print(f"âœ— RC Car ì¸í„°í˜ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("  ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.rc_car = None
    
    def get_state(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ (ì¹´ë©”ë¼ ì´ë¯¸ì§€) ê°€ì ¸ì˜¤ê¸°"""
        if self.rc_car:
            img = self.rc_car.get_image_from_camera()
            return np.reshape(img, -1).astype(np.float32) / 255.0
        else:
            # ì‹œë®¬ë ˆì´ì…˜: ëœë¤ ìƒíƒœ
            return np.random.rand(256).astype(np.float32)
    
    def execute_action(self, action: int):
        """
        ì•¡ì…˜ ì‹¤í–‰
        
        Args:
            action: ì´ì‚° ì•¡ì…˜ (0-4)
                0: ì •ì§€
                1: ì „ì§„ ì§ì§„
                2: ì „ì§„ ì¢ŒíšŒì „
                3: ì „ì§„ ìš°íšŒì „
                4: í›„ì§„
        """
        if not self.rc_car:
            return
        
        speed = 180  # ê¸°ë³¸ ì†ë„ (0-255)
        turn_diff = 60  # íšŒì „ ì‹œ ì†ë„ ì°¨ì´
        
        if action == 0:
            # ì •ì§€
            self.rc_car.set_left_speed(0)
            self.rc_car.set_right_speed(0)
        elif action == 1:
            # ì „ì§„ ì§ì§„
            self.rc_car.set_left_speed(speed)
            self.rc_car.set_right_speed(speed)
        elif action == 2:
            # ì „ì§„ ì¢ŒíšŒì „
            self.rc_car.set_left_speed(speed - turn_diff)
            self.rc_car.set_right_speed(speed + turn_diff)
        elif action == 3:
            # ì „ì§„ ìš°íšŒì „
            self.rc_car.set_left_speed(speed + turn_diff)
            self.rc_car.set_right_speed(speed - turn_diff)
        elif action == 4:
            # í›„ì§„
            self.rc_car.set_left_speed(0)  # í›„ì§„ì€ ë³„ë„ ì²˜ë¦¬ í•„ìš”
            self.rc_car.set_right_speed(0)
    
    def stop_car(self):
        """ì°¨ëŸ‰ ì •ì§€"""
        if self.rc_car:
            self.rc_car.stop()
    
    def process_feedback(self, key: str) -> tuple:
        """
        í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬
        
        Args:
            key: ì…ë ¥ëœ í‚¤
        
        Returns:
            (reward, done, action): ë¦¬ì›Œë“œ, ì¢…ë£Œ ì—¬ë¶€, íŠ¹ìˆ˜ ì•¡ì…˜
        """
        if key is None:
            # í”¼ë“œë°± ì—†ìŒ - ì¤‘ë¦½ ë¦¬ì›Œë“œ
            return self.neutral_reward, False, None
        
        key = key.lower()
        
        if key == ' ' or key == '+' or key == '=':
            # ê¸ì • í”¼ë“œë°±: ì„ ë¡œ ìœ ì§€ ì¤‘ â†’ ê³„ì† ì§„í–‰
            self.positive_count += 1
            self.metrics.record_feedback('positive')
            print("  âœ“ ì„ ë¡œ ìœ ì§€ (+) - ê³„ì† ì§„í–‰", end='\r')
            return self.positive_reward, False, None
        
        elif key == '-' or key == 'n':
            # ë¶€ì • í”¼ë“œë°±: ì„ ë¡œ ì´íƒˆ â†’ ì¦‰ì‹œ ì •ì§€!
            self.negative_count += 1
            self.metrics.record_feedback('negative')
            self.stop_car()  # ì¦‰ì‹œ ì •ì§€
            print("\n  âœ— ì„ ë¡œ ì´íƒˆ! ì°¨ëŸ‰ ì •ì§€ë¨")
            print("    â†’ ì°¨ëŸ‰ì„ ì„ ë¡œì— ë‹¤ì‹œ ì˜¬ë ¤ë†“ìœ¼ì„¸ìš”")
            print("    â†’ [SPACE] ëˆ„ë¥´ë©´ ì¬ê°œ, [r] ëˆ„ë¥´ë©´ ì—í”¼ì†Œë“œ ë¦¬ì…‹")
            return self.negative_reward, False, 'wait_reposition'
        
        elif key == 'r':
            # ì—í”¼ì†Œë“œ ë¦¬ì…‹
            self.stop_car()
            print("\n  â†º ì—í”¼ì†Œë“œ ë¦¬ì…‹")
            return self.negative_reward, True, 'reset'
        
        elif key == 's':
            # ëª¨ë¸ ì €ì¥
            self.agent.save(self.save_path)
            print(f"\n  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨: {self.save_path}")
            return None, False, 'save'
        
        elif key == 'p':
            # ì¼ì‹œì •ì§€/ì¬ê°œ
            self.paused = not self.paused
            status = "ì¼ì‹œì •ì§€" if self.paused else "ì¬ê°œ"
            print(f"\n  â¸ {status}")
            return None, False, 'pause'
        
        elif key == 'q' or key == '\x03':  # q ë˜ëŠ” Ctrl+C
            # ì¢…ë£Œ
            print("\n  â¹ í•™ìŠµ ì¢…ë£Œ")
            return None, True, 'quit'
        
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” í‚¤
            return self.neutral_reward, False, None
    
    def print_status(self, episode_reward: float):
        """í˜„ì¬ ìƒíƒœ ì¶œë ¥"""
        print(f"\r[Ep {self.episode_count}] "
              f"Step: {self.step_count:4d} | "
              f"Total: {self.total_steps:6d} | "
              f"Reward: {episode_reward:+.2f} | "
              f"(+): {self.positive_count} (-): {self.negative_count}    ", end='')
    
    def train(self, max_episodes: int = 1000, max_steps_per_episode: int = 500):
        """
        ìˆ˜ë™ í”¼ë“œë°± ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰
        
        Args:
            max_episodes: ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜
            max_steps_per_episode: ì—í”¼ì†Œë“œ ë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜
        """
        print("=" * 60)
        print("ìˆ˜ë™ í”¼ë“œë°± ê¸°ë°˜ PPO í•™ìŠµ")
        print("=" * 60)
        print()
        print("í‚¤ë³´ë“œ ì¡°ì‘:")
        print("  [SPACE] / [+]: ê¸ì • í”¼ë“œë°± (ì„ ë¡œ ìœ ì§€)")
        print("  [-] / [n]   : ë¶€ì • í”¼ë“œë°± (ì„ ë¡œ ì´íƒˆ)")
        print("  [r]         : ì—í”¼ì†Œë“œ ë¦¬ì…‹")
        print("  [s]         : ëª¨ë¸ ì €ì¥")
        print("  [p]         : ì¼ì‹œì •ì§€/ì¬ê°œ")
        print("  [q]         : í•™ìŠµ ì¢…ë£Œ")
        print()
        print("=" * 60)
        print()
        
        # TRM-PPO ëª¨ë“œ í™•ì¸
        use_recurrent = getattr(self.agent, 'use_recurrent', False)
        if use_recurrent:
            print(f"TRM-PPO ëª¨ë“œ í™œì„±í™” (n_cycles={self.agent.n_cycles})")
        
        # í‚¤ë³´ë“œ ë¦¬ìŠ¤ë„ˆ ì‹œì‘
        self.keyboard.start()
        self.running = True
        
        try:
            for episode in range(max_episodes):
                if not self.running:
                    break
                
                self.episode_count = episode + 1
                self.step_count = 0
                episode_reward = 0.0
                
                # ìƒíƒœ ì´ˆê¸°í™”
                state = self.get_state()
                
                # TRM-PPO: ì ì¬ ìƒíƒœ ë¦¬ì…‹
                if use_recurrent:
                    self.agent.reset_carry()
                
                print(f"\n--- ì—í”¼ì†Œë“œ {self.episode_count} ì‹œì‘ ---")
                print("ì°¨ëŸ‰ì„ ì„ ë¡œ ìœ„ì— ì˜¬ë ¤ë†“ê³  í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”.")
                print("ì¤€ë¹„ë˜ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”...")
                
                # ì‹œì‘ ëŒ€ê¸°
                while self.running:
                    key = self.keyboard.get_input()
                    if key:
                        break
                    time.sleep(0.1)
                
                if not self.running:
                    break
                
                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                done = False
                while not done and self.step_count < max_steps_per_episode and self.running:
                    # ì¼ì‹œì •ì§€ ì²´í¬
                    while self.paused and self.running:
                        self.stop_car()
                        time.sleep(0.1)
                        key = self.keyboard.get_input()
                        if key:
                            self.process_feedback(key)
                    
                    if not self.running:
                        break
                    
                    # ìƒíƒœ í…ì„œ ë³€í™˜
                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.agent.device)
                    
                    # ì•¡ì…˜ ì„ íƒ
                    if use_recurrent:
                        action, log_prob, value, latent_np = self.agent.get_action_with_carry(state_tensor)
                    else:
                        action, log_prob, value = self.agent.actor_critic.get_action(state_tensor)
                        latent_np = None
                    
                    # ì´ì‚° ì•¡ì…˜ ì¶”ì¶œ
                    action_int = action.squeeze().cpu().item()
                    if isinstance(action_int, float):
                        action_int = int(action_int)
                    
                    # ì•¡ì…˜ ì‹¤í–‰
                    self.execute_action(action_int)
                    
                    # í”¼ë“œë°± ëŒ€ê¸°
                    time.sleep(self.feedback_timeout)
                    
                    # ë‹¤ìŒ ìƒíƒœ
                    next_state = self.get_state()
                    
                    # í‚¤ë³´ë“œ í”¼ë“œë°± ì²˜ë¦¬
                    key = self.keyboard.get_input()
                    reward, done, special_action = self.process_feedback(key)
                    
                    if special_action == 'quit':
                        self.running = False
                        break
                    elif special_action in ['save', 'pause']:
                        continue
                    elif special_action == 'wait_reposition':
                        # ì„ ë¡œ ì´íƒˆ! ì°¨ëŸ‰ì´ ë©ˆì¶”ê³  ì¬ë°°ì¹˜ ëŒ€ê¸°
                        # ì‚¬ìš©ìê°€ SPACEë¥¼ ëˆ„ë¥´ë©´ ì¬ê°œ, 'r'ì„ ëˆ„ë¥´ë©´ ì—í”¼ì†Œë“œ ë¦¬ì…‹
                        while self.running:
                            reposition_key = self.keyboard.get_input()
                            if reposition_key == ' ' or reposition_key == '+':
                                print("    â–¶ ì¬ê°œ!")
                                break
                            elif reposition_key == 'r':
                                print("    â†º ì—í”¼ì†Œë“œ ë¦¬ì…‹")
                                done = True
                                break
                            elif reposition_key == 'q':
                                self.running = False
                                break
                            time.sleep(0.1)
                        
                        if done or not self.running:
                            break
                        
                        # ìƒˆ ìƒíƒœ ê°±ì‹  (ì¬ë°°ì¹˜ í›„)
                        next_state = self.get_state()
                    
                    if reward is None:
                        reward = self.neutral_reward
                    
                    # ë²„í¼ì— ì €ì¥
                    log_prob_val = log_prob.cpu().item() if log_prob is not None else 0.0
                    value_val = value.squeeze().cpu().item()
                    
                    if use_recurrent:
                        self.agent.store_transition(
                            state, action_int, reward, done,
                            log_prob_val, value_val, latent=latent_np
                        )
                    else:
                        self.agent.store_transition(
                            state, action_int, reward, done,
                            log_prob_val, value_val
                        )
                    
                    episode_reward += reward
                    self.step_count += 1
                    self.total_steps += 1
                    state = next_state
                    
                    # ìƒíƒœ ì¶œë ¥
                    self.print_status(episode_reward)
                    
                    # PPO ì—…ë°ì´íŠ¸
                    if len(self.agent.buffer['states']) >= self.update_frequency:
                        self.stop_car()
                        print(f"\n  ğŸ”„ PPO ì—…ë°ì´íŠ¸ ì¤‘...")
                        loss_info = self.agent.update(epochs=self.update_epochs)
                        if loss_info:
                            print(f"     Loss: {loss_info['loss']:.4f}, "
                                  f"Policy: {loss_info['policy_loss']:.4f}, "
                                  f"Value: {loss_info['value_loss']:.4f}")
                            # ë©”íŠ¸ë¦­ì— ì†ì‹¤ ê¸°ë¡
                            self.metrics.record_loss(
                                loss_info['policy_loss'],
                                loss_info['value_loss'],
                                loss_info['entropy']
                            )
                
                # ì—í”¼ì†Œë“œ ì¢…ë£Œ
                self.stop_car()
                self.episode_rewards.append(episode_reward)
                
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                self.metrics.end_episode(self.episode_count, episode_reward, self.step_count)
                self.metrics.reset_episode()
                
                print(f"\n--- ì—í”¼ì†Œë“œ {self.episode_count} ì¢…ë£Œ ---")
                print(f"    ì´ ìŠ¤í…: {self.step_count}")
                print(f"    ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ: {episode_reward:.2f}")
                
                # í•™ìŠµ ìƒíƒœ ìš”ì•½ (5 ì—í”¼ì†Œë“œë§ˆë‹¤)
                if self.episode_count % 5 == 0:
                    print()
                    print(self.metrics.get_summary())
                elif len(self.episode_rewards) > 1:
                    avg_reward = self.metrics.get_avg_reward()
                    pos_rate = self.metrics.get_avg_positive_rate()
                    status = self.metrics.get_learning_status()
                    print(f"    {status} | í‰ê·  ë¦¬ì›Œë“œ: {avg_reward:+.2f} | ê¸ì •ë¥ : {pos_rate*100:.1f}%")
                
                # ì—í”¼ì†Œë“œë§ˆë‹¤ ìë™ ì €ì¥
                if episode % 5 == 0:
                    self.agent.save(self.save_path)
                    print(f"    ğŸ’¾ ìë™ ì €ì¥ë¨")
        
        except KeyboardInterrupt:
            print("\n\ní•™ìŠµ ì¤‘ë‹¨ë¨ (Ctrl+C)")
        
        finally:
            self.stop_car()
            self.keyboard.stop()
            
            # ìµœì¢… ì €ì¥
            self.agent.save(self.save_path)
            
            # ë¡œê·¸ íŒŒì¼ ì €ì¥
            self.metrics.save_log()
            
            # ìµœì¢… í•™ìŠµ ìš”ì•½
            print("\n")
            print("=" * 60)
            print("ğŸ“Š í•™ìŠµ ì™„ë£Œ - ìµœì¢… ìš”ì•½")
            print("=" * 60)
            print()
            print(self.metrics.get_summary())
            print()
            print(f"ì´ ì—í”¼ì†Œë“œ: {self.episode_count}")
            print(f"ì´ ìŠ¤í…: {self.total_steps}")
            print(f"ê¸ì • í”¼ë“œë°± ìˆ˜: {self.positive_count}")
            print(f"ë¶€ì • í”¼ë“œë°± ìˆ˜: {self.negative_count}")
            
            if self.episode_rewards:
                print()
                print("ë¦¬ì›Œë“œ í†µê³„:")
                print(f"  - í‰ê· : {np.mean(self.episode_rewards):.2f}")
                print(f"  - ìµœê³ : {np.max(self.episode_rewards):.2f}")
                print(f"  - ìµœì €: {np.min(self.episode_rewards):.2f}")
                print(f"  - í‘œì¤€í¸ì°¨: {np.std(self.episode_rewards):.2f}")
            
            print()
            print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {self.save_path}")
            print(f"ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: {self.metrics.log_file}")
            print()
            
            # í•™ìŠµ ê²°ê³¼ íŒì •
            final_status = self.metrics.get_learning_status()
            avg_pos_rate = self.metrics.get_avg_positive_rate()
            
            if avg_pos_rate > 0.8:
                print("ğŸ‰ í›Œë¥­í•©ë‹ˆë‹¤! ëª¨ë¸ì´ ì„ ë¡œë¥¼ ì˜ ë”°ë¼ê°‘ë‹ˆë‹¤.")
            elif avg_pos_rate > 0.6:
                print("ğŸ‘ ì¢‹ì€ ì§„ì „ì…ë‹ˆë‹¤. ë” í•™ìŠµí•˜ë©´ ê°œì„ ë  ê²ƒì…ë‹ˆë‹¤.")
            elif avg_pos_rate > 0.4:
                print("ğŸ“ˆ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ë” ë§ì€ í”¼ë“œë°±ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                print("ğŸ’¡ ë” ë§ì€ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.")
            
            print("=" * 60)
            
            # RC Car ì •ë¦¬
            if self.rc_car:
                self.rc_car.close()


def main():
    parser = argparse.ArgumentParser(description='ìˆ˜ë™ í”¼ë“œë°± ê¸°ë°˜ PPO í•™ìŠµ')
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    parser.add_argument('--max-episodes', type=int, default=100,
                        help='ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--max-steps', type=int, default=500,
                        help='ì—í”¼ì†Œë“œ ë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸: 500)')
    parser.add_argument('--update-frequency', type=int, default=64,
                        help='PPO ì—…ë°ì´íŠ¸ ì£¼ê¸° (ê¸°ë³¸: 64)')
    parser.add_argument('--update-epochs', type=int, default=4,
                        help='PPO ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜ (ê¸°ë³¸: 4)')
    
    # ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
    parser.add_argument('--positive-reward', type=float, default=1.0,
                        help='ê¸ì • í”¼ë“œë°± ë¦¬ì›Œë“œ (ê¸°ë³¸: 1.0)')
    parser.add_argument('--negative-reward', type=float, default=-2.0,
                        help='ë¶€ì • í”¼ë“œë°± ë¦¬ì›Œë“œ (ê¸°ë³¸: -2.0)')
    parser.add_argument('--neutral-reward', type=float, default=0.1,
                        help='ì¤‘ë¦½ ë¦¬ì›Œë“œ (ê¸°ë³¸: 0.1)')
    parser.add_argument('--feedback-timeout', type=float, default=0.3,
                        help='í”¼ë“œë°± ëŒ€ê¸° ì‹œê°„ ì´ˆ (ê¸°ë³¸: 0.3)')
    
    # TRM-PPO íŒŒë¼ë¯¸í„°
    parser.add_argument('--use-recurrent', action='store_true', default=True,
                        help='TRM-PPO ëª¨ë“œ ì‚¬ìš© (ê¸°ë³¸: True)')
    parser.add_argument('--no-recurrent', dest='use_recurrent', action='store_false',
                        help='ê¸°ì¡´ PPO ëª¨ë“œ ì‚¬ìš©')
    parser.add_argument('--n-cycles', type=int, default=4,
                        help='TRM-PPO ì¬ê·€ ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 4)')
    parser.add_argument('--latent-dim', type=int, default=256,
                        help='TRM-PPO ì ì¬ ìƒíƒœ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--hidden-dim', type=int, default=256,
                        help='íˆë“  ë ˆì´ì–´ ì°¨ì› (ê¸°ë³¸: 256)')
    
    # ì €ì¥/ë¡œë“œ
    parser.add_argument('--save-path', type=str, default='ppo_manual.pth',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ppo_manual.pth)')
    parser.add_argument('--load-path', type=str, default=None,
                        help='ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ê²½ë¡œ')
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None,
                        help='ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™)')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = PPOAgent(
        state_dim=256,
        action_dim=5,
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=True,
        discrete_action=True,
        num_discrete_actions=5,
        use_recurrent=args.use_recurrent,
        device=device,
        # ìˆ˜ë™ í”¼ë“œë°±ì— ë§ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
        lr_actor=1e-4,
        lr_critic=1e-4,
        gamma=0.95,
        gae_lambda=0.9,
        clip_epsilon=0.1,
        entropy_coef=0.05  # íƒìƒ‰ ê°•í™”
    )
    
    # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ
    if args.load_path and os.path.exists(args.load_path):
        agent.load(args.load_path)
        print(f"ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œë¨: {args.load_path}")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° ì‹¤í–‰
    trainer = ManualFeedbackTrainer(
        agent=agent,
        save_path=args.save_path,
        positive_reward=args.positive_reward,
        negative_reward=args.negative_reward,
        neutral_reward=args.neutral_reward,
        feedback_timeout=args.feedback_timeout,
        update_frequency=args.update_frequency,
        update_epochs=args.update_epochs
    )
    
    trainer.train(
        max_episodes=args.max_episodes,
        max_steps_per_episode=args.max_steps
    )


if __name__ == "__main__":
    main()

