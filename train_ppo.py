#!/usr/bin/env python3
"""
PPO ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
RC Car í™˜ê²½ì—ì„œ PPO ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµ
"""

import argparse
import numpy as np
import torch
import time
import sys
import os
from datetime import datetime
from collections import deque
from rc_car_sim_env import RCCarSimEnv
from car_racing_env import CarRacingEnvWrapper
from ppo_agent import PPOAgent

# TensorBoard ì§€ì› (ì„ íƒì )
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False
    print("âš ï¸  TensorBoard ë¯¸ì„¤ì¹˜ - pip install tensorboard ë¡œ ì„¤ì¹˜í•˜ë©´ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥")

# ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ì€ ì„ íƒì  ì„í¬íŠ¸
try:
    from rc_car_env import RCCarEnv
    HAS_REAL_ENV = True
except ImportError:
    HAS_REAL_ENV = False
    RCCarEnv = None


def train_ppo(
    env,
    agent,
    total_steps=100000,
    max_episode_steps=1000,
    update_frequency=2048,
    update_epochs=10,
    save_frequency=10000,
    save_path='ppo_model.pth',
    log_frequency=100,
    use_tensorboard=True,
    log_dir='runs',
    mc_update_on_done=False
):
    """
    PPO í•™ìŠµ í•¨ìˆ˜ (TRM-PPO ì§€ì›)
    
    Args:
        env: í™˜ê²½ ê°ì²´
        agent: PPO ì—ì´ì „íŠ¸ (TRM-PPO ë˜ëŠ” ê¸°ì¡´ PPO)
        total_steps: ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
        max_episode_steps: ì—í”¼ì†Œë“œ ìµœëŒ€ ìŠ¤í… ìˆ˜
        update_frequency: ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë²„í¼ í¬ê¸°)
        update_epochs: ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
        save_frequency: ëª¨ë¸ ì €ì¥ ì£¼ê¸°
        save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        log_frequency: ë¡œê·¸ ì¶œë ¥ ì£¼ê¸°
        use_tensorboard: TensorBoard ì‚¬ìš© ì—¬ë¶€
        log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
    """
    step_count = 0
    episode_count = 0
    episode_rewards = []
    episode_lengths = []
    
    # ì´ë™ í‰ê· ì„ ìœ„í•œ deque
    recent_rewards = deque(maxlen=100)
    recent_lengths = deque(maxlen=100)
    best_avg_reward = float('-inf')
    
    # TRM-PPO ëª¨ë“œ í™•ì¸
    use_recurrent = getattr(agent, 'use_recurrent', False)
    
    # TensorBoard ì„¤ì •
    writer = None
    if use_tensorboard and HAS_TENSORBOARD:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        mode_str = f"TRM-PPO_n{agent.n_cycles}" if use_recurrent else "PPO"
        log_path = os.path.join(log_dir, f"{mode_str}_{timestamp}")
        writer = SummaryWriter(log_path)
        print(f"ğŸ“Š TensorBoard ë¡œê·¸: {log_path}")
        print(f"   ì‹¤í–‰: tensorboard --logdir={log_dir}")
    
    print("=" * 60)
    print("PPO ê°•í™”í•™ìŠµ ì‹œì‘")
    if use_recurrent:
        print("  -> TRM-PPO ëª¨ë“œ (ì¬ê·€ ì¶”ë¡  + ì ì¬ ìƒíƒœ carry-over)")
        print(f"  -> n_cycles: {agent.n_cycles}, carry_latent: {agent.carry_latent}")
    print("=" * 60)
    print(f"ì´ í•™ìŠµ ìŠ¤í…: {total_steps}")
    print(f"ì—…ë°ì´íŠ¸ ì£¼ê¸°: {update_frequency} ìŠ¤í…")
    print(f"ì—í”¼ì†Œë“œ ìµœëŒ€ ê¸¸ì´: {max_episode_steps}")
    print("=" * 60)
    
    # Gymnasium vs Gym API ì°¨ì´ ì²˜ë¦¬
    reset_result = env.reset()
    if isinstance(reset_result, tuple) and len(reset_result) == 2:
        state, _ = reset_result  # Gymnasium
    else:
        state = reset_result  # Gym
    
    # TRM-PPO: ì ì¬ ìƒíƒœ ì´ˆê¸°í™”
    if use_recurrent:
        agent.reset_carry()
    
    episode_reward = 0
    episode_length = 0
    
    try:
        while step_count < total_steps:
            # ìƒíƒœ ì •ê·œí™” [0, 255] -> [0, 1] (ì¤‘ìš”!)
            state_normalized = state.astype(np.float32) / 255.0
            
            # ì•¡ì…˜ ì„ íƒ (TRM-PPO ë˜ëŠ” ê¸°ì¡´ PPO)
            state_tensor = torch.FloatTensor(state_normalized).unsqueeze(0).to(agent.device)
            
            if use_recurrent:
                # TRM-PPO: get_action_with_carry ì‚¬ìš©
                action, log_prob, value, latent_np = agent.get_action_with_carry(state_tensor)
            else:
                # ê¸°ì¡´ PPO: actor_critic.get_action ì‚¬ìš©
                action, log_prob, value = agent.actor_critic.get_action(state_tensor)
                latent_np = None
            
            # ì´ì‚° ì•¡ì…˜ê³¼ ì—°ì† ì•¡ì…˜ ì²˜ë¦¬
            if agent.actor_critic.discrete_action:
                action_np = action.squeeze(0).cpu().detach().numpy().item()  # ì •ìˆ˜ë¡œ ë³€í™˜
            else:
                action_np = action.squeeze(0).cpu().detach().numpy()
            log_prob_np = log_prob.squeeze(0).cpu().item() if log_prob is not None else 0.0
            value_np = value.squeeze(0).cpu().item()
            
            # í™˜ê²½ ìŠ¤í…
            next_state, reward, done, info = env.step(action_np)
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ í™•ì¸ (ë²„í¼ ì €ì¥ ì „ì— ê³„ì‚°)
            is_terminal = done or (episode_length + 1) >= max_episode_steps
            
            # ë²„í¼ì— ì €ì¥ (ì •ê·œí™”ëœ ìƒíƒœ ì‚¬ìš©, TRM-PPOëŠ” ì ì¬ ìƒíƒœë„ ì €ì¥)
            # MC ëª¨ë“œì—ì„œëŠ” truncationë„ doneìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë¦¬í„´ ê³„ì‚° ì •í™•ì„± ë³´ì¥
            done_for_buffer = done if not mc_update_on_done else is_terminal
            
            if use_recurrent:
                agent.store_transition(
                    state_normalized.copy(),
                    action_np,
                    reward,
                    done_for_buffer,
                    log_prob_np,
                    value_np,
                    latent=latent_np
                )
            else:
                agent.store_transition(
                    state_normalized.copy(),
                    action_np,
                    reward,
                    done_for_buffer,
                    log_prob_np,
                    value_np
                )
            
            episode_reward += reward
            episode_length += 1
            step_count += 1
            state = next_state
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ (í™˜ê²½ done OR ìµœëŒ€ ìŠ¤í… ë„ë‹¬)
            episode_done = done or episode_length >= max_episode_steps
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ë˜ëŠ” ìµœëŒ€ ìŠ¤í… ë„ë‹¬
            if episode_done:
                episode_count += 1
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
                recent_rewards.append(episode_reward)
                recent_lengths.append(episode_length)
                
                # TensorBoard ë¡œê¹…
                if writer:
                    writer.add_scalar('Episode/Reward', episode_reward, episode_count)
                    writer.add_scalar('Episode/Length', episode_length, episode_count)
                    if len(recent_rewards) >= 10:
                        writer.add_scalar('Episode/AvgReward_100', np.mean(recent_rewards), episode_count)
                        writer.add_scalar('Episode/AvgLength_100', np.mean(recent_lengths), episode_count)
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if len(recent_rewards) >= 10:
                    current_avg = np.mean(recent_rewards)
                    if current_avg > best_avg_reward:
                        best_avg_reward = current_avg
                        best_path = save_path.replace('.pth', '_best.pth')
                        agent.save(best_path)
                        print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ê¸°ë¡! Avg Reward: {best_avg_reward:.2f}")
                
                # ì—í”¼ì†Œë“œ ì •ë³´ ì¶œë ¥ (ë§¤ ì—í”¼ì†Œë“œ)
                avg_reward = np.mean(recent_rewards) if recent_rewards else episode_reward
                avg_length = np.mean(recent_lengths) if recent_lengths else episode_length
                print(f"[Ep {episode_count}] "
                      f"R: {episode_reward:.2f} (Avg100: {avg_reward:.2f}), "
                      f"Len: {episode_length}, Steps: {step_count}")
                
                # í™˜ê²½ ë¦¬ì…‹
                reset_result = env.reset()
                if isinstance(reset_result, tuple) and len(reset_result) == 2:
                    state, _ = reset_result  # Gymnasium
                else:
                    state = reset_result  # Gym
                
                # TRM-PPO: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ì ì¬ ìƒíƒœ ë¦¬ì…‹
                if use_recurrent:
                    agent.reset_carry()
                
                episode_reward = 0
                episode_length = 0
            
            # ì—…ë°ì´íŠ¸ ì¡°ê±´ ê²°ì •
            should_update = False
            if mc_update_on_done:
                # Monte Carlo ìŠ¤íƒ€ì¼: ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œì—ë§Œ ì—…ë°ì´íŠ¸
                # episode_done = í™˜ê²½ done OR ìµœëŒ€ ìŠ¤í… ë„ë‹¬ (truncation í¬í•¨)
                should_update = episode_done and len(agent.buffer['states']) > 0
            else:
                # ì¼ë°˜ PPO: ë²„í¼ê°€ ì¶©ë¶„íˆ ì°¼ì„ ë•Œ ì—…ë°ì´íŠ¸
                should_update = len(agent.buffer['states']) >= update_frequency
            
            if should_update:
                loss_info = agent.update(epochs=update_epochs)
                
                if loss_info:
                    mc_tag = "[MC] " if mc_update_on_done else ""
                    print(f"{mc_tag}[Step {step_count}] "
                          f"Loss: {loss_info['loss']:.4f}, "
                          f"Ï€: {loss_info['policy_loss']:.4f}, "
                          f"V: {loss_info['value_loss']:.4f}, "
                          f"H: {loss_info['entropy']:.3f}, "
                          f"Adv: {loss_info.get('adv_mean', 0):.2f}Â±{loss_info.get('adv_std', 0):.2f}, "
                          f"Ratio: {loss_info.get('ratio_mean', 1):.3f}")
                    
                    # TensorBoard ë¡œê¹…
                    if writer:
                        writer.add_scalar('Train/Loss', loss_info['loss'], step_count)
                        writer.add_scalar('Train/PolicyLoss', loss_info['policy_loss'], step_count)
                        writer.add_scalar('Train/ValueLoss', loss_info['value_loss'], step_count)
                        writer.add_scalar('Train/Entropy', loss_info['entropy'], step_count)
                        writer.flush()
            
            # ì •ê¸° ì €ì¥
            if step_count % save_frequency == 0 and step_count > 0:
                agent.save(save_path)
                print(f"Model saved at step {step_count}")
    
    except KeyboardInterrupt:
        print("\ní•™ìŠµ ì¤‘ë‹¨ë¨")
    
    finally:
        # TensorBoard ì¢…ë£Œ
        if writer:
            writer.close()
        
        # ìµœì¢… ì €ì¥
        agent.save(save_path)
        env.close()
        
        # ìµœì¢… í†µê³„
        if episode_rewards:
            print("\n" + "=" * 60)
            print("í•™ìŠµ ì™„ë£Œ")
            print("=" * 60)
            print(f"ì´ ì—í”¼ì†Œë“œ: {episode_count}")
            print(f"ì´ ìŠ¤í…: {step_count}")
            print(f"í‰ê·  ë¦¬ì›Œë“œ: {np.mean(episode_rewards):.2f}")
            print(f"ìµœê³  ë¦¬ì›Œë“œ: {np.max(episode_rewards):.2f}")
            print(f"ìµœê³  í‰ê· (100ep) ë¦¬ì›Œë“œ: {best_avg_reward:.2f}")
            print(f"í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´: {np.mean(episode_lengths):.1f}")
            print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {save_path}")
            if use_recurrent:
                print(f"ëª¨ë“œ: TRM-PPO (n_cycles={agent.n_cycles})")
            if writer:
                print(f"ğŸ“Š TensorBoard: tensorboard --logdir={log_dir}")
            print("=" * 60)


def test_agent(env, agent, num_episodes=5, max_steps=1000):
    """
    í•™ìŠµëœ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ (TRM-PPO ì§€ì›)
    
    Args:
        env: í™˜ê²½ ê°ì²´
        agent: í•™ìŠµëœ PPO ì—ì´ì „íŠ¸ (TRM-PPO ë˜ëŠ” ê¸°ì¡´ PPO)
        num_episodes: í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜
        max_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜
    """
    # TRM-PPO ëª¨ë“œ í™•ì¸
    use_recurrent = getattr(agent, 'use_recurrent', False)
    
    print("=" * 60)
    print("ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    if use_recurrent:
        print(f"  -> TRM-PPO ëª¨ë“œ (n_cycles={agent.n_cycles})")
    print("=" * 60)
    
    episode_rewards = []
    
    for episode in range(num_episodes):
        # Gymnasium vs Gym API ì°¨ì´ ì²˜ë¦¬
        reset_result = env.reset()
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            state, _ = reset_result  # Gymnasium
        else:
            state = reset_result  # Gym
        
        # TRM-PPO: ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ì ì¬ ìƒíƒœ ë¦¬ì…‹
        if use_recurrent:
            agent.reset_carry()
        
        episode_reward = 0
        
        for step in range(max_steps):
            # ìƒíƒœ ì •ê·œí™”
            state_normalized = state.astype(np.float32) / 255.0
            state_tensor = torch.FloatTensor(state_normalized).unsqueeze(0).to(agent.device)
            
            if use_recurrent:
                # TRM-PPO: get_action_with_carry ì‚¬ìš© (deterministic)
                action, _, _, _ = agent.get_action_with_carry(state_tensor, deterministic=True)
            else:
                # ê¸°ì¡´ PPO
                action, _, _ = agent.actor_critic.get_action(state_tensor, deterministic=True)
            
            # ì´ì‚° ì•¡ì…˜ê³¼ ì—°ì† ì•¡ì…˜ ì²˜ë¦¬
            if agent.actor_critic.discrete_action:
                action_np = action.squeeze(0).cpu().detach().numpy().item()  # ì •ìˆ˜ë¡œ ë³€í™˜
            else:
                action_np = action.squeeze(0).cpu().detach().numpy()
            next_state, reward, done, info = env.step(action_np)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {step + 1}")
    
    print("=" * 60)
    print(f"í‰ê·  ë¦¬ì›Œë“œ: {np.mean(episode_rewards):.2f}")
    print("=" * 60)
    
    env.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='PPO ê°•í™”í•™ìŠµ í›ˆë ¨')
    
    # í™˜ê²½ íŒŒë¼ë¯¸í„°
    parser.add_argument('--max-episode-steps', type=int, default=1000,
                        help='ì—í”¼ì†Œë“œ ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸: 1000)')
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    parser.add_argument('--total-steps', type=int, default=100000,
                        help='ì´ í•™ìŠµ ìŠ¤í… ìˆ˜ (ê¸°ë³¸: 100000)')
    parser.add_argument('--update-frequency', type=int, default=2048,
                        help='ì—…ë°ì´íŠ¸ ì£¼ê¸° (ê¸°ë³¸: 2048)')
    parser.add_argument('--update-epochs', type=int, default=10,
                        help='ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜ (ê¸°ë³¸: 10)')
    
    # ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°
    parser.add_argument('--hidden-dim', type=int, default=256,
                        help='íˆë“  ë ˆì´ì–´ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--lr-actor', type=float, default=3e-4,
                        help='Actor í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)')
    parser.add_argument('--lr-critic', type=float, default=3e-4,
                        help='Critic í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)')
    parser.add_argument('--gamma', type=float, default=0.99,
                        help='í• ì¸ìœ¨ (ê¸°ë³¸: 0.99)')
    parser.add_argument('--gae-lambda', type=float, default=0.95,
                        help='GAE ëŒë‹¤ (ê¸°ë³¸: 0.95)')
    parser.add_argument('--clip-epsilon', type=float, default=0.2,
                        help='PPO í´ë¦½ ë²”ìœ„ (ê¸°ë³¸: 0.2)')
    parser.add_argument('--entropy-coef', type=float, default=0.01,
                        help='ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (ê¸°ë³¸: 0.01, ë‚®ì¶œìˆ˜ë¡ exploitation)')
    parser.add_argument('--value-coef', type=float, default=0.5,
                        help='ê°€ì¹˜ ì†ì‹¤ ê³„ìˆ˜ (ê¸°ë³¸: 0.5)')
    
    # TRM-PPO íŒŒë¼ë¯¸í„°
    parser.add_argument('--use-recurrent', action='store_true', default=True,
                        help='TRM-PPO ëª¨ë“œ ì‚¬ìš© (ì¬ê·€ ì¶”ë¡ , ê¸°ë³¸: True)')
    parser.add_argument('--no-recurrent', dest='use_recurrent', action='store_false',
                        help='ê¸°ì¡´ PPO ëª¨ë“œ ì‚¬ìš© (TRM ë¹„í™œì„±í™”)')
    parser.add_argument('--n-cycles', type=int, default=4,
                        help='TRM-PPO ì¬ê·€ ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 4)')
    parser.add_argument('--latent-dim', type=int, default=256,
                        help='TRM-PPO ì ì¬ ìƒíƒœ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--carry-latent', action='store_true', default=True,
                        help='ì—í”¼ì†Œë“œ ë‚´ ì ì¬ ìƒíƒœ carry-over (ê¸°ë³¸: True)')
    parser.add_argument('--no-carry-latent', dest='carry_latent', action='store_false',
                        help='ë§¤ ìŠ¤í… ì ì¬ ìƒíƒœ ì´ˆê¸°í™”')
    
    # Monte Carlo ì˜µì…˜
    parser.add_argument('--use-mc', action='store_true', default=False,
                        help='Monte Carlo ë¦¬í„´ ì‚¬ìš© (GAE ëŒ€ì‹  ìˆœìˆ˜ ì—í”¼ì†Œë“œ ë¦¬í„´)')
    parser.add_argument('--mc-update-on-done', action='store_true', default=False,
                        help='ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œì—ë§Œ ì—…ë°ì´íŠ¸ (MC ìŠ¤íƒ€ì¼)')
    
    # ì €ì¥/ë¡œë“œ
    parser.add_argument('--save-path', type=str, default='ppo_model.pth',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ppo_model.pth)')
    parser.add_argument('--load-path', type=str, default=None,
                        help='ëª¨ë¸ ë¡œë“œ ê²½ë¡œ (ì—†ìœ¼ë©´ ìƒˆë¡œ í•™ìŠµ)')
    parser.add_argument('--save-frequency', type=int, default=10000,
                        help='ëª¨ë¸ ì €ì¥ ì£¼ê¸° (ê¸°ë³¸: 10000)')
    
    # ëª¨ë‹ˆí„°ë§
    parser.add_argument('--tensorboard', action='store_true', default=True,
                        help='TensorBoard ë¡œê¹… í™œì„±í™” (ê¸°ë³¸: True)')
    parser.add_argument('--no-tensorboard', dest='tensorboard', action='store_false',
                        help='TensorBoard ë¹„í™œì„±í™”')
    parser.add_argument('--log-dir', type=str, default='runs',
                        help='TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: runs)')
    
    # ëª¨ë“œ
    parser.add_argument('--mode', choices=['train', 'test'], default='train',
                        help='ì‹¤í–‰ ëª¨ë“œ: train(í•™ìŠµ) ë˜ëŠ” test(í…ŒìŠ¤íŠ¸)')
    parser.add_argument('--test-episodes', type=int, default=5,
                        help='í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 5)')
    
    # í™˜ê²½ ì„ íƒ
    parser.add_argument('--env-type', choices=['real', 'sim', 'carracing'], default='carracing',
                        help='í™˜ê²½ íƒ€ì…: real(ì‹¤ì œ í•˜ë“œì›¨ì–´-ì¶”ë¡ ì „ìš©), sim(ì‹œë®¬ë ˆì´ì…˜), carracing(Gym CarRacing ì‚¬ì „í•™ìŠµ-ê¶Œì¥)')
    parser.add_argument('--use-extended-actions', action='store_true', default=True,
                        help='í™•ì¥ëœ ì•¡ì…˜ ê³µê°„ ì‚¬ìš© (ì „ì§„/í›„ì§„, ì¢ŒíšŒì „/ìš°íšŒì „) - ì—°ì† ì•¡ì…˜ ëª¨ë“œ')
    parser.add_argument('--use-discrete-actions', action='store_true', default=True,
                        help='ì´ì‚° ì•¡ì…˜ ê³µê°„ ì‚¬ìš© (ê¸°ë³¸ê°’, CarRacing: 0-4)')
    parser.add_argument('--use-continuous-actions', dest='use_discrete_actions', action='store_false',
                        help='ì—°ì† ì•¡ì…˜ ê³µê°„ ì‚¬ìš© (ì´ì‚° ì•¡ì…˜ ë¹„í™œì„±í™”)')
    parser.add_argument('--render', action='store_true',
                        help='í™˜ê²½ ë Œë”ë§ (ì‹œë®¬ë ˆì´ì…˜/CarRacing ëª¨ë“œì—ì„œë§Œ)')
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None,
                        help='ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™ ì„ íƒ)')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"í™˜ê²½ íƒ€ì…: {args.env_type}")
    print(f"í™•ì¥ëœ ì•¡ì…˜ ê³µê°„: {args.use_extended_actions}")
    
    # í™˜ê²½ ìƒì„±
    if args.env_type == 'carracing':
        # Gym CarRacing í™˜ê²½ (ì‚¬ì „í•™ìŠµìš©)
        try:
            env = CarRacingEnvWrapper(
                max_steps=args.max_episode_steps,
                use_extended_actions=args.use_extended_actions,
                use_discrete_actions=args.use_discrete_actions
            )
            print("=" * 60)
            print("Gym CarRacing í™˜ê²½ ì‚¬ìš© - ì‚¬ì „í•™ìŠµ ê¶Œì¥")
            print("=" * 60)
            if args.render:
                print("ë Œë”ë§ ëª¨ë“œ í™œì„±í™” - í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        except ImportError as e:
            print("=" * 60)
            print("âŒ CarRacing í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("=" * 60)
            print(str(e))
            print("\nëŒ€ì•ˆ: ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì‚¬ìš©")
            print("python train_ppo.py --env-type sim --use-extended-actions")
            print("=" * 60)
            sys.exit(1)
    elif args.env_type == 'sim':
        # ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½
        render_mode = 'human' if args.render else None
        env = RCCarSimEnv(
            max_steps=args.max_episode_steps,
            render_mode=render_mode,
            use_extended_actions=args.use_extended_actions
        )
        print("ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì‚¬ìš© - ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥")
        if args.render:
            print("ë Œë”ë§ ëª¨ë“œ í™œì„±í™” - í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    else:
        # ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ (ì¶”ë¡  ì „ìš©)
        if not HAS_REAL_ENV:
            raise ImportError(
                "ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                "picamera ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¼ì¦ˆë² ë¦¬ íŒŒì´ í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤.\n"
                "ì‚¬ì „í•™ìŠµì„ ìœ„í•´ CarRacing í™˜ê²½ì„ ì‚¬ìš©í•˜ì„¸ìš”: --env-type carracing\n"
                "ë˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½: --env-type sim"
            )
        
        env = RCCarEnv(
            max_steps=args.max_episode_steps,
            use_extended_actions=args.use_extended_actions,
            use_discrete_actions=args.use_discrete_actions
        )
        print("=" * 60)
        print("âš ï¸  ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ ì‚¬ìš©")
        print("âš ï¸  í•™ìŠµ ëª¨ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!")
        print("âš ï¸  í…ŒìŠ¤íŠ¸/ì¶”ë¡  ì „ìš©ì…ë‹ˆë‹¤!")
        print("=" * 60)
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    # ì´ì‚° ì•¡ì…˜ ëª¨ë“œì¸ì§€ í™•ì¸ (ê¸°ë³¸ê°’: True)
    use_discrete = args.use_discrete_actions
    
    agent = PPOAgent(
        state_dim=256,
        action_dim=2 if not use_discrete else 5,  # ì´ì‚° ì•¡ì…˜: 5ê°œ
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=args.carry_latent,
        lr_actor=args.lr_actor,
        lr_critic=args.lr_critic,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_epsilon=args.clip_epsilon,
        entropy_coef=args.entropy_coef,
        value_coef=args.value_coef,
        device=device,
        discrete_action=use_discrete,
        num_discrete_actions=5,
        use_recurrent=args.use_recurrent,
        use_monte_carlo=args.use_mc
    )
    
    # TRM-PPO ëª¨ë“œ ì¶œë ¥
    if args.use_recurrent:
        print(f"TRM-PPO ëª¨ë“œ: n_cycles={args.n_cycles}, latent_dim={args.latent_dim}, carry_latent={args.carry_latent}")
    
    # Monte Carlo ëª¨ë“œ ì¶œë ¥
    if args.use_mc:
        print("=" * 60)
        print("ğŸ“Š Monte Carlo ëª¨ë“œ í™œì„±í™”")
        print("  -> GAE ëŒ€ì‹  ìˆœìˆ˜ ì—í”¼ì†Œë“œ ë¦¬í„´ ì‚¬ìš©")
        if args.mc_update_on_done:
            print("  -> ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œì—ë§Œ ì—…ë°ì´íŠ¸")
        print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    if args.load_path:
        agent.load(args.load_path)
    
    # í•™ìŠµ ë˜ëŠ” í…ŒìŠ¤íŠ¸
    if args.mode == 'train':
        train_ppo(
            env=env,
            agent=agent,
            total_steps=args.total_steps,
            max_episode_steps=args.max_episode_steps,
            update_frequency=args.update_frequency,
            update_epochs=args.update_epochs,
            save_frequency=args.save_frequency,
            save_path=args.save_path,
            use_tensorboard=args.tensorboard,
            log_dir=args.log_dir,
            mc_update_on_done=args.mc_update_on_done
        )
    elif args.mode == 'test':
        if not args.load_path:
            print("ê²½ê³ : í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ëª¨ë¸ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. --load-pathë¥¼ ì§€ì •í•˜ì„¸ìš”.")
        else:
            test_agent(env, agent, num_episodes=args.test_episodes, max_steps=args.max_episode_steps)


if __name__ == "__main__":
    main()

