# Teacher Forcing + Imitation RL í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

í•™ìŠµ ì†ë„ë¥¼ ë³´ì¥í•˜ê³  ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ **Teacher Forcing (Supervised Learning)**ê³¼ **Imitation RL**ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ íŒŒì´í”„ë¼ì¸

```
1. Teacher Forcing (Supervised Learning)
   â†“ ë¹ ë¥¸ ì‚¬ì „ í•™ìŠµ (ì‚¬ëŒì˜ í–‰ë™ íŒ¨í„´ ì§ì ‘ í•™ìŠµ)
   â†“
2. Imitation RL (Reinforcement Learning)
   â†“ Fine-tuning (ì¼ì¹˜ìœ¨ ê¸°ë°˜ ë¦¬ì›Œë“œë¡œ ê°œì„ )
   â†“
3. ìµœì¢… ëª¨ë¸
```

## ğŸ“š README í™•ì¸ ê²°ê³¼

`README_TRAINING_PIPELINE.md`ì— ì´ë¯¸ íŒŒì´í”„ë¼ì¸ì´ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

### ì˜ˆì‹œ 1: ì‹œë®¬ë ˆì´ì…˜ ì¤‘ì‹¬ (ê¶Œì¥)

```bash
# 1ë‹¨ê³„: CarRacing í™˜ê²½ì—ì„œ ì‚¬ì „ í•™ìŠµ (ì„ íƒì‚¬í•­)
python train_ppo.py --env-type carracing --total-steps 500000 --save-path ppo_carracing.pth

# 2ë‹¨ê³„: ì‚¬ëŒ ë°ëª¨ ë°ì´í„° ìˆ˜ì§‘
python collect_human_demonstrations.py \
    --env-type real \
    --port /dev/ttyACM0 \
    --episodes 5 \
    --output human_demos.pkl

# 3ë‹¨ê³„: Teacher Forcing ì‚¬ì „ í•™ìŠµ
python train_with_teacher_forcing.py \
    --demos human_demos.pkl \
    --pretrain-epochs 100 \
    --pretrain-save pretrained_model.pth

# 4ë‹¨ê³„: Imitation RL Fine-tuning
python train_imitation_rl.py \
    --demos human_demos.pkl \
    --model pretrained_model.pth \
    --epochs 20 \
    --save imitation_rl_model.pth
```

## âœ… ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€

**ë„¤, ì§€ê¸ˆ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤!**

### 1. Teacher Forcing ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰
python3 train_with_teacher_forcing.py \
    --demos uploaded_data/demos_20251129_164827.pkl \
    --pretrain-epochs 100 \
    --pretrain-save pretrained_model.pth

# ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°
python3 train_with_teacher_forcing.py \
    --demos uploaded_data/demos_20251129_164827.pkl \
    --pretrain-epochs 50 \
    --pretrain-batch-size 64 \
    --pretrain-lr 3e-4 \
    --pretrain-save pretrained_model.pth
```

**ì£¼ìš” íŒŒë¼ë¯¸í„°:**
- `--demos`: ë°ëª¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (í•„ìˆ˜)
- `--pretrain-epochs`: ì‚¬ì „ í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 0, 0ì´ë©´ ìƒëµ)
- `--pretrain-batch-size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)
- `--pretrain-lr`: í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)
- `--pretrain-save`: ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: `pretrained_model.pth`)

### 2. Imitation RL ì‹¤í–‰ (Teacher Forcing ëª¨ë¸ ì‚¬ìš©)

```bash
# Teacher Forcingìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Imitation RL
python3 train_imitation_rl.py \
    --demos uploaded_data/demos_20251129_164827.pkl \
    --model pretrained_model.pth \
    --epochs 20 \
    --batch-size 64 \
    --learning-rate 3e-4 \
    --save imitation_rl_model.pth
```

### 3. ì„œë²„ APIë¥¼ í†µí•œ ì‹¤í–‰

```bash
# 1. Teacher Forcing í•™ìŠµ ìš”ì²­
python3 client_upload.py \
    --server http://SERVER_IP:5000 \
    --train-supervised uploaded_data/demos_XXX.pkl \
    --pretrain-epochs 100

# 2. Imitation RL í•™ìŠµ ìš”ì²­ (Teacher Forcing ëª¨ë¸ ì‚¬ìš©)
python3 client_upload.py \
    --server http://SERVER_IP:5000 \
    --train uploaded_data/demos_XXX.pkl \
    --pretrain-model pretrained_model.pth \
    --epochs 20 \
    --batch-size 64 \
    --learning-rate 3e-4
```

## ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš°

### ë‹¨ê³„ë³„ ì„¤ëª…

#### 1ë‹¨ê³„: Teacher Forcing (Supervised Learning)

**ëª©ì **: ì‚¬ëŒì˜ í–‰ë™ íŒ¨í„´ì„ ë¹ ë¥´ê²Œ í•™ìŠµ

**ë°©ì‹**:
- ì‚¬ëŒì´ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒì„ ì‚¬ìš©
- Maximum Likelihood Estimation (MLE)
- ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”
- ë¹ ë¥¸ ìˆ˜ë ´ (ì¼ë°˜ì ìœ¼ë¡œ 50-100 ì—í¬í¬ë©´ ì¶©ë¶„)

**ì˜ˆìƒ ê²°ê³¼**:
- Match Rate: 60-80% (ì´ˆê¸°)
- ë¹ ë¥¸ í•™ìŠµ (ìˆ˜ ë¶„ ~ ìˆ˜ì‹­ ë¶„)

#### 2ë‹¨ê³„: Imitation RL (Reinforcement Learning)

**ëª©ì **: Teacher Forcing ëª¨ë¸ì„ Fine-tuningí•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ ë‹¬ì„±

**ë°©ì‹**:
- Teacher Forcingìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
- ëª¨ë¸ ì•¡ì…˜ê³¼ ì „ë¬¸ê°€ ì•¡ì…˜ì˜ ì¼ì¹˜ìœ¨ì„ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©
- PPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµ
- ë” ì •ë°€í•œ ì¡°ì •

**ì˜ˆìƒ ê²°ê³¼**:
- Match Rate: 80-95% (ê°œì„ )
- ë” ì•ˆì •ì ì¸ ì •ì±…

## ğŸ“Š ë¹„êµ: Teacher Forcing vs Imitation RL

| ë°©ë²• | í•™ìŠµ ì†ë„ | ì´ˆê¸° ì„±ëŠ¥ | ìµœì¢… ì„±ëŠ¥ | ë°ì´í„° íš¨ìœ¨ |
|-----|---------|---------|---------|-----------|
| **Teacher Forcingë§Œ** | ë§¤ìš° ë¹ ë¦„ | ë†’ìŒ (60-80%) | ì¤‘ê°„ (70-85%) | ë†’ìŒ |
| **Imitation RLë§Œ** | ëŠë¦¼ | ë‚®ìŒ (35-40%) | ë†’ìŒ (80-95%) | ì¤‘ê°„ |
| **Teacher Forcing â†’ Imitation RL** | ë¹ ë¦„ | ë†’ìŒ (60-80%) | ë§¤ìš° ë†’ìŒ (85-95%) | ë§¤ìš° ë†’ìŒ |

## ğŸ¯ ê¶Œì¥ ì„¤ì •

### Teacher Forcing
```bash
python3 train_with_teacher_forcing.py \
    --demos human_demos.pkl \
    --pretrain-epochs 50-100 \
    --pretrain-batch-size 64 \
    --pretrain-lr 3e-4 \
    --pretrain-save pretrained_model.pth
```

### Imitation RL (Teacher Forcing ëª¨ë¸ ì‚¬ìš©)
```bash
python3 train_imitation_rl.py \
    --demos human_demos.pkl \
    --model pretrained_model.pth \
    --epochs 20-50 \
    --batch-size 64 \
    --learning-rate 3e-4 \
    --save imitation_rl_model.pth
```

## ğŸ’¡ ì™œ ì´ ì¡°í•©ì´ íš¨ê³¼ì ì¸ê°€?

1. **Teacher Forcing**: ë¹ ë¥´ê²Œ ê¸°ë³¸ íŒ¨í„´ í•™ìŠµ (Supervised Learningì˜ ì¥ì )
2. **Imitation RL**: ì„¸ë°€í•œ ì¡°ì • ë° ê°œì„  (Reinforcement Learningì˜ ì¥ì )
3. **ì‹œë„ˆì§€**: ë‘ ë°©ë²•ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ë¹ ë¥´ê³  íš¨ê³¼ì ì¸ í•™ìŠµ

---

# Human Demonstration ìˆ˜ì§‘ ì‹œ ë³´ìƒ ì¸¡ì • ë°©ë²•

## ğŸ“‹ ê°œìš”

`collect_human_demonstrations.py`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ëŒì´ ì§ì ‘ ì¡°ì‘í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ë•Œ, í™˜ê²½ì—ì„œ ìë™ìœ¼ë¡œ ë³´ìƒ(reward)ì„ ê³„ì‚°í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

## ğŸ” ë³´ìƒ ê³„ì‚° ë°©ì‹

### 1. ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ (`rc_car_env.py`)

ì‹¤ì œ RC Car í™˜ê²½ì—ì„œ ë³´ìƒì€ **ì¹´ë©”ë¼ ì´ë¯¸ì§€**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:

```python
def _compute_reward(self, img, action):
    """
    ë¦¬ì›Œë“œ ê³„ì‚° (rc_car_env.py)
    
    ê³„ì‚° ìš”ì†Œ:
    1. ì°¨ì„  ì¶”ì  ë¦¬ì›Œë“œ: ì´ë¯¸ì§€ ì¤‘ì•™ ì˜ì—­ì˜ ë°ê¸° (ì°¨ì„ ì´ ìˆìœ¼ë©´ ë³´ìƒ)
    2. ì†ë„ ìœ ì§€ ë¦¬ì›Œë“œ: ì ë‹¹í•œ ì†ë„ ìœ ì§€ (0.5 ê·¼ì²˜ì—ì„œ ìµœëŒ€)
    3. ì•ˆì •ì„± ë¦¬ì›Œë“œ: ì´ì „ ì´ë¯¸ì§€ì™€ì˜ ìœ ì‚¬ì„± (ì•ˆì •ì ì¸ ì£¼í–‰)
    4. ë°©í–¥ ì¼ê´€ì„± ë¦¬ì›Œë“œ: ì§ì§„ ì„ í˜¸
    5. í˜ë„í‹°: ë„ˆë¬´ ëŠë¦¬ê±°ë‚˜ ë©ˆì¶¤
    6. ì „ì§„ ì•¡ì…˜ ë³´ë„ˆìŠ¤: ì „ì§„ ì•¡ì…˜ì— ì‘ì€ ë³´ë„ˆìŠ¤
    """
```

**êµ¬ì²´ì ì¸ ê³„ì‚°:**

1. **ì°¨ì„  ì¶”ì  ë¦¬ì›Œë“œ** (ìµœëŒ€ 0.5)
   ```python
   center_region = img[6:10, 6:10]  # ì¤‘ì•™ 4x4 ì˜ì—­
   center_brightness = np.mean(center_region) / 255.0
   lane_reward = center_brightness * 0.5
   ```
   - ì¤‘ì•™ì´ ë°ì„ìˆ˜ë¡ (ì°¨ì„ ì´ ë³´ì¼ìˆ˜ë¡) ë†’ì€ ë³´ìƒ

2. **ì†ë„ ìœ ì§€ ë¦¬ì›Œë“œ** (ìµœëŒ€ 0.3)
   ```python
   speed = np.mean([abs(action[0]), abs(action[1])])
   speed_reward = -abs(speed - 0.5) * 0.3
   ```
   - ì†ë„ê°€ 0.5 ê·¼ì²˜ì¼ ë•Œ ìµœëŒ€ ë³´ìƒ

3. **ì•ˆì •ì„± ë¦¬ì›Œë“œ** (ìµœëŒ€ 0.2)
   ```python
   stability = 1.0 - np.mean(np.abs(img - last_image)) / 255.0
   stability_reward = stability * 0.2
   ```
   - ì´ì „ í”„ë ˆì„ê³¼ ìœ ì‚¬í• ìˆ˜ë¡ ë†’ì€ ë³´ìƒ

4. **ë°©í–¥ ì¼ê´€ì„± ë¦¬ì›Œë“œ** (ìµœëŒ€ 0.1)
   ```python
   direction_diff = abs(action[0] - action[1])
   direction_reward = (1.0 - direction_diff) * 0.1
   ```
   - ì§ì§„í• ìˆ˜ë¡ ë†’ì€ ë³´ìƒ

5. **í˜ë„í‹°** (-0.5)
   ```python
   if speed < 0.1:
       reward -= 0.5
   ```
   - ë„ˆë¬´ ëŠë¦¬ê±°ë‚˜ ë©ˆì¶”ë©´ í˜ë„í‹°

6. **ì „ì§„ ì•¡ì…˜ ë³´ë„ˆìŠ¤** (+0.1)
   ```python
   if use_discrete_actions and abs(action[0]) > 0.5:
       reward += 0.1
   ```

**ì´ ë³´ìƒ ë²”ìœ„**: ì•½ -0.5 ~ 1.2

### 2. CarRacing í™˜ê²½

CarRacing í™˜ê²½ì—ì„œëŠ” Gymì´ ì œê³µí•˜ëŠ” ê¸°ë³¸ ë³´ìƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
- ì°¨ì„  ìœ ì§€: ì–‘ìˆ˜ ë³´ìƒ
- íŠ¸ë™ ì´íƒˆ: ìŒìˆ˜ ë³´ìƒ
- ì†ë„: ì–‘ìˆ˜ ë³´ìƒ

### 3. ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½

ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½(`rc_car_sim_env.py`)ì—ì„œë„ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë³´ìƒì„ ê³„ì‚°í•©ë‹ˆë‹¤.

## ğŸ“ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •

### ìˆ˜ì§‘ íë¦„

```
1. ì‚¬ìš©ìê°€ í‚¤ë³´ë“œë¡œ ì•¡ì…˜ ì…ë ¥ (w/a/s/d/x)
   â†“
2. í˜„ì¬ ìƒíƒœ(ì¹´ë©”ë¼ ì´ë¯¸ì§€) ì €ì¥
   â†“
3. í™˜ê²½ì— ì•¡ì…˜ ì „ë‹¬: env.step(action)
   â†“
4. í™˜ê²½ì´ ë³´ìƒ ê³„ì‚°: _compute_reward(img, action)
   â†“
5. ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, done, info ë°˜í™˜
   â†“
6. ëª¨ë“  ë°ì´í„° ì €ì¥:
   - states: ìƒíƒœ (ì´ë¯¸ì§€)
   - actions: ì•¡ì…˜
   - rewards: ë³´ìƒ (í™˜ê²½ì—ì„œ ê³„ì‚°)
   - dones: ì¢…ë£Œ í”Œë˜ê·¸
   - timestamps: íƒ€ì„ìŠ¤íƒ¬í”„
```

### ì½”ë“œ ì˜ˆì‹œ

```python
# collect_human_demonstrations.pyì˜ collect_episode ë©”ì„œë“œ

# í™˜ê²½ ìŠ¤í… ì‹¤í–‰
next_state, reward, done, info = self.env.step(action)

# ë°ì´í„° ì €ì¥
episode_data['actions'].append(action)
episode_data['rewards'].append(reward)  # â† í™˜ê²½ì—ì„œ ê³„ì‚°ëœ ë³´ìƒ
episode_data['dones'].append(done)
```

## ğŸ” ë³´ìƒ í™•ì¸ ë°©ë²•

### 1. ìˆ˜ì§‘ ì¤‘ í™•ì¸

ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ë³´ìƒ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤:

```
ì—í”¼ì†Œë“œ ì™„ë£Œ:
  ê¸¸ì´: 250 ìŠ¤í…
  ì´ ë¦¬ì›Œë“œ: 45.320
  í‰ê·  ë¦¬ì›Œë“œ: 0.181
```

### 2. ì €ì¥ëœ ë°ì´í„° í™•ì¸

```python
import pickle
import numpy as np

# ë°ì´í„° ë¡œë“œ
with open('human_demos.pkl', 'rb') as f:
    data = pickle.load(f)

# ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œì˜ ë³´ìƒ í™•ì¸
episode = data['demonstrations'][0]
rewards = episode['rewards']

print(f"ì—í”¼ì†Œë“œ ê¸¸ì´: {len(rewards)}")
print(f"ì´ ë³´ìƒ: {sum(rewards):.2f}")
print(f"í‰ê·  ë³´ìƒ: {np.mean(rewards):.3f}")
print(f"ìµœëŒ€ ë³´ìƒ: {max(rewards):.3f}")
print(f"ìµœì†Œ ë³´ìƒ: {min(rewards):.3f}")
```

## âš ï¸ ì¤‘ìš” ì‚¬í•­

### Imitation Learningì—ì„œ ë³´ìƒ ì‚¬ìš©

**ì¤‘ìš”**: `train_imitation_rl.py`ëŠ” **í™˜ê²½ì˜ ë³´ìƒì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**.

- Imitation RLì€ ëª¨ë¸ ì•¡ì…˜ê³¼ ì „ë¬¸ê°€ ì•¡ì…˜ì˜ **ì¼ì¹˜ìœ¨**ì„ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©
- ì¼ì¹˜: +1.0
- ë¶ˆì¼ì¹˜: -0.1

ë”°ë¼ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ `rewards` í•„ë“œëŠ”:
- **Teacher Forcing**: ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ìƒíƒœ-ì•¡ì…˜ ìŒë§Œ ì‚¬ìš©)
- **Imitation RL**: ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì¼ì¹˜ìœ¨ ê¸°ë°˜ ë¦¬ì›Œë“œ ì‚¬ìš©)
- **ì¼ë°˜ RL**: ì‚¬ìš© ê°€ëŠ¥ (í™˜ê²½ ë³´ìƒ ì‚¬ìš©)

### ë³´ìƒì´ í•„ìš”í•œ ê²½ìš°

ë§Œì•½ í™˜ê²½ ë³´ìƒì„ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´:
- `train_ppo.py` ì‚¬ìš© (ì¼ë°˜ ê°•í™”í•™ìŠµ)
- ë˜ëŠ” `train_with_teacher_forcing.py`ì˜ `--rl-steps` ì˜µì…˜ ì‚¬ìš©

## ğŸ“Š ë³´ìƒ í†µê³„ ì˜ˆì‹œ

```python
# ëª¨ë“  ì—í”¼ì†Œë“œì˜ ë³´ìƒ í†µê³„
all_rewards = []
for episode in data['demonstrations']:
    all_rewards.extend(episode['rewards'])

print(f"ì „ì²´ í†µê³„:")
print(f"  ì´ ìŠ¤í…: {len(all_rewards)}")
print(f"  í‰ê·  ë³´ìƒ: {np.mean(all_rewards):.3f}")
print(f"  í‘œì¤€í¸ì°¨: {np.std(all_rewards):.3f}")
print(f"  ìµœëŒ€ ë³´ìƒ: {max(all_rewards):.3f}")
print(f"  ìµœì†Œ ë³´ìƒ: {min(all_rewards):.3f}")
```

## ğŸ¯ ìš”ì•½

1. **ë³´ìƒì€ ìë™ìœ¼ë¡œ ê³„ì‚°ë¨**: `env.step(action)` í˜¸ì¶œ ì‹œ í™˜ê²½ì´ ìë™ ê³„ì‚°
2. **ì‹¤ì œ í•˜ë“œì›¨ì–´**: ì¹´ë©”ë¼ ì´ë¯¸ì§€ ê¸°ë°˜ ë³´ìƒ (ì°¨ì„  ì¶”ì , ì†ë„, ì•ˆì •ì„± ë“±)
3. **CarRacing/ì‹œë®¬ë ˆì´ì…˜**: ê° í™˜ê²½ì˜ ê¸°ë³¸ ë³´ìƒ ì‚¬ìš©
4. **Imitation RLì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨**: ì¼ì¹˜ìœ¨ ê¸°ë°˜ ë¦¬ì›Œë“œ ì‚¬ìš©
5. **ë°ì´í„°ì—ëŠ” ì €ì¥ë¨**: ë‚˜ì¤‘ì— ë¶„ì„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ìš©ë„ë¡œ ì‚¬ìš© ê°€ëŠ¥

