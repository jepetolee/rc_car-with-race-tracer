#!/usr/bin/env python3
"""
TRM ê¸°ë°˜ Multi-Worker DQN í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (A3C ìŠ¤íƒ€ì¼)
ì—¬ëŸ¬ workerê°€ ë³‘ë ¬ë¡œ í™˜ê²½ì„ ì‹¤í–‰í•˜ë©° ê°ì í•™ìŠµí•˜ê³ , ì£¼ê¸°ì ìœ¼ë¡œ global agentì™€ ë™ê¸°í™”
"""

import argparse
import os
import time
from collections import deque
from datetime import datetime
from multiprocessing import Process, Queue, Manager
import copy

import numpy as np
import torch
import torch.multiprocessing as mp

from car_racing_env import CarRacingEnvWrapper
from ppo_agent import DQNAgent

try:
    from torch.utils.tensorboard import SummaryWriter

    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False


def linear_epsilon(step, start, end, decay_steps):
    if decay_steps <= 0:
        return end
    return max(end, start - (start - end) * (step / decay_steps))


def worker_process(
    worker_id: int,
    args,
    global_step_counter,
    episode_rewards_queue: Queue,
    global_weights_queue: Queue,
    worker_weights_queue: Queue,  # Workerê°€ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ ì „ì†¡í•˜ëŠ” Queue
    lock,
    device: str = "cpu",
):
    """ê° ì›Œì»¤ê°€ ë…ë¦½ì ìœ¼ë¡œ í™˜ê²½ì„ ì‹¤í–‰í•˜ë©° í•™ìŠµ"""
    print(f"[Worker {worker_id}] ì‹œì‘ (Device: {device})...", flush=True)
    print(f"[Worker {worker_id}] í™˜ê²½ ì´ˆê¸°í™” ì¤‘...", flush=True)
    env = CarRacingEnvWrapper(
        max_steps=args.max_episode_steps,
        use_extended_actions=True,
        use_discrete_actions=True,
    )
    print(f"[Worker {worker_id}] í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ", flush=True)

    # Workerë³„ ë¡œì»¬ DQN
    print(f"[Worker {worker_id}] DQN Agent ìƒì„± ì¤‘ (Device: {device})...", flush=True)
    if device.startswith("cuda"):
        # ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ê°™ì€ GPUë¥¼ ê³µìœ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ í• ë‹¹ì„ ì¡°ì ˆ
        torch.cuda.set_device(0)  # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
    local_agent = DQNAgent(
        state_dim=args.state_dim,
        action_dim=args.action_dim,
        hidden_dim=args.hidden_dim,
        latent_dim=args.latent_dim,
        gamma=args.gamma,
        lr=args.learning_rate,
        device=device,
        buffer_size=args.replay_buffer // args.num_workers,  # Workerë‹¹ ë²„í¼ í¬ê¸°
        batch_size=args.batch_size,
        target_update_interval=args.target_update_interval,
        n_deep_loops=args.n_deep_loops,
        n_latent_loops=args.n_latent_loops,
        max_grad_norm=args.max_grad_norm,
    )
    print(f"[Worker {worker_id}] DQN Agent ìƒì„± ì™„ë£Œ. í•™ìŠµ ì‹œì‘!", flush=True)

    episode = 0
    local_steps = 0
    sync_counter = 0

    while True:
        with lock:
            total_steps = global_step_counter.value
        if total_steps >= args.max_steps:
            break

        # ì—í”¼ì†Œë“œ ì‹œì‘ ì „ì— í•­ìƒ Global weights ë‹¤ìš´ë¡œë“œ (ìµœì‹  ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        if not global_weights_queue.empty():
            try:
                global_weights = global_weights_queue.get_nowait()
                local_agent.q_network.load_state_dict(global_weights)
                local_agent.target_network.load_state_dict(global_weights)
            except:
                pass  # Queueê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ì´ì „ ê°€ì¤‘ì¹˜ ìœ ì§€

        # ì—í”¼ì†Œë“œ ì‹œì‘
        reset_result = env.reset()
        if isinstance(reset_result, tuple):
            state, _ = reset_result
        else:
            state = reset_result

        state = state.astype(np.float32).reshape(-1) / 255.0
        done = False
        episode_reward = 0
        step = 0

        while not done and step < args.max_episode_steps:
            # Epsilon ê³„ì‚°
            epsilon = linear_epsilon(
                total_steps, args.eps_start, args.eps_end, args.eps_decay
            )

            # ì•¡ì…˜ ì„ íƒ
            action = local_agent.select_action(state, epsilon=epsilon)

            # í™˜ê²½ ìŠ¤í…
            next_state, reward, done, _ = env.step(action)
            next_state_norm = next_state.astype(np.float32).reshape(-1) / 255.0

            # ë¡œì»¬ replay bufferì— ì €ì¥
            local_agent.store_transition(state.copy(), action, reward, next_state_norm.copy(), done)

            # ë¡œì»¬ í•™ìŠµ
            if len(local_agent.replay_buffer) >= args.batch_size:
                local_agent.update()

            state = next_state_norm
            episode_reward += reward
            step += 1
            local_steps += 1
            sync_counter += 1

            # Global step counter ì—…ë°ì´íŠ¸
            with lock:
                global_step_counter.value += 1

        episode += 1

        # ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ ì „ì†¡
        episode_rewards_queue.put((worker_id, episode, episode_reward, local_steps))
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ coordinatorì—ê²Œ ì „ì†¡ (A3C ìŠ¤íƒ€ì¼)
        try:
            weights = copy.deepcopy(local_agent.q_network.state_dict())
            worker_weights_queue.put((worker_id, episode, weights), block=False)
        except:
            pass  # Queueê°€ ê°€ë“ ì°¨ë©´ ìŠ¤í‚µ (non-blocking)

        # ëª¨ë“  ì—í”¼ì†Œë“œ ë¡œê·¸ ì¶œë ¥ (ì²« 10ê°œ, ê·¸ í›„ 10ê°œë§ˆë‹¤)
        if episode <= 10 or episode % 10 == 0:
            print(
                f"[Worker {worker_id}] Episode {episode} | "
                f"Reward: {episode_reward:.2f} | Local Steps: {local_steps} | "
                f"Buffer: {len(local_agent.replay_buffer)}/{local_agent.replay_buffer.capacity}",
                flush=True
            )

    env.close()


def coordinator_process(
    args,
    global_step_counter,
    episode_rewards_queue: Queue,
    global_weights_queue: Queue,
    worker_weights_queue: Queue,  # Workerê°€ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ ë°›ëŠ” Queue
    lock,
    device: str = "cuda",
):
    """Coordinatorê°€ global agentë¥¼ ê´€ë¦¬í•˜ê³  ëª¨ë“  ì›Œì»¤ì™€ ë™ê¸°í™”"""
    print("[Coordinator] Global Agent ì´ˆê¸°í™” ì¤‘...", flush=True)
    # Global agent (ëª¨ë“  ì›Œì»¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì§‘ê³„)
    global_agent = DQNAgent(
        state_dim=args.state_dim,
        action_dim=args.action_dim,
        hidden_dim=args.hidden_dim,
        latent_dim=args.latent_dim,
        gamma=args.gamma,
        lr=args.learning_rate,
        device=device,
        buffer_size=0,  # CoordinatorëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ
        batch_size=args.batch_size,
        target_update_interval=args.target_update_interval,
        n_deep_loops=args.n_deep_loops,
        n_latent_loops=args.n_latent_loops,
        max_grad_norm=args.max_grad_norm,
    )

    writer = None
    if args.use_tensorboard and HAS_TENSORBOARD:
        log_dir = os.path.join(
            "runs", f"dqn_multi_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        writer = SummaryWriter(log_dir)
        print(f"TensorBoard: {log_dir}")

    recent_rewards = deque(maxlen=100)
    last_sync = 0
    best_avg = float("-inf")
    best_model_path = os.path.join(args.save_dir, "dqn_multi_best.pth")
    os.makedirs(args.save_dir, exist_ok=True)
    print("[Coordinator] ì‹œì‘ë¨. ì›Œì»¤ë“¤ë¡œë¶€í„° ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ ìˆ˜ì§‘ ì¤‘...", flush=True)

    while True:
        with lock:
            total_steps = global_step_counter.value
        if total_steps >= args.max_steps:
            break

        # Workerë¡œë¶€í„° í•™ìŠµí•œ ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
        # ê° ì›Œì»¤ê°€ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œë§ˆë‹¤ (worker_id, episode, weights)ë¥¼ ì „ì†¡
        collected_updates = []  # (worker_id, episode, weights) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        while not worker_weights_queue.empty():
            try:
                update = worker_weights_queue.get_nowait()
                collected_updates.append(update)
            except:
                break
        
        # Worker ê°€ì¤‘ì¹˜ ì§‘ê³„ ë° Global Agent ì—…ë°ì´íŠ¸ (A3C ìŠ¤íƒ€ì¼)
        # ì§‘ê³„ ë°©ì‹: ëª¨ë“  ì›Œì»¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ íŒŒë¼ë¯¸í„°ë³„ë¡œ í‰ê· í™”
        if collected_updates:
            # ê° ì›Œì»¤ì˜ ìµœì‹  ê°€ì¤‘ì¹˜ë§Œ ì‚¬ìš© (ê°™ì€ ì›Œì»¤ê°€ ì—¬ëŸ¬ ë²ˆ ë³´ë‚¸ ê²½ìš° ìµœì‹  ê²ƒë§Œ)
            latest_weights_per_worker = {}
            for worker_id, episode, weights in collected_updates:
                if worker_id not in latest_weights_per_worker or episode > latest_weights_per_worker[worker_id][0]:
                    latest_weights_per_worker[worker_id] = (episode, weights)
            
            # A3C ìŠ¤íƒ€ì¼: ëª¨ë“  ì›Œì»¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ í‰ê· í™”
            # ì§‘ê³„ ë°©ì‹: ê° íŒŒë¼ë¯¸í„°(weight, bias ë“±)ì— ëŒ€í•´ ëª¨ë“  ì›Œì»¤ì˜ ê°’ì„ í‰ê· 
            # ì˜ˆ: W_global = (W_worker1 + W_worker2 + ... + W_workerN) / N
            if len(latest_weights_per_worker) > 0:
                avg_weights = {}
                for param_name in global_agent.q_network.state_dict().keys():
                    param_tensors = [weights[param_name] for _, weights in latest_weights_per_worker.values()]
                    # ëª¨ë“  ì›Œì»¤ì˜ íŒŒë¼ë¯¸í„° í…ì„œë¥¼ ìŠ¤íƒí•˜ê³  í‰ê·  ê³„ì‚°
                    avg_param = torch.stack(param_tensors).mean(dim=0)
                    avg_weights[param_name] = avg_param
                
                # Global agentì— í‰ê· í™”ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
                global_agent.q_network.load_state_dict(avg_weights)
                global_agent.target_network.load_state_dict(avg_weights)
                
                worker_ids = list(latest_weights_per_worker.keys())
                print(
                    f"[Coordinator] Worker ê°€ì¤‘ì¹˜ ì§‘ê³„ ì™„ë£Œ | "
                    f"Workers: {worker_ids} ({len(worker_ids)}ê°œ) | "
                    f"Steps: {total_steps}",
                    flush=True
                )
        
        # ì—í”¼ì†Œë“œë§ˆë‹¤ global weightsë¥¼ workerë“¤ì—ê²Œ ì „ì†¡ (í•­ìƒ ìµœì‹  ê°€ì¤‘ì¹˜ ìœ ì§€)
        current_weights = copy.deepcopy(global_agent.q_network.state_dict())
        # Queue ë¹„ìš°ê³  ìµœì‹  weights ë„£ê¸°
        while not global_weights_queue.empty():
            try:
                global_weights_queue.get_nowait()
            except:
                break
        global_weights_queue.put(current_weights)

        # ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ ìˆ˜ì§‘
        collected = 0
        while not episode_rewards_queue.empty() and collected < 100:
            try:
                worker_id, episode, reward, local_steps = episode_rewards_queue.get_nowait()
                recent_rewards.append(reward)
                collected += 1
                
                # ì²˜ìŒ 20ê°œ ì—í”¼ì†Œë“œëŠ” ëª¨ë‘ ì¶œë ¥, ì´í›„ëŠ” 10ê°œë§ˆë‹¤
                total_episodes = len(recent_rewards)
                if total_episodes <= 20 or total_episodes % 10 == 0:
                    avg_reward = np.mean(recent_rewards) if recent_rewards else 0.0
                    print(
                        f"[Coordinator] Worker {worker_id} Ep{episode} | "
                        f"Reward: {reward:.2f} | Avg(100): {avg_reward:.2f} | "
                        f"Steps: {total_steps}",
                        flush=True
                    )

                # Best model ê°±ì‹  (ì¶©ë¶„í•œ ì—í”¼ì†Œë“œê°€ ëª¨ì˜€ì„ ë•Œë§Œ)
                if len(recent_rewards) >= 10:
                    avg_reward = np.mean(recent_rewards)
                    
                    # TensorBoard ë¡œê¹…
                    if writer:
                        writer.add_scalar("Eval/EpisodeReward", reward, total_steps)
                        writer.add_scalar("Eval/AvgReward100", avg_reward, total_steps)
                        writer.add_scalar("Train/GlobalSteps", total_steps, total_steps)
                        writer.add_scalar("Eval/BestAvgReward", best_avg, total_steps)
                    
                    # Best model ê°±ì‹  (í‰ê·  ë¦¬ì›Œë“œê°€ ê°œì„ ë˜ë©´ ì €ì¥)
                    if avg_reward > best_avg:
                        old_best = best_avg
                        best_avg = avg_reward
                        global_agent.save(best_model_path)
                        print(
                            f"[Coordinator] ğŸ† Best Model ê°±ì‹ ! | "
                            f"Avg Reward: {avg_reward:.2f} (ì´ì „: {old_best:.2f}) | "
                            f"Steps: {total_steps} | ì €ì¥: {best_model_path}",
                            flush=True
                        )

            except:
                break

        time.sleep(0.1)  # CPU ì‚¬ìš©ë¥  ì¡°ì ˆ

    # í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ëª¨ë¸ ì €ì¥ (Best modelê³¼ ë³„ë„)
    final_save_path = os.path.join(
        args.save_dir,
        f"dqn_multi_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth",
    )
    global_agent.save(final_save_path)
    avg_reward = np.mean(recent_rewards) if recent_rewards else 0.0
    print(
        f"[Coordinator] í•™ìŠµ ì¢…ë£Œ | "
        f"ìµœì¢… ëª¨ë¸: {final_save_path} | "
        f"Best ëª¨ë¸: {best_model_path} (Avg Reward: {best_avg:.2f}) | "
        f"ìµœì¢… Avg Reward: {avg_reward:.2f} | "
        f"Steps: {global_step_counter.value}",
        flush=True
    )

    if writer:
        writer.close()


def train(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("=" * 60)
    print("Multi-Worker TRM-DQN í•™ìŠµ ì‹œì‘ (A3C ìŠ¤íƒ€ì¼)")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ì›Œì»¤ ìˆ˜: {args.num_workers}")
    print(f"ìƒíƒœ ì°¨ì›: {args.state_dim}")
    print(f"ì•¡ì…˜ ê°œìˆ˜: {args.action_dim}")
    print(f"ìµœëŒ€ ìŠ¤í…: {args.max_steps}")
    print(f"ë™ê¸°í™” ê°„ê²©: {args.sync_interval} ìŠ¤í…")
    print("=" * 60)

    # Shared objects
    manager = Manager()
    global_step_counter = manager.Value("i", 0)
    lock = manager.Lock()
    episode_rewards_queue = Queue()
    global_weights_queue = Queue()
    worker_weights_queue = Queue()  # Workerê°€ í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ ì „ì†¡í•˜ëŠ” Queue

    # Processes
    processes = []

    # Coordinator process (global agent ê´€ë¦¬) - í•™ìŠµí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ CPU ì‚¬ìš©
    coordinator_p = Process(
        target=coordinator_process,
        args=(
            args,
            global_step_counter,
            episode_rewards_queue,
            global_weights_queue,
            worker_weights_queue,  # Worker ê°€ì¤‘ì¹˜ ìˆ˜ì§‘ìš©
            lock,
            "cpu",  # CoordinatorëŠ” í•™ìŠµí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ CPUë§Œ ì‚¬ìš©
        ),
    )
    coordinator_p.start()
    processes.append(coordinator_p)

    # Worker processes
    for worker_id in range(args.num_workers):
        # ëª¨ë“  ì›Œì»¤ê°€ GPUë¥¼ ê³µìœ í•˜ì—¬ ì‚¬ìš© (PyTorchëŠ” ë©€í‹°í”„ë¡œì„¸ìŠ¤ GPU ê³µìœ  ì§€ì›)
        worker_device = device if torch.cuda.is_available() else "cpu"
        worker_p = Process(
            target=worker_process,
            args=(
                worker_id,
                args,
                global_step_counter,
                episode_rewards_queue,
                global_weights_queue,
                worker_weights_queue,  # Worker ê°€ì¤‘ì¹˜ ì „ì†¡ìš©
                lock,
                worker_device,
            ),
        )
        worker_p.start()
        processes.append(worker_p)

    # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
    try:
        for p in processes:
            p.join()
    except KeyboardInterrupt:
        print("\nâš ï¸  í•™ìŠµ ì¤‘ë‹¨ ì¤‘...")
        for p in processes:
            p.terminate()
            p.join()

    print("í•™ìŠµ ì¢…ë£Œ")


def parse_args():
    parser = argparse.ArgumentParser(description="Multi-Worker TRM-DQN í•™ìŠµ (A3C ìŠ¤íƒ€ì¼)")
    parser.add_argument("--state-dim", type=int, default=784)
    parser.add_argument("--action-dim", type=int, default=5)
    parser.add_argument("--hidden-dim", type=int, default=256)
    parser.add_argument("--latent-dim", type=int, default=256)
    parser.add_argument("--n-deep-loops", type=int, default=2)
    parser.add_argument("--n-latent-loops", type=int, default=2)
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--learning-rate", type=float, default=3e-4)
    parser.add_argument("--batch-size", type=int, default=128)
    parser.add_argument("--replay-buffer", type=int, default=200_000)
    parser.add_argument("--target-update-interval", type=int, default=2000)
    parser.add_argument("--max-grad-norm", type=float, default=1.0)
    parser.add_argument("--max-steps", type=int, default=1_000_000)
    parser.add_argument("--max-episode-steps", type=int, default=1000)
    parser.add_argument(
        "--num-workers", type=int, default=4, help="ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (A3C ìŠ¤íƒ€ì¼)"
    )
    parser.add_argument(
        "--sync-interval",
        type=int,
        default=1000,
        help="Global agentì™€ ë™ê¸°í™” ê°„ê²© (ìŠ¤í…)",
    )
    parser.add_argument("--eps-start", type=float, default=1.0)
    parser.add_argument("--eps-end", type=float, default=0.05)
    parser.add_argument("--eps-decay", type=int, default=300_000)
    parser.add_argument("--save-dir", type=str, default="trained_models")
    parser.add_argument(
        "--save-interval-steps", type=int, default=50000, help="ì €ì¥ ê°„ê²© (ìŠ¤í…)"
    )
    parser.add_argument("--use-tensorboard", action="store_true", default=False)
    return parser.parse_args()


if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)  # Windows/Mac í˜¸í™˜ì„±
    args = parse_args()
    train(args)
