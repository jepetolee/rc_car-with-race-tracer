#!/usr/bin/env python3
"""
A3C (Asynchronous Advantage Actor-Critic) í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ PPOAgentë¥¼ ì¬ì‚¬ìš©í•˜ë©´ì„œ ë³‘ë ¬ í™˜ê²½ ì›Œì»¤ë¡œ í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ
GPU ì‚¬ìš©ë¥  ìµœëŒ€í™” ë° í•™ìŠµ ì†ë„ ê°œì„ 
"""

import argparse
import numpy as np
import torch
import torch.multiprocessing as mp
import time
import os
import sys
import warnings
from collections import deque
from datetime import datetime

# ë¶ˆí•„ìš”í•œ ê²½ê³  ì–µì œ
warnings.filterwarnings('ignore', category=UserWarning, module='pygame')
warnings.filterwarnings('ignore', message='.*Gym has been unmaintained.*')

from car_racing_env import CarRacingEnvWrapper
from ppo_agent import PPOAgent

# TensorBoard ì§€ì›
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False


def worker(worker_id, global_agent, args, global_step, global_episode, 
           global_rewards, best_avg_reward, lock, device):
    """
    A3C ì›Œì»¤ í”„ë¡œì„¸ìŠ¤
    ê¸°ì¡´ PPOAgentë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì›Œì»¤ê°€ ë…ë¦½ì ìœ¼ë¡œ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° í•™ìŠµ
    """
    # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œë„ ê²½ê³  ì–µì œ
    import warnings
    warnings.filterwarnings('ignore', category=UserWarning, module='pygame')
    warnings.filterwarnings('ignore', message='.*Gym has been unmaintained.*')
    
    # ë¡œì»¬ ì—ì´ì „íŠ¸ ìƒì„± (ê¸€ë¡œë²Œê³¼ ë™ì¼í•œ êµ¬ì¡°, ë‹¨ ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ì‚¬ìš© ì•ˆ í•¨)
    # ë¡œì»¬ ì—ì´ì „íŠ¸ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë§Œ ê³„ì‚°í•˜ë¯€ë¡œ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ í•„ìš” ì—†ìŒ
    local_agent = PPOAgent(
        state_dim=args.state_dim,
        action_dim=5,
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=args.carry_latent,
        lr_actor=args.lr_actor,
        lr_critic=args.lr_critic,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_epsilon=args.clip_epsilon,
        value_coef=args.value_coef,
        entropy_coef=args.entropy_coef,
        max_grad_norm=args.max_grad_norm,
        device=device,
        discrete_action=True,
        num_discrete_actions=5,
        use_recurrent=args.use_recurrent,
        use_monte_carlo=args.use_mc,
        total_steps=args.total_steps,
        lr_schedule='none'  # ë¡œì»¬ ì—ì´ì „íŠ¸ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© ì•ˆ í•¨
    )
    
    # ê¸€ë¡œë²Œ ì—ì´ì „íŠ¸ì˜ ê°€ì¤‘ì¹˜ ë³µì‚¬
    local_agent.actor_critic.load_state_dict(global_agent.actor_critic.state_dict())
    
    # í™˜ê²½ ìƒì„±
    env = CarRacingEnvWrapper(
        max_steps=args.max_episode_steps,
        use_extended_actions=True,
        use_discrete_actions=True
    )
    
    episode_count = 0
    step_count = 0
    
    try:
        while step_count < args.total_steps // args.num_workers:
            # ì—í”¼ì†Œë“œ ì‹œì‘
            reset_result = env.reset()
            if isinstance(reset_result, tuple) and len(reset_result) == 2:
                state, _ = reset_result
            else:
                state = reset_result
            
            # ì ì¬ ìƒíƒœ ì´ˆê¸°í™”
            if args.use_recurrent:
                local_agent.reset_carry()
            
            episode_reward = 0
            episode_length = 0
            
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            while episode_length < args.max_episode_steps:
                # ìƒíƒœ ì •ê·œí™”
                state_normalized = state.astype(np.float32) / 255.0
                state_tensor = torch.FloatTensor(state_normalized).unsqueeze(0).to(device)
                
                # ì•¡ì…˜ ì„ íƒ (ê¸°ì¡´ PPOAgent ë©”ì†Œë“œ ì‚¬ìš©)
                if args.use_recurrent:
                    action, log_prob, value, latent_np = local_agent.get_action_with_carry(state_tensor)
                else:
                    action, log_prob, value = local_agent.actor_critic.get_action(state_tensor)
                    latent_np = None
                
                action_np = action.squeeze(0).cpu().detach().numpy().item()
                log_prob_np = log_prob.squeeze(0).cpu().item() if log_prob is not None else 0.0
                value_np = value.squeeze(0).cpu().item()
                
                # í™˜ê²½ ìŠ¤í…
                next_state, reward, done, info = env.step(action_np)
                
                # ë²„í¼ì— ì €ì¥ (ê¸°ì¡´ PPOAgent ë©”ì†Œë“œ ì‚¬ìš©)
                is_terminal = done or (episode_length + 1) >= args.max_episode_steps
                done_for_buffer = done if not args.mc_update_on_done else is_terminal
                
                if args.use_recurrent:
                    local_agent.store_transition(
                        state_normalized.copy(),
                        action_np,
                        reward,
                        done_for_buffer,
                        log_prob_np,
                        value_np,
                        latent=latent_np
                    )
                else:
                    local_agent.store_transition(
                        state_normalized.copy(),
                        action_np,
                        reward,
                        done_for_buffer,
                        log_prob_np,
                        value_np
                    )
                
                episode_reward += reward
                episode_length += 1
                step_count += 1
                state = next_state
                
                # ê¸€ë¡œë²Œ ìŠ¤í… ì—…ë°ì´íŠ¸ (ë§¤ ìŠ¤í…ë§ˆë‹¤)
                with lock:
                    global_step.value += 1
                
                # ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ (A3C ìŠ¤íƒ€ì¼: ì‘ì€ ë°°ì¹˜ë¡œ ìì£¼ ì—…ë°ì´íŠ¸)
                should_update = False
                if args.mc_update_on_done:
                    should_update = is_terminal and len(local_agent.buffer['states']) > 0
                else:
                    should_update = len(local_agent.buffer['states']) >= args.update_frequency
                
                if should_update:
                    # ì—…ë°ì´íŠ¸ ì „ ë²„í¼ í¬ê¸° ì €ì¥ (ë¡œê¹…ìš©)
                    buffer_size_before = len(local_agent.buffer['states'])
                    
                    # ì§„í–‰ë¥  ê³„ì‚° (ì—”íŠ¸ë¡œí”¼ ìŠ¤ì¼€ì¤„ë§ìš©)
                    progress = min(global_step.value / args.total_steps, 1.0) if args.total_steps > 0 else 0.0
                    
                    # TRM Step-wise Update: ê° Epochë§ˆë‹¤ ë™ê¸°í™”, Epoch ë‚´ì—ì„œëŠ” Kë²ˆ ì—°ì† ìˆ˜í–‰
                    # ì´ë ‡ê²Œ í•˜ë©´ TRMì˜ ì ì§„ì  ê°œì„  íš¨ê³¼ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ì•ˆì •ì„± í™•ë³´
                    total_loss_sum = 0
                    total_policy_loss_sum = 0
                    total_value_loss_sum = 0
                    total_entropy_sum = 0
                    
                    # ëª¨ë“  Epochì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì 
                    accumulated_gradients = None
                    
                    for epoch in range(args.update_epochs):
                        # ê° Epoch ì‹œì‘ ì‹œì—ë§Œ ë©”ì¸ ëª¨ë¸ê³¼ ë™ê¸°í™”
                        with lock:
                            local_agent.actor_critic.load_state_dict(
                                global_agent.actor_critic.state_dict()
                            )
                        
                        # Epoch ë‚´ì—ì„œ Kë²ˆì˜ Supervision Stepì„ ì—°ì†ìœ¼ë¡œ ìˆ˜í–‰
                        # (TRMì˜ ì ì§„ì  ê°œì„  íš¨ê³¼ ìœ ì§€)
                        loss_info = local_agent.update(
                            epochs=1,  # í•œ Epoch = Kë²ˆì˜ Step
                            progress=progress, 
                            return_gradients=True,
                            supervision_step_only=False  # ì „ì²´ Kë²ˆ ìˆ˜í–‰
                        )
                        
                        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                        if 'gradients' in loss_info and loss_info['gradients']:
                            gradients = loss_info['gradients']
                            
                            if accumulated_gradients is None:
                                accumulated_gradients = {}
                                for name, grad in gradients.items():
                                    accumulated_gradients[name] = grad.clone()
                            else:
                                # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                                for name, grad in gradients.items():
                                    if name in accumulated_gradients:
                                        accumulated_gradients[name] += grad.clone()
                                    else:
                                        accumulated_gradients[name] = grad.clone()
                        
                        # í†µê³„ ëˆ„ì 
                        if loss_info:
                            total_loss_sum += loss_info.get('loss', 0)
                            total_policy_loss_sum += loss_info.get('policy_loss', 0)
                            total_value_loss_sum += loss_info.get('value_loss', 0)
                            total_entropy_sum += loss_info.get('entropy', 0)
                    
                    # ëª¨ë“  Epochì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í•œ ë²ˆì— ì ìš© (Lock ê²½í•© ê°ì†Œ)
                    with lock:
                        if accumulated_gradients is not None:
                            # ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬ì— ê·¸ë˜ë””ì–¸íŠ¸ ì ìš©
                            global_agent.optimizer.zero_grad()
                            
                            # ëˆ„ì ëœ ê·¸ë˜ë””ì–¸íŠ¸ ì„¤ì •
                            for name, param in global_agent.actor_critic.named_parameters():
                                if name in accumulated_gradients:
                                    param.grad = accumulated_gradients[name].clone()
                            
                            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                            torch.nn.utils.clip_grad_norm_(
                                global_agent.actor_critic.parameters(), 
                                global_agent.max_grad_norm
                            )
                            
                            # ê¸€ë¡œë²Œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                            global_agent.optimizer.step()
                            
                            # ìµœì¢… ë™ê¸°í™” (ë‹¤ìŒ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´)
                            local_agent.actor_critic.load_state_dict(
                                global_agent.actor_critic.state_dict()
                            )
                    
                    # ì—…ë°ì´íŠ¸ ì •ë³´ ì¶œë ¥ (ì›Œì»¤ 0ë§Œ, í‰ê· ê°’)
                    if worker_id == 0:
                        n_steps = args.update_epochs * local_agent.n_supervision_steps
                        current_step = global_step.value
                        print(f"[Update] Step {current_step}: "
                              f"Loss={total_loss_sum/args.update_epochs:.4f}, "
                              f"Ï€={total_policy_loss_sum/args.update_epochs:.4f}, "
                              f"V={total_value_loss_sum/args.update_epochs:.4f}, "
                              f"H={total_entropy_sum/args.update_epochs:.3f}, "
                              f"Buffer={buffer_size_before}", flush=True)
                
                if done or episode_length >= args.max_episode_steps:
                    break
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ
            episode_count += 1
            
            # ê¸€ë¡œë²Œ í†µê³„ ì—…ë°ì´íŠ¸
            with lock:
                global_episode.value += 1
                # maxlen=100 êµ¬í˜„ (list ì‚¬ìš©)
                global_rewards.append(episode_reward)
                if len(global_rewards) > 100:
                    global_rewards.pop(0)  # ì²« ë²ˆì§¸ ìš”ì†Œ ì œê±°
                
                # ì—í”¼ì†Œë“œ ì •ë³´ ì¶œë ¥ ë° best model ì €ì¥ (ì›Œì»¤ 0ë§Œ, ë§¤ ì—í”¼ì†Œë“œ)
                if worker_id == 0:
                    avg_reward = np.mean(list(global_rewards)) if global_rewards else episode_reward
                    progress = (global_step.value / args.total_steps * 100) if args.total_steps > 0 else 0
                    
                    # Best model ì €ì¥ (í‰ê·  ë¦¬ì›Œë“œê°€ ê°œì„ ë˜ì—ˆì„ ë•Œ)
                    if avg_reward > best_avg_reward.value:
                        best_avg_reward.value = avg_reward
                        best_model_path = args.save_path.replace('.pth', '_best.pth')
                        global_agent.save(best_model_path)
                        print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ê¸°ë¡! Avg Reward: {avg_reward:.2f} â†’ Best Model ì €ì¥: {best_model_path}", flush=True)
                    
                    print(f"[Ep {global_episode.value}] "
                          f"R={episode_reward:.2f} (Avg100={avg_reward:.2f}, Best={best_avg_reward.value:.2f}), "
                          f"Len={episode_length}, "
                          f"Steps={global_step.value:,}/{args.total_steps:,} ({progress:.1f}%)", flush=True)
    
    except Exception as e:
        print(f"Worker {worker_id} error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        env.close()


def main():
    parser = argparse.ArgumentParser(description='A3C ê°•í™”í•™ìŠµ í›ˆë ¨ (PPOAgent í˜¸í™˜)')
    
    # í™˜ê²½ íŒŒë¼ë¯¸í„°
    parser.add_argument('--max-episode-steps', type=int, default=1000)
    parser.add_argument('--state-dim', type=int, default=784)
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    parser.add_argument('--total-steps', type=int, default=1000000)
    parser.add_argument('--update-frequency', type=int, default=20)  # A3CëŠ” ì‘ì€ ë°°ì¹˜
    parser.add_argument('--update-epochs', type=int, default=1)  # A3CëŠ” 1 ì—í­
    parser.add_argument('--num-workers', type=int, default=4, help='ë³‘ë ¬ ì›Œì»¤ ìˆ˜')
    
    # ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° (PPOAgentì™€ ë™ì¼)
    parser.add_argument('--hidden-dim', type=int, default=256)
    parser.add_argument('--lr-actor', type=float, default=3e-4)
    parser.add_argument('--lr-critic', type=float, default=3e-4)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--gae-lambda', type=float, default=0.95)
    parser.add_argument('--clip-epsilon', type=float, default=0.2)
    parser.add_argument('--value-coef', type=float, default=0.5)
    parser.add_argument('--entropy-coef', type=float, default=0.01)
    parser.add_argument('--max-grad-norm', type=float, default=0.5)
    parser.add_argument('--lr-schedule', type=str, default='cosine', 
                        choices=['cosine', 'linear', 'none'],
                        help='í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ë°©ì‹: cosine (ì½”ì‚¬ì¸ ê°ì†Œ), linear (ì„ í˜• ê°ì†Œ), none (ì—†ìŒ)')
    
    # TRM-PPO íŒŒë¼ë¯¸í„° (PPOAgentì™€ ë™ì¼)
    parser.add_argument('--use-recurrent', action='store_true', default=True)
    parser.add_argument('--n-cycles', type=int, default=4)
    parser.add_argument('--latent-dim', type=int, default=256)
    parser.add_argument('--carry-latent', action='store_true', default=True)
    parser.add_argument('--use-mc', action='store_true', default=False)
    parser.add_argument('--mc-update-on-done', action='store_true', default=False)
    
    # ì €ì¥
    parser.add_argument('--save-path', type=str, default='a3c_model.pth')
    parser.add_argument('--save-frequency', type=int, default=10000)
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None)
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print("=" * 60)
    print("A3C (Asynchronous Advantage Actor-Critic) í•™ìŠµ")
    print("  -> ê¸°ì¡´ PPOAgent êµ¬ì¡° ì¬ì‚¬ìš©")
    print("=" * 60)
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ì›Œì»¤ ìˆ˜: {args.num_workers}")
    print(f"ìƒíƒœ ì°¨ì›: {args.state_dim} ({int(args.state_dim**0.5)}x{int(args.state_dim**0.5)})")
    print(f"ì´ ìŠ¤í…: {args.total_steps}")
    if args.use_recurrent:
        print(f"TRM-PPO ëª¨ë“œ: n_cycles={args.n_cycles}, latent_dim={args.latent_dim}")
    if args.use_mc:
        print(f"Monte Carlo ëª¨ë“œ í™œì„±í™”")
    print("=" * 60)
    
    # ê¸€ë¡œë²Œ ì—ì´ì „íŠ¸ ìƒì„± (PPOAgent ì‚¬ìš©)
    global_agent = PPOAgent(
        state_dim=args.state_dim,
        action_dim=5,
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=args.carry_latent,
        lr_actor=args.lr_actor,
        lr_critic=args.lr_critic,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_epsilon=args.clip_epsilon,
        value_coef=args.value_coef,
        entropy_coef=args.entropy_coef,
        max_grad_norm=args.max_grad_norm,
        device=device,
        discrete_action=True,
        num_discrete_actions=5,
        use_recurrent=args.use_recurrent,
        use_monte_carlo=args.use_mc,
        total_steps=args.total_steps,
        lr_schedule='none'  # A3Cì—ì„œëŠ” ìŠ¤ì¼€ì¤„ë§ ì‚¬ìš© ì•ˆ í•¨ (ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì—…ë°ì´íŠ¸)
    )
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ê³µìœ ë¥¼ ìœ„í•´ ë„¤íŠ¸ì›Œí¬ ê³µìœ  ë©”ëª¨ë¦¬ ì„¤ì •
    global_agent.actor_critic.share_memory()
    
    # ê³µìœ  ë³€ìˆ˜
    global_step = mp.Value('i', 0)
    global_episode = mp.Value('i', 0)
    best_avg_reward = mp.Value('f', float('-inf'))  # ìµœê³  í‰ê·  ë¦¬ì›Œë“œ ì¶”ì 
    manager = mp.Manager()
    global_rewards = manager.list()  # deque ëŒ€ì‹  list ì‚¬ìš©
    lock = mp.Lock()
    
    # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    processes = []
    for worker_id in range(args.num_workers):
        p = mp.Process(
            target=worker,
            args=(worker_id, global_agent, args,
                  global_step, global_episode, global_rewards, best_avg_reward, lock, device)
        )
        p.start()
        processes.append(p)
        time.sleep(0.1)  # ìˆœì°¨ ì‹œì‘
    
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤: ì£¼ê¸°ì  ì €ì¥ ë° í†µê³„ ì¶œë ¥
    try:
        last_save_step = 0
        last_stat_time = time.time()
        last_stat_step = 0
        
        print("\nğŸš€ í•™ìŠµ ì‹œì‘! ì›Œì»¤ë“¤ì´ ê²½í—˜ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...\n", flush=True)
        
        while global_step.value < args.total_steps:
            time.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
            
            current_time = time.time()
            elapsed_time = current_time - last_stat_time
            
            # 30ì´ˆë§ˆë‹¤ í†µê³„ ì¶œë ¥
            if elapsed_time >= 30:
                with lock:
                    current_step = global_step.value
                    current_episode = global_episode.value
                    step_diff = current_step - last_stat_step
                    steps_per_sec = step_diff / elapsed_time if elapsed_time > 0 else 0
                    
                    avg_reward = np.mean(list(global_rewards)) if global_rewards else 0.0
                    best_reward = best_avg_reward.value
                    progress = (current_step / args.total_steps * 100) if args.total_steps > 0 else 0
                    remaining_steps = args.total_steps - current_step
                    eta_seconds = remaining_steps / steps_per_sec if steps_per_sec > 0 else 0
                    eta_hours = eta_seconds / 3600
                    
                    print(f"\n{'='*60}")
                    print(f"ğŸ“Š í•™ìŠµ ì§„í–‰ ìƒí™©")
                    print(f"{'='*60}")
                    print(f"ì´ ìŠ¤í…: {current_step:,} / {args.total_steps:,} ({progress:.1f}%)")
                    print(f"ì´ ì—í”¼ì†Œë“œ: {current_episode:,}")
                    print(f"í‰ê·  ë¦¬ì›Œë“œ (100ep): {avg_reward:.2f}")
                    print(f"ìµœê³  í‰ê·  ë¦¬ì›Œë“œ: {best_reward:.2f}")
                    print(f"ìŠ¤í…/ì´ˆ: {steps_per_sec:.1f}")
                    print(f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta_hours:.1f}ì‹œê°„")
                    print(f"{'='*60}\n", flush=True)
                
                last_stat_time = current_time
                last_stat_step = current_step
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
            if global_step.value - last_save_step >= args.save_frequency:
                # ëª¨ë¸ ì €ì¥ (PPOAgentì™€ ë™ì¼í•œ í˜•ì‹)
                global_agent.save(args.save_path)
                print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {args.save_path} (Step: {global_step.value:,})", flush=True)
                last_save_step = global_step.value
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  í•™ìŠµ ì¤‘ë‹¨ë¨ (Ctrl+C)", flush=True)
    
    finally:
        # ì›Œì»¤ ì¢…ë£Œ
        print("\nì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì¤‘...", flush=True)
        for p in processes:
            p.terminate()
            p.join()
        
        # ìµœì¢… í†µê³„
        with lock:
            final_step = global_step.value
            final_episode = global_episode.value
            final_avg_reward = np.mean(list(global_rewards)) if global_rewards else 0.0
        
        print(f"\n{'='*60}")
        print(f"âœ… í•™ìŠµ ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"ì´ ìŠ¤í…: {final_step:,}")
        print(f"ì´ ì—í”¼ì†Œë“œ: {final_episode:,}")
        print(f"í‰ê·  ë¦¬ì›Œë“œ (100ep): {final_avg_reward:.2f}")
        print(f"{'='*60}\n")
        
        # ìµœì¢… ì €ì¥
        global_agent.save(args.save_path)
        print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {args.save_path}", flush=True)


if __name__ == "__main__":
    mp.set_start_method('spawn')  # Windows/Linux í˜¸í™˜
    main()

