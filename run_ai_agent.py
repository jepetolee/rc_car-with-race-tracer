#!/usr/bin/env python3
"""
AI ì—ì´ì „íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµëœ PPO ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ RC Carë¥¼ 0.1ì´ˆ ê°„ê²©ìœ¼ë¡œ ì œì–´

ì‚¬ìš©ë²•:
    python run_ai_agent.py --model ppo_model.pth --port /dev/ttyACM0 --delay 0.1
"""

import os
# NumPy/PyTorch ì„í¬íŠ¸ ì „ì— í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Bus error ë°©ì§€)
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'
# PyTorch ìŠ¤ë ˆë“œ ì œí•œ
os.environ['TORCH_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

import argparse
import numpy as np

# PyTorch ì„í¬íŠ¸ (ì•ˆì „í•˜ê²Œ)
print("PyTorch ì„í¬íŠ¸ ì¤‘...", flush=True)
try:
    import torch
    print(f"âœ… PyTorch {torch.__version__} ì„í¬íŠ¸ ì„±ê³µ", flush=True)
except Exception as e:
    print(f"âŒ PyTorch ì„í¬íŠ¸ ì‹¤íŒ¨: {e}", flush=True)
    print("ë¼ì¦ˆë² ë¦¬ íŒŒì´ìš© PyTorchë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:", flush=True)
    print("  pip install torch --index-url https://download.pytorch.org/whl/cpu", flush=True)
    import sys
    sys.exit(1)
import time
import sys
import os
from datetime import datetime

# í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„í¬íŠ¸
from rc_car_sim_env import RCCarSimEnv
from car_racing_env import CarRacingEnvWrapper
from ppo_agent import PPOAgent
from rc_car_controller import RCCarController

# ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ì€ ì„ íƒì  ì„í¬íŠ¸
try:
    from rc_car_env import RCCarEnv
    HAS_REAL_ENV = True
except ImportError:
    HAS_REAL_ENV = False
    RCCarEnv = None


class AIAgentRunner:
    """
    AI ì—ì´ì „íŠ¸ ì‹¤í–‰ í´ë˜ìŠ¤
    0.1ì´ˆ ê°„ê²©ìœ¼ë¡œ ì•¡ì…˜ì„ ì‹¤í–‰í•˜ë©° RC Carë¥¼ ì œì–´
    """
    
    def __init__(
        self,
        model_path: str,
        env_type: str = 'carracing',
        port: str = '/dev/ttyACM0',
        action_delay: float = 0.1,
        max_steps: int = 1000,
        use_discrete_actions: bool = True,  # ì´ì‚° ì•¡ì…˜ë§Œ ì‚¬ìš©
        use_extended_actions: bool = True,
        device: str = None
    ):
        """
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            env_type: í™˜ê²½ íƒ€ì… ('carracing', 'sim', 'real')
            port: ì‹œë¦¬ì–¼ í¬íŠ¸ (ì‹¤ì œ í•˜ë“œì›¨ì–´ ì‚¬ìš© ì‹œ)
            action_delay: ì•¡ì…˜ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 0.1)
            max_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜
            use_discrete_actions: ì´ì‚° ì•¡ì…˜ ì‚¬ìš© ì—¬ë¶€
            use_extended_actions: í™•ì¥ëœ ì•¡ì…˜ ê³µê°„ ì‚¬ìš© ì—¬ë¶€
            device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
        """
        self.model_path = model_path
        self.env_type = env_type
        self.port = port
        self.action_delay = action_delay
        self.max_steps = max_steps
        self.use_discrete_actions = use_discrete_actions
        self.use_extended_actions = use_extended_actions
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (ë¼ì¦ˆë² ë¦¬ íŒŒì´ì—ì„œëŠ” í•­ìƒ CPU)
        if device is None:
            # ë¼ì¦ˆë² ë¦¬ íŒŒì´ì—ì„œëŠ” GPUê°€ ì—†ìœ¼ë¯€ë¡œ í•­ìƒ CPU ì‚¬ìš©
            self.device = 'cpu'
        else:
            self.device = device
        
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ì•¡ì…˜ ì§€ì—° ì‹œê°„: {action_delay:.3f}ì´ˆ")
        print(f"í™˜ê²½ íƒ€ì…: {env_type}")
        
        # ë‹¨ê³„ë³„ ì´ˆê¸°í™” (Bus error ë°©ì§€)
        print("\n[ì´ˆê¸°í™” ë‹¨ê³„ 1/4] í™˜ê²½ ìƒì„± ì¤‘...")
        try:
            self.env = self._create_env()
            print("âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # í•˜ë“œì›¨ì–´ ì œì–´ê¸° ìƒì„± (real ëª¨ë“œì¼ ë•Œë§Œ)
        print("\n[ì´ˆê¸°í™” ë‹¨ê³„ 2/4] í•˜ë“œì›¨ì–´ ì œì–´ê¸° ì—°ê²° ì¤‘...")
        self.controller = None
        if env_type == 'real':
            try:
                self.controller = RCCarController(port=port, delay=action_delay)
                print(f"âœ… ì‹¤ì œ í•˜ë“œì›¨ì–´ ì—°ê²°: {port}")
            except Exception as e:
                print(f"âš ï¸  ì‹¤ì œ í•˜ë“œì›¨ì–´ ì—°ê²° ì‹¤íŒ¨: {e}")
                print("ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.env_type = 'sim'
                self.env = self._create_env()
        
        # ì—ì´ì „íŠ¸ ìƒì„± ë° ëª¨ë¸ ë¡œë“œ
        print("\n[ì´ˆê¸°í™” ë‹¨ê³„ 3/4] ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
        try:
            self.agent = self._load_agent()
            print("âœ… ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        print("\n[ì´ˆê¸°í™” ë‹¨ê³„ 4/4] ì´ˆê¸°í™” ì™„ë£Œ!")
        print("=" * 60)
    
    def _create_env(self):
        """í™˜ê²½ ìƒì„±"""
        if self.env_type == 'carracing':
            try:
                env = CarRacingEnvWrapper(
                    max_steps=self.max_steps,
                    use_extended_actions=self.use_extended_actions,
                    use_discrete_actions=self.use_discrete_actions
                )
                print("âœ… CarRacing í™˜ê²½ ì‚¬ìš©")
                return env
            except ImportError as e:
                print(f"âŒ CarRacing í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                print("ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.env_type = 'sim'
                return self._create_env()
        
        elif self.env_type == 'sim':
            env = RCCarSimEnv(
                max_steps=self.max_steps,
                use_extended_actions=self.use_extended_actions,
                use_discrete_actions=self.use_discrete_actions
            )
            print("âœ… ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì‚¬ìš©")
            return env
        
        elif self.env_type == 'real':
            if not HAS_REAL_ENV:
                raise ImportError(
                    "ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    "ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì„ ì‚¬ìš©í•˜ì„¸ìš”: --env-type sim"
                )
            env = RCCarEnv(
                max_steps=self.max_steps,
                use_extended_actions=self.use_extended_actions,
                use_discrete_actions=self.use_discrete_actions
            )
            print("âœ… ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ ì‚¬ìš©")
            return env
        
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” í™˜ê²½ íƒ€ì…: {self.env_type}")
    
    def _load_agent(self):
        """ì—ì´ì „íŠ¸ ìƒì„± ë° ëª¨ë¸ ë¡œë“œ"""
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = PPOAgent(
            state_dim=784,  # 28x28 ì´ë¯¸ì§€ = 784 ì°¨ì› (í™˜ê²½ ì¶œë ¥ê³¼ ì¼ì¹˜)
            action_dim=5,  # ì´ì‚° ì•¡ì…˜ë§Œ (ê³ ì •)
            latent_dim=256,
            hidden_dim=256,
            n_cycles=4,
            carry_latent=True,
            device=self.device,
            discrete_action=True,  # ì´ì‚° ì•¡ì…˜ë§Œ
            num_discrete_actions=5,
            use_recurrent=True
        )
        
        # ëª¨ë¸ ë¡œë“œ (ì•ˆì „í•œ ë°©ì‹)
        if os.path.exists(self.model_path):
            try:
                print(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
                agent.load(self.model_path)
                print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            except Exception as e:
                print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ëœë¤ ì •ì±…ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        else:
            print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            print("ëœë¤ ì •ì±…ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        
        return agent
    
    def run_episode(self, render: bool = False, verbose: bool = True):
        """
        ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰
        
        Args:
            render: ë Œë”ë§ ì—¬ë¶€
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            episode_reward: ì—í”¼ì†Œë“œ ì´ ë¦¬ì›Œë“œ
            episode_length: ì—í”¼ì†Œë“œ ê¸¸ì´
        """
        # í™˜ê²½ ë¦¬ì…‹
        reset_result = self.env.reset()
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            state, _ = reset_result  # Gymnasium
        else:
            state = reset_result  # Gym
        
        # TRM-PPO: ì ì¬ ìƒíƒœ ì´ˆê¸°í™”
        if hasattr(self.agent, 'use_recurrent') and self.agent.use_recurrent:
            self.agent.reset_carry()
        
        episode_reward = 0.0
        episode_length = 0
        
        if verbose:
            print("\n" + "=" * 60)
            print("AI ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘")
            print("=" * 60)
            print(f"ì•¡ì…˜ ê°„ê²©: {self.action_delay:.3f}ì´ˆ")
            print("=" * 60 + "\n")
        
        try:
            for step in range(self.max_steps):
                # ìƒíƒœ ì •ê·œí™” [0, 255] -> [0, 1]
                state_normalized = state.astype(np.float32) / 255.0
                state_tensor = torch.FloatTensor(state_normalized).unsqueeze(0).to(self.device)
                
                # ì•¡ì…˜ ì„ íƒ (deterministic: ìµœëŒ€ í™•ë¥  ì•¡ì…˜)
                if hasattr(self.agent, 'use_recurrent') and self.agent.use_recurrent:
                    action, _, value, _ = self.agent.get_action_with_carry(
                        state_tensor, deterministic=True
                    )
                else:
                    action, _, value = self.agent.actor_critic.get_action(
                        state_tensor, deterministic=True
                    )
                
                # ì•¡ì…˜ ë³€í™˜
                if self.use_discrete_actions:
                    if isinstance(action, torch.Tensor):
                        action_np = action.squeeze(0).cpu().detach().numpy()
                        if action_np.ndim == 0:
                            action_np = int(action_np)
                        else:
                            action_np = int(action_np[0]) if len(action_np) > 0 else int(action_np)
                    else:
                        action_np = int(action)
                else:
                    if isinstance(action, torch.Tensor):
                        action_np = action.squeeze(0).cpu().detach().numpy()
                    else:
                        action_np = np.array(action)
                
                # ì‹¤ì œ í•˜ë“œì›¨ì–´ ì œì–´ (real ëª¨ë“œì¼ ë•Œ)
                if self.controller is not None and self.use_discrete_actions:
                    self.controller.execute_discrete_action(action_np)
                
                # í™˜ê²½ ìŠ¤í… ì‹¤í–‰
                next_state, reward, done, info = self.env.step(action_np)
                
                episode_reward += reward
                episode_length += 1
                
                # ì¶œë ¥
                if verbose:
                    action_name = {
                        0: "Stop", 1: "Right+Gas", 2: "Left+Gas", 
                        3: "Gas", 4: "Brake"
                    }.get(action_np, f"Action {action_np}") if self.use_discrete_actions else f"Action {action_np}"
                    
                    print(f"[Step {step+1:4d}] "
                          f"Action: {action_name:12s} | "
                          f"Reward: {reward:7.3f} | "
                          f"Total: {episode_reward:7.3f} | "
                          f"Value: {value.item():7.3f}")
                
                # ë Œë”ë§
                if render and hasattr(self.env, 'render'):
                    self.env.render()
                
                # 0.1ì´ˆ ì§€ì—° (ì•¡ì…˜ ê°„ê²©)
                time.sleep(self.action_delay)
                
                # ì—í”¼ì†Œë“œ ì¢…ë£Œ
                if done:
                    break
                
                state = next_state
        
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if verbose:
            print("\n" + "=" * 60)
            print("ì—í”¼ì†Œë“œ ì™„ë£Œ")
            print("=" * 60)
            print(f"ì´ ë¦¬ì›Œë“œ: {episode_reward:.3f}")
            print(f"ì—í”¼ì†Œë“œ ê¸¸ì´: {episode_length} ìŠ¤í…")
            print(f"í‰ê·  ë¦¬ì›Œë“œ: {episode_reward/episode_length:.3f}" if episode_length > 0 else "í‰ê·  ë¦¬ì›Œë“œ: 0.000")
            print("=" * 60 + "\n")
        
        # ì •ì§€ (ì‹¤ì œ í•˜ë“œì›¨ì–´)
        if self.controller is not None:
            self.controller.stop()
        
        return episode_reward, episode_length
    
    def run_multiple_episodes(self, num_episodes: int = 5, render: bool = False, verbose: bool = True):
        """
        ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹¤í–‰
        
        Args:
            num_episodes: ì—í”¼ì†Œë“œ ìˆ˜
            render: ë Œë”ë§ ì—¬ë¶€
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            episode_rewards: ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            episode_lengths: ì—í”¼ì†Œë“œ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸
        """
        episode_rewards = []
        episode_lengths = []
        
        print(f"\n{'='*60}")
        print(f"ì´ {num_episodes}ê°œ ì—í”¼ì†Œë“œ ì‹¤í–‰")
        print(f"{'='*60}\n")
        
        for episode in range(num_episodes):
            if verbose:
                print(f"\n>>> ì—í”¼ì†Œë“œ {episode + 1}/{num_episodes} <<<")
            
            reward, length = self.run_episode(render=render, verbose=verbose)
            episode_rewards.append(reward)
            episode_lengths.append(length)
            
            # ì—í”¼ì†Œë“œ ê°„ ì§§ì€ ëŒ€ê¸°
            if episode < num_episodes - 1:
                time.sleep(1.0)
        
        # í†µê³„ ì¶œë ¥
        print(f"\n{'='*60}")
        print("ì „ì²´ í†µê³„")
        print(f"{'='*60}")
        print(f"í‰ê·  ë¦¬ì›Œë“œ: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
        print(f"ìµœê³  ë¦¬ì›Œë“œ: {np.max(episode_rewards):.3f}")
        print(f"ìµœì € ë¦¬ì›Œë“œ: {np.min(episode_rewards):.3f}")
        print(f"í‰ê·  ê¸¸ì´: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f} ìŠ¤í…")
        print(f"{'='*60}\n")
        
        return episode_rewards, episode_lengths
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.env:
            self.env.close()
        if self.controller:
            self.controller.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='AI ì—ì´ì „íŠ¸ ì‹¤í–‰ - í•™ìŠµëœ ëª¨ë¸ë¡œ RC Car ì œì–´',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # CarRacing í™˜ê²½ì—ì„œ ì‹¤í–‰ (0.1ì´ˆ ê°„ê²©)
  python run_ai_agent.py --model ppo_model.pth --env-type carracing --delay 0.1
  
  # ì‹¤ì œ í•˜ë“œì›¨ì–´ì—ì„œ ì‹¤í–‰ (0.1ì´ˆ ê°„ê²©)
  python run_ai_agent.py --model ppo_model.pth --env-type real --port /dev/ttyACM0 --delay 0.1
  
  # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹¤í–‰
  python run_ai_agent.py --model ppo_model.pth --episodes 5 --delay 0.1
        """
    )
    
    # ëª¨ë¸ ê²½ë¡œ
    parser.add_argument('--model', type=str, required=True,
                        help='í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: ppo_model.pth)')
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument('--env-type', choices=['carracing', 'sim', 'real'],
                        default='carracing',
                        help='í™˜ê²½ íƒ€ì… (ê¸°ë³¸: carracing)')
    parser.add_argument('--port', type=str, default='/dev/ttyACM0',
                        help='ì‹œë¦¬ì–¼ í¬íŠ¸ (real ëª¨ë“œ ì‚¬ìš© ì‹œ, ê¸°ë³¸: /dev/ttyACM0)')
    parser.add_argument('--max-steps', type=int, default=1000,
                        help='ìµœëŒ€ ìŠ¤í… ìˆ˜ (ê¸°ë³¸: 1000)')
    
    # ì•¡ì…˜ ì„¤ì •
    parser.add_argument('--delay', type=float, default=0.1,
                        help='ì•¡ì…˜ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 0.1)')
    parser.add_argument('--use-discrete-actions', action='store_true', default=True,
                        help='ì´ì‚° ì•¡ì…˜ ì‚¬ìš© (ê¸°ë³¸: True)')
    parser.add_argument('--use-continuous-actions', dest='use_discrete_actions', action='store_false',
                        help='ì—°ì† ì•¡ì…˜ ì‚¬ìš©')
    parser.add_argument('--use-extended-actions', action='store_true', default=True,
                        help='í™•ì¥ëœ ì•¡ì…˜ ê³µê°„ ì‚¬ìš© (ê¸°ë³¸: True)')
    
    # ì‹¤í–‰ ì„¤ì •
    parser.add_argument('--episodes', type=int, default=1,
                        help='ì‹¤í–‰í•  ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸: 1)')
    parser.add_argument('--render', action='store_true',
                        help='ë Œë”ë§ í™œì„±í™” (ì‹œë®¬ë ˆì´ì…˜/CarRacing ëª¨ë“œ)')
    parser.add_argument('--quiet', action='store_true',
                        help='ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”')
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None,
                        help='ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™ ì„ íƒ)')
    
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("AIAgentRunner ìƒì„± ì‹œì‘")
    print("=" * 60)
    
    # AI ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„± (ë‹¨ê³„ë³„)
    try:
        print("\n[ë‹¨ê³„ 1] AIAgentRunner ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        runner = AIAgentRunner(
            model_path=args.model,
            env_type=args.env_type,
            port=args.port,
            action_delay=args.delay,
            max_steps=args.max_steps,
            use_discrete_actions=args.use_discrete_actions,
            use_extended_actions=args.use_extended_actions,
            device=args.device
        )
        print("âœ… AIAgentRunner ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"\nâŒ AIAgentRunner ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    try:
        # ì—í”¼ì†Œë“œ ì‹¤í–‰
        if args.episodes == 1:
            runner.run_episode(render=args.render, verbose=not args.quiet)
        else:
            runner.run_multiple_episodes(
                num_episodes=args.episodes,
                render=args.render,
                verbose=not args.quiet
            )
    finally:
        runner.close()


if __name__ == "__main__":
    main()

