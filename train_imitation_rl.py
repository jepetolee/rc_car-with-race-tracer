#!/usr/bin/env python3
"""
Imitation Learning via Reinforcement Learning
ì‚¬ìš©ì ë°ëª¨ ë°ì´í„°ì™€ì˜ ì¼ì¹˜ìœ¨ì„ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©í•˜ì—¬ ê°•í™”í•™ìŠµ

Supervised Learningì´ ì•„ë‹Œ ê°•í™”í•™ìŠµìœ¼ë¡œ:
- ëª¨ë¸ì´ ì•¡ì…˜ì„ ì„ íƒ
- ì‚¬ìš©ìê°€ ì„ íƒí•œ ì•¡ì…˜ê³¼ ë¹„êµ
- ì¼ì¹˜ìœ¨ì— ë”°ë¼ ë¦¬ì›Œë“œ ë¶€ì—¬
- PPOë¡œ í•™ìŠµ
"""

import os
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['TORCH_NUM_THREADS'] = '1'

import argparse
import pickle
import numpy as np
import torch
import torch.nn.functional as F
from datetime import datetime, timedelta
import sys
import time

from ppo_agent import PPOAgent, LatentCarry
from train_ppo import train_ppo


class ImitationRLTrainer:
    """
    ì‚¬ìš©ì ë°ëª¨ ë°ì´í„°ì™€ì˜ ì¼ì¹˜ìœ¨ì„ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©í•˜ëŠ” ê°•í™”í•™ìŠµ
    """
    
    def __init__(
        self,
        demos_path: str,
        model_path: str = None,
        device: str = 'cpu',
        learning_rate: float = 3e-4,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_epsilon: float = 0.2,
        value_coef: float = 0.5,
        entropy_coef: float = 0.01,
        max_grad_norm: float = 0.5,
        batch_size: int = 64,
        update_epochs: int = 10
    ):
        """
        Args:
            demos_path: ì‚¬ìš©ì ë°ëª¨ ë°ì´í„° ê²½ë¡œ (pickle íŒŒì¼)
            model_path: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒ)
            device: ë””ë°”ì´ìŠ¤
            learning_rate: í•™ìŠµë¥ 
            gamma: í• ì¸ ê³„ìˆ˜
            gae_lambda: GAE ëŒë‹¤
            clip_epsilon: PPO clip epsilon
            value_coef: Value loss ê³„ìˆ˜
            entropy_coef: Entropy ê³„ìˆ˜
            max_grad_norm: Gradient clipping
            batch_size: ë°°ì¹˜ í¬ê¸°
            update_epochs: ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
        """
        self.demos_path = demos_path
        self.device = device
        self.batch_size = batch_size
        self.update_epochs = update_epochs
        self.use_sequence_mode = True  # ì‹œí€€ìŠ¤ ëª¨ë“œ: ì—í”¼ì†Œë“œ ë‚´ ì‹œí€€ìŠ¤ ìœ ì§€ ë° latent ì „ë‹¬
        
        # ë°ëª¨ ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“‚ ë°ëª¨ ë°ì´í„° ë¡œë“œ: {demos_path}")
        with open(demos_path, 'rb') as f:
            data = pickle.load(f)
        
        all_demos = data.get('demonstrations', [])
        if len(all_demos) == 0:
            raise ValueError("ë°ëª¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # statesë‚˜ actionsê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ì—í”¼ì†Œë“œ í•„í„°ë§
        self.demos = []
        filtered_count = 0
        
        for episode in all_demos:
            states = episode.get('states', [])
            actions = episode.get('actions', [])
            
            # statesë‚˜ actionsê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì œì™¸
            if not states or not actions or len(states) == 0 or len(actions) == 0:
                filtered_count += 1
                continue
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            if len(states) != len(actions):
                min_len = min(len(states), len(actions))
                states = states[:min_len]
                actions = actions[:min_len]
            
            # ìœ íš¨í•œ ë°ì´í„°ë§Œ í¬í•¨
            if len(states) > 0 and len(actions) > 0:
                self.demos.append({
                    'states': states,
                    'actions': actions
                })
            else:
                filtered_count += 1
        
        if len(self.demos) == 0:
            raise ValueError("ìœ íš¨í•œ ë°ëª¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë“  ì—í”¼ì†Œë“œê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤).")
        
        if filtered_count > 0:
            print(f"âš ï¸  {filtered_count}ê°œ ì—í”¼ì†Œë“œ í•„í„°ë§ë¨ (statesë‚˜ actionsê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ)")
        
        print(f"âœ… {len(self.demos)}ê°œ ìœ íš¨í•œ ì—í”¼ì†Œë“œ ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë“  (state, action) ìŒ ì¶”ì¶œ
        # ì£¼ì˜: Imitation Learningì´ë¯€ë¡œ pklì˜ 'rewards'ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        # ë¦¬ì›Œë“œëŠ” ëª¨ë¸ ì•¡ì…˜ê³¼ ì „ë¬¸ê°€ ì•¡ì…˜ì„ ë¹„êµí•˜ì—¬ ìë™ ìƒì„±ë¨
        self.demo_states = []
        self.demo_actions = []
        
        for episode in self.demos:
            states = episode.get('states', [])
            actions = episode.get('actions', [])
            # rewards, dones, timestampsëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (Imitation Learning)
            
            self.demo_states.extend(states)
            self.demo_actions.extend(actions)
        
        print(f"âœ… ì´ {len(self.demo_states)}ê°œ (state, action) ìŒ")
        
        # ìƒíƒœ ì°¨ì› ìë™ ê°ì§€
        if len(self.demo_states) > 0:
            # ì²« ë²ˆì§¸ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ì°¨ì› ê²°ì •
            first_state = np.array(self.demo_states[0])
            state_dim = first_state.shape[0] if len(first_state.shape) == 1 else first_state.size
            print(f"ğŸ“ ìƒíƒœ ì°¨ì› ìë™ ê°ì§€: {state_dim}")
        else:
            raise ValueError("ë°ëª¨ ë°ì´í„°ì— ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        print(f"\nğŸ¤– ì—ì´ì „íŠ¸ ìƒì„±...")
        # PPOAgentëŠ” lr_actor, lr_critic íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš© (learning_rateê°€ ì•„ë‹˜)
        actor_lr = float(learning_rate)
        critic_lr = float(learning_rate)
        self.agent = PPOAgent(
            state_dim=state_dim,
            action_dim=5,
            discrete_action=True,
            lr_actor=actor_lr,
            lr_critic=critic_lr,
            gamma=gamma,
            gae_lambda=gae_lambda,
            clip_epsilon=clip_epsilon,
            value_coef=value_coef,
            entropy_coef=entropy_coef,
            max_grad_norm=max_grad_norm,
            device=device,
            use_recurrent=True,  # Deep Supervisionì„ ìœ„í•´ Recurrent í™œì„±í™”
            deep_supervision=True
        )
        
        # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
        # model_pathê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ a3c_model_best.pth ì‚¬ìš©
        if not model_path:
            default_model = 'a3c_model_best.pth'
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ í™•ì¸
            if os.path.exists(default_model):
                model_path = default_model
                print(f"ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ìë™ ê°ì§€: {default_model}")
            else:
                print(f"âš ï¸  ê¸°ë³¸ ëª¨ë¸({default_model})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                model_path = None
        
        if model_path and os.path.exists(model_path):
            print(f"ğŸ“¥ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ: {model_path}")
            try:
                self.agent.load(model_path)
                print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ëœë¤ ì´ˆê¸°í™”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        elif model_path:
            print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            print("ëœë¤ ì´ˆê¸°í™”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    def compute_imitation_reward(self, predicted_action: int, expert_action: int) -> float:
        """
        ëª¨ë¸ì˜ ì•¡ì…˜ê³¼ ì „ë¬¸ê°€ ì•¡ì…˜ì˜ ì¼ì¹˜ìœ¨ì— ë”°ë¥¸ ë¦¬ì›Œë“œ ê³„ì‚°
        
        Args:
            predicted_action: ëª¨ë¸ì´ ì„ íƒí•œ ì•¡ì…˜
            expert_action: ì‚¬ìš©ìê°€ ì„ íƒí•œ ì•¡ì…˜
        
        Returns:
            reward: ì¼ì¹˜í•˜ë©´ 1.0, ë¶ˆì¼ì¹˜í•˜ë©´ -0.1
        """
        if predicted_action == expert_action:
            return 1.0  # ì™„ì „ ì¼ì¹˜
        else:
            return -0.1  # ë¶ˆì¼ì¹˜ í˜ë„í‹°
    
    def train_step_sequence(
        self, 
        states: np.ndarray, 
        expert_actions: np.ndarray,
        is_first_batch: bool = False,
        prev_latent: torch.Tensor = None
    ):
        """
        ì‹œí€€ìŠ¤ í•™ìŠµ ìŠ¤í… (ì´ì „ latent ì „ë‹¬)
        
        Args:
            states: ìƒíƒœ ë°°ì—´ [batch_size, 256]
            expert_actions: ì „ë¬¸ê°€ ì•¡ì…˜ ë°°ì—´ [batch_size]
            is_first_batch: ì—í”¼ì†Œë“œì˜ ì²« ë°°ì¹˜ì¸ì§€
            prev_latent: ì´ì „ ë°°ì¹˜ì˜ latent (Noneì´ë©´ ì´ˆê¸°í™”)
        
        Returns:
            stats: í†µê³„ ë”•ì…”ë„ˆë¦¬
            next_latent: ë‹¤ìŒ ë°°ì¹˜ë¡œ ì „ë‹¬í•  latent
        """
        states_tensor = torch.FloatTensor(states).to(self.device)
        expert_actions_tensor = torch.LongTensor(expert_actions).to(self.device)
        
        batch_size = states_tensor.shape[0]
        
        # Latent ì´ˆê¸°í™” ë˜ëŠ” ì´ì „ latent ì‚¬ìš©
        if is_first_batch or prev_latent is None:
            latent = self.agent.actor_critic.init_latent.unsqueeze(0).expand(batch_size, -1).clone()
        else:
            # ì´ì „ ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ latent ì‚¬ìš© (ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì •)
            if prev_latent.shape[0] == batch_size:
                latent = prev_latent.clone()
            else:
                # ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë§ˆì§€ë§‰ latentë¥¼ ë³µì œ
                latent = prev_latent[-1:].expand(batch_size, -1).clone()
        
        # ì´ˆê¸° ì•¡ì…˜ ì„ íƒ (ë¦¬ì›Œë“œ ê³„ì‚°ìš©)
        # ì‹œí€€ìŠ¤ ëª¨ë“œì—ì„œëŠ” carryë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ì •ë³´ ì „ë‹¬
        # actor_critic.get_actionì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ carryë¥¼ ì „ë‹¬
        if prev_latent is not None and not is_first_batch:
            # prev_latentì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ í˜„ì¬ ë°°ì¹˜ í¬ê¸°ì— ë§ì¶¤
            if prev_latent.shape[0] == batch_size:
                carry_latent = prev_latent.clone()
            else:
                # ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë§ˆì§€ë§‰ latentë¥¼ expandí•˜ì—¬ ì‚¬ìš©
                carry_latent = prev_latent[-1:].expand(batch_size, -1).clone()
            carry = LatentCarry(latent=carry_latent)
        else:
            carry = None
        
        # actor_critic.get_action ì§ì ‘ í˜¸ì¶œ (carry ì „ë‹¬)
        actions, log_probs, values, new_carry = self.agent.actor_critic.get_action(
            states_tensor,
            carry=carry,
            deterministic=False,
            n_cycles=None  # Deep Supervisionì€ í•™ìŠµ ë£¨í”„ì—ì„œ ì²˜ë¦¬
        )
        
        # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•œ latent ì—…ë°ì´íŠ¸ (ì´ˆê¸° ì•¡ì…˜ ì„ íƒ í›„)
        # new_carry.latentì˜ ë°°ì¹˜ í¬ê¸° í™•ì¸
        if new_carry is not None:
            if new_carry.latent.shape[0] == batch_size:
                latent = new_carry.latent.clone()
            else:
                # ë°°ì¹˜ í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë§ˆì§€ë§‰ latentë¥¼ expand
                latent = new_carry.latent[-1:].expand(batch_size, -1).clone()
        actions_np = actions.cpu().numpy().flatten()
        
        # ë¦¬ì›Œë“œ ê³„ì‚°
        rewards = np.array([
            self.compute_imitation_reward(pred, expert)
            for pred, expert in zip(actions_np, expert_actions)
        ])
        rewards_tensor = torch.FloatTensor(rewards).to(self.device)
        
        # Advantage ê³„ì‚° (detachí•˜ì—¬ ê·¸ë˜í”„ ë¶„ë¦¬)
        advantages = (rewards_tensor - values.squeeze()).detach()
        old_log_probs = log_probs.detach()
        
        # í†µê³„ ëˆ„ì ìš©
        total_loss_sum = 0
        total_actor_loss_sum = 0
        total_value_loss_sum = 0
        total_entropy_sum = 0
        
        # TRM ìŠ¤íƒ€ì¼: Step-wise Update (Kë²ˆ ë°˜ë³µ)
        # ì‹œí€€ìŠ¤ ëª¨ë“œì—ì„œëŠ” ê° ìƒíƒœë§ˆë‹¤ latentë¥¼ ì „ë‹¬
        # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì „ë‹¬í•  latent ì´ˆê¸°í™” (ì´ˆê¸° ì•¡ì…˜ ì„ íƒ í›„ì˜ latent)
        
        if self.agent.use_recurrent and self.agent.deep_supervision:
            # Kë²ˆì˜ Supervision Loop
            # ê° stepì—ì„œ ìƒˆë¡œìš´ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ latentë¥¼ detach
            # latentê°€ ê·¸ë˜í”„ì— ì—°ê²°ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ detach
            if new_carry is not None:
                current_latent = latent.detach().clone().requires_grad_(False)
            else:
                current_latent = latent.clone().detach().requires_grad_(False)
            
            for step in range(self.agent.n_supervision_steps):
                # ë§¤ stepë§ˆë‹¤ ì™„ì „íˆ ìƒˆë¡œìš´ forward passë¥¼ ìœ„í•´ ëª¨ë“  í…ì„œë¥¼ ìƒˆë¡œ ìƒì„±
                # numpy arrayì—ì„œ ìƒˆë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë˜í”„ ì—°ê²° ë°©ì§€
                states_tensor_fresh = torch.FloatTensor(states).to(self.device)
                expert_actions_tensor_fresh = torch.LongTensor(expert_actions).to(self.device)
                rewards_tensor_fresh = torch.FloatTensor(rewards).to(self.device)
                
                # 1. State Encoding (ë§¤ stepë§ˆë‹¤ ìƒˆë¡œ ê³„ì‚° - ìƒˆë¡œìš´ ê·¸ë˜í”„)
                state_emb = self.agent.actor_critic.encoder(states_tensor_fresh)
                
                # 2. Deep Recursion (One Step of M x N)
                # current_latentëŠ” detachë˜ì–´ ìˆì–´ì„œ ìƒˆë¡œìš´ ê·¸ë˜í”„ë¥¼ ìƒì„±
                next_latent, latent_grad, value, action_output = self.agent.actor_critic.deep_recursion(
                    state_emb, current_latent, self.agent.n_deep_loops, self.agent.n_latent_loops
                )
                
                # 3. Loss Calculation
                value_pred = value.squeeze(-1)
                # rewards_tensor_freshë¥¼ detachí•˜ì—¬ value lossì—ë§Œ ì‚¬ìš© (advantage ê³„ì‚°ì€ ì´ˆê¸° ì•¡ì…˜ ê¸°ì¤€)
                value_loss = F.mse_loss(value_pred, rewards_tensor_fresh.detach())
                
                # Policy Loss & Entropy
                if self.agent.actor_critic.discrete_action:
                    action_logits = action_output
                    dist = torch.distributions.Categorical(logits=action_logits)
                    new_log_probs = dist.log_prob(expert_actions_tensor_fresh.squeeze(-1))
                    entropy = dist.entropy().mean()
                else:
                    action_mean, action_log_std = action_output
                    std = torch.exp(action_log_std)
                    dist = torch.distributions.Normal(action_mean, std)
                    action_inv = torch.atanh(torch.clamp(expert_actions_tensor_fresh, -0.999, 0.999))
                    log_prob = dist.log_prob(action_inv).sum(dim=-1, keepdim=True)
                    log_prob -= torch.log(1 - torch.tanh(action_inv).pow(2) + 1e-6).sum(dim=-1, keepdim=True)
                    new_log_probs = log_prob
                    entropy = dist.entropy().sum(dim=-1, keepdim=True).mean()
                
                # Ratio & Surrogate Loss (Deep Supervisionì—ì„œëŠ” ë§¤ stepë§ˆë‹¤ ìƒˆë¡œìš´ advantages ê³„ì‚°)
                # í˜„ì¬ stepì˜ valueë¡œ advantages ì¬ê³„ì‚°
                current_advantages = (rewards_tensor_fresh - value_pred).detach()
                ratio = torch.exp(new_log_probs - old_log_probs)
                surr1 = ratio * current_advantages
                surr2 = torch.clamp(ratio, 1 - self.agent.clip_epsilon, 1 + self.agent.clip_epsilon) * current_advantages
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # Total Loss for this step
                loss = actor_loss + self.agent.value_coef * value_loss - self.agent.entropy_coef * entropy
                
                # 4. Backward & Update (ê° stepë§ˆë‹¤ ë…ë¦½ì ì¸ ê·¸ë˜í”„)
                self.agent.optimizer.zero_grad()
                loss.backward(retain_graph=False)  # retain_graph=Falseë¡œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
                
                torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
                self.agent.optimizer.step()
                
                # í†µê³„ ëˆ„ì 
                total_loss_sum += loss.item()
                total_actor_loss_sum += actor_loss.item()
                total_value_loss_sum += value_loss.item()
                total_entropy_sum += entropy.item()
                
                # 5. ë‹¤ìŒ stepì„ ìœ„í•œ latent ì¤€ë¹„ (detachí•˜ì—¬ ìƒˆë¡œìš´ ê·¸ë˜í”„ ìƒì„±)
                # next_latentëŠ” deep_recursionì—ì„œ ì´ë¯¸ detachë˜ì–´ ë°˜í™˜ë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ ë‹¤ì‹œ detach
                current_latent = next_latent.detach().clone().requires_grad_(False)
            
            # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì „ë‹¬í•  latent (ë§ˆì§€ë§‰ ìƒíƒœì˜ latent, ë£¨í”„ í›„ current_latent ì‚¬ìš©)
            # current_latentëŠ” ë§ˆì§€ë§‰ stepì—ì„œ ê³„ì‚°ëœ latent
            if current_latent is not None and current_latent.shape[0] > 0:
                next_latent = current_latent[-1:].detach().clone()  # ë§ˆì§€ë§‰ ìƒíƒœì˜ latentë§Œ ì „ë‹¬
            else:
                next_latent = None
        else:
            # ê¸°ì¡´ ë°©ì‹
            new_log_probs, new_values, entropy = self.agent.actor_critic.evaluate(
                states_tensor, expert_actions_tensor.unsqueeze(-1)
            )
            
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.agent.clip_epsilon, 1.0 + self.agent.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            value_loss = F.mse_loss(new_values.squeeze(), rewards_tensor)
            entropy_loss = -entropy.mean()
            
            total_loss = actor_loss + self.agent.value_coef * value_loss + self.agent.entropy_coef * entropy_loss
            
            self.agent.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
            self.agent.optimizer.step()
            
            total_loss_sum = total_loss.item()
            total_actor_loss_sum = actor_loss.item()
            total_value_loss_sum = value_loss.item()
            total_entropy_sum = entropy.mean().item()
            
            # ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•œ latent ì—…ë°ì´íŠ¸ (Deep Supervisionì´ ì•„ë‹Œ ê²½ìš°)
            if new_carry is not None:
                next_latent = new_carry.latent[-1:].detach() if new_carry.latent.shape[0] > 0 else None
        
        match_rate = np.mean(actions_np == expert_actions)
        avg_reward = np.mean(rewards)
        
        stats = {
            'total_loss': total_loss_sum / self.agent.n_supervision_steps if self.agent.use_recurrent else total_loss_sum,
            'actor_loss': total_actor_loss_sum / self.agent.n_supervision_steps if self.agent.use_recurrent else total_actor_loss_sum,
            'value_loss': total_value_loss_sum / self.agent.n_supervision_steps if self.agent.use_recurrent else total_value_loss_sum,
            'entropy': total_entropy_sum / self.agent.n_supervision_steps if self.agent.use_recurrent else total_entropy_sum,
            'match_rate': match_rate,
            'avg_reward': avg_reward
        }
        
        return stats, next_latent
    
    def train_step(self, states: np.ndarray, expert_actions: np.ndarray):
        """
        ë‹¨ì¼ í•™ìŠµ ìŠ¤í… (TRM ìŠ¤íƒ€ì¼ Step-wise Update)
        
        Args:
            states: ìƒíƒœ ë°°ì—´ [batch_size, 256]
            expert_actions: ì „ë¬¸ê°€ ì•¡ì…˜ ë°°ì—´ [batch_size]
        """
        states_tensor = torch.FloatTensor(states).to(self.device)
        expert_actions_tensor = torch.LongTensor(expert_actions).to(self.device)
        
        # ì´ˆê¸° ì•¡ì…˜ ì„ íƒ (ë¦¬ì›Œë“œ ê³„ì‚°ìš©)
        actions, log_probs, values = self.agent.actor_critic.get_action(states_tensor)
        actions_np = actions.cpu().numpy().flatten()
        
        # ë¦¬ì›Œë“œ ê³„ì‚° (ì¼ì¹˜ìœ¨ ê¸°ë°˜)
        rewards = np.array([
            self.compute_imitation_reward(pred, expert)
            for pred, expert in zip(actions_np, expert_actions)
        ])
        rewards_tensor = torch.FloatTensor(rewards).to(self.device)
        
        # Advantage ê³„ì‚°
        advantages = rewards_tensor - values.squeeze()
        old_log_probs = log_probs.detach()
        
        # í†µê³„ ëˆ„ì ìš©
        total_loss_sum = 0
        total_actor_loss_sum = 0
        total_value_loss_sum = 0
        total_entropy_sum = 0
        
        # TRM ìŠ¤íƒ€ì¼: Step-wise Update (Kë²ˆ ë°˜ë³µ)
        if self.agent.use_recurrent and self.agent.deep_supervision:
            batch_size = states_tensor.shape[0]
            # Latent ì´ˆê¸°í™”
            latent = self.agent.actor_critic.init_latent.unsqueeze(0).expand(batch_size, -1).clone()
            
            # Kë²ˆì˜ Supervision Loop
            # ê° stepì—ì„œ ìƒˆë¡œìš´ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ latentë¥¼ detach
            current_latent_step = latent.clone().detach().requires_grad_(False)
            
            for step in range(self.agent.n_supervision_steps):
                # ë§¤ stepë§ˆë‹¤ ì™„ì „íˆ ìƒˆë¡œìš´ forward passë¥¼ ìœ„í•´ í…ì„œë¥¼ ìƒˆë¡œ ìƒì„±
                states_tensor_step = torch.FloatTensor(states).to(self.device)
                expert_actions_tensor_step = torch.LongTensor(expert_actions).to(self.device)
                rewards_tensor_step = torch.FloatTensor(rewards).to(self.device)
                
                # 1. State Encoding
                state_emb = self.agent.actor_critic.encoder(states_tensor_step)
                
                # 2. Deep Recursion (One Step of M x N)
                next_latent, latent_grad, value, action_output = self.agent.actor_critic.deep_recursion(
                    state_emb, current_latent_step, self.agent.n_deep_loops, self.agent.n_latent_loops
                )
                
                # 3. Loss Calculation for THIS step
                value_pred = value.squeeze(-1)
                value_loss = F.mse_loss(value_pred, rewards_tensor_step)
                
                # Policy Loss & Entropy
                if self.agent.actor_critic.discrete_action:
                    action_logits = action_output
                    dist = torch.distributions.Categorical(logits=action_logits)
                    new_log_probs = dist.log_prob(expert_actions_tensor_step.squeeze(-1))
                    entropy = dist.entropy().mean()
                else:
                    action_mean, action_log_std = action_output
                    std = torch.exp(action_log_std)
                    dist = torch.distributions.Normal(action_mean, std)
                    action_inv = torch.atanh(torch.clamp(expert_actions_tensor_step, -0.999, 0.999))
                    log_prob = dist.log_prob(action_inv).sum(dim=-1, keepdim=True)
                    log_prob -= torch.log(1 - torch.tanh(action_inv).pow(2) + 1e-6).sum(dim=-1, keepdim=True)
                    new_log_probs = log_prob
                    entropy = dist.entropy().sum(dim=-1, keepdim=True).mean()
                
                # Ratio & Surrogate Loss
                ratio = torch.exp(new_log_probs - old_log_probs)
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - self.agent.clip_epsilon, 1 + self.agent.clip_epsilon) * advantages
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # Total Loss for this step
                loss = actor_loss + self.agent.value_coef * value_loss - self.agent.entropy_coef * entropy
                
                # 4. Backward & Update (IMMEDIATELY - TRM Style)
                self.agent.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
                self.agent.optimizer.step()
                
                # í†µê³„ ëˆ„ì 
                total_loss_sum += loss.item()
                total_actor_loss_sum += actor_loss.item()
                total_value_loss_sum += value_loss.item()
                total_entropy_sum += entropy.item()
                
                # 5. Pass detached latent to next step
                current_latent_step = next_latent.detach().clone().requires_grad_(False)
        else:
            # ê¸°ì¡´ ë°©ì‹ (Non-recurrent ë˜ëŠ” Deep Supervision ë¹„í™œì„±í™”)
            new_log_probs, new_values, entropy = self.agent.actor_critic.evaluate(
                states_tensor, expert_actions_tensor.unsqueeze(-1)
            )
            
            # PPO ì†ì‹¤ ê³„ì‚°
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.agent.clip_epsilon, 1.0 + self.agent.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # Value loss
            value_loss = F.mse_loss(new_values.squeeze(), rewards_tensor)
            
            # Entropy
            entropy_loss = -entropy.mean()
            
            # ì´ ì†ì‹¤
            total_loss = (
                actor_loss +
                self.agent.value_coef * value_loss +
                self.agent.entropy_coef * entropy_loss
            )
            
            # ì—­ì „íŒŒ
            self.agent.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
            self.agent.optimizer.step()
            
            total_loss_sum = total_loss.item()
            total_actor_loss_sum = actor_loss.item()
            total_value_loss_sum = value_loss.item()
            total_entropy_sum = entropy.mean().item()
        
        # í†µê³„ (í‰ê·  ê³„ì‚°)
        if self.agent.use_recurrent and self.agent.deep_supervision:
            n_steps = self.agent.n_supervision_steps
        else:
            n_steps = 1
        
        match_rate = np.mean(actions_np == expert_actions)
        avg_reward = np.mean(rewards)
        
        return {
            'total_loss': total_loss_sum / n_steps,
            'actor_loss': total_actor_loss_sum / n_steps,
            'value_loss': total_value_loss_sum / n_steps,
            'entropy': total_entropy_sum / n_steps,
            'match_rate': match_rate,
            'avg_reward': avg_reward
        }
    
    def train(self, epochs: int = 100, save_path: str = 'imitation_rl_model.pth', verbose: bool = True):
        """
        í•™ìŠµ ì‹¤í–‰
        
        Args:
            epochs: í•™ìŠµ ì—í­ ìˆ˜
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            verbose: ìƒì„¸ ì¶œë ¥
        """
        print(f"\n{'='*60}")
        print("Imitation Learning via Reinforcement Learning ì‹œì‘")
        print(f"{'='*60}")
        print(f"ë°ëª¨ ë°ì´í„°: {len(self.demo_states)}ê°œ ìƒ˜í”Œ")
        print(f"ì—í”¼ì†Œë“œ ìˆ˜: {len(self.demos)}ê°œ")
        print(f"í•™ìŠµ ì—í­: {epochs}")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ë°°ì¹˜ë‹¹ ì—…ë°ì´íŠ¸: {self.update_epochs}ë²ˆ")
        
        # ì „ì²´ ì‘ì—…ëŸ‰ ê³„ì‚°
        total_samples = len(self.demo_states)
        total_batches = 0
        if self.use_sequence_mode:
            for episode in self.demos:
                episode_len = len(episode.get('states', []))
                if episode_len > 0:
                    total_batches += (episode_len + self.batch_size - 1) // self.batch_size
        else:
            total_batches = (total_samples + self.batch_size - 1) // self.batch_size
        
        total_updates = total_batches * self.update_epochs * epochs
        print(f"ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ/ì—í­")
        print(f"ì´ ì—…ë°ì´íŠ¸: {total_updates:,}ë²ˆ ({total_batches} ë°°ì¹˜ Ã— {self.update_epochs} ì—…ë°ì´íŠ¸ Ã— {epochs} ì—í­)")
        print(f"{'='*60}\n")
        
        # ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        epoch_start_time = time.time()
        
        for epoch in range(epochs):
            epoch_stats = {
                'total_loss': [],
                'actor_loss': [],
                'value_loss': [],
                'entropy': [],
                'match_rate': [],
                'avg_reward': []
            }
            
            if self.use_sequence_mode:
                # ì‹œí€€ìŠ¤ ëª¨ë“œ: ì—í”¼ì†Œë“œë³„ë¡œ í•™ìŠµ, ì´ì „ latent ì „ë‹¬
                # ì—í”¼ì†Œë“œ ìˆœì„œ ì…”í”Œ (ì—í”¼ì†Œë“œ ë‚´ ì‹œí€€ìŠ¤ëŠ” ìœ ì§€)
                episode_indices = list(range(len(self.demos)))
                np.random.shuffle(episode_indices)
                
                # ì—í¬í¬ ì§„í–‰ ìƒí™© í‘œì‹œ
                epoch_progress = (epoch + 1) / epochs * 100
                elapsed_time = time.time() - start_time
                if epoch > 0:
                    avg_epoch_time = elapsed_time / epoch
                    remaining_epochs = epochs - epoch - 1
                    eta_seconds = avg_epoch_time * remaining_epochs
                    eta_str = str(timedelta(seconds=int(eta_seconds)))
                else:
                    eta_str = "ê³„ì‚° ì¤‘..."
                
                print(f"\n[{epoch+1}/{epochs}] ì—í¬í¬ ì‹œì‘ ({epoch_progress:.1f}%) | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta_str}")
                print(f"{'='*60}")
                
                episode_count = 0
                batch_count = 0
                update_count = 0
                
                for ep_idx in episode_indices:
                    episode = self.demos[ep_idx]
                    states = np.array(episode.get('states', []))
                    actions = np.array(episode.get('actions', []))
                    
                    if len(states) == 0 or len(actions) == 0:
                        continue
                    
                    # ì—í”¼ì†Œë“œ ë‚´ ì‹œí€€ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ (latent ì „ë‹¬)
                    episode_count += 1
                    episode_len = len(states)
                    num_batches_episode = (episode_len + self.batch_size - 1) // self.batch_size
                    
                    prev_latent = None
                    for batch_idx, i in enumerate(range(0, len(states), self.batch_size)):
                        batch_states = states[i:i+self.batch_size]
                        batch_actions = actions[i:i+self.batch_size]
                        
                        if len(batch_states) < 1:  # ìµœì†Œ 1ê°œëŠ” í•„ìš”
                            continue
                        
                        batch_count += 1
                        
                        # ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸
                        for update_iter in range(self.update_epochs):
                            is_first = (i == 0 and update_iter == 0)
                            stats, prev_latent = self.train_step_sequence(
                                batch_states, 
                                batch_actions,
                                is_first_batch=is_first,
                                prev_latent=prev_latent if not is_first else None
                            )
                            
                            update_count += 1
                            
                            for key, value in stats.items():
                                epoch_stats[key].append(value)
                        
                        # ë°°ì¹˜ ì§„í–‰ ìƒí™© ì¶œë ¥ (ì—í”¼ì†Œë“œë‹¹ ì²« ë°°ì¹˜ì™€ ë§ˆì§€ë§‰ ë°°ì¹˜, ë˜ëŠ” 5ê°œ ë°°ì¹˜ë§ˆë‹¤)
                        should_print = (batch_idx == 0 or 
                                       batch_idx == num_batches_episode - 1 or 
                                       (batch_idx + 1) % 5 == 0)
                        
                        if should_print and epoch_stats.get('match_rate'):
                            current_match_rate = np.mean(epoch_stats['match_rate'])
                            current_loss = np.mean(epoch_stats['total_loss']) if epoch_stats.get('total_loss') else 0
                            print(f"  [ì—í”¼ì†Œë“œ {episode_count}/{len(episode_indices)}] "
                                  f"ë°°ì¹˜ {batch_idx+1}/{num_batches_episode} "
                                  f"| ì—…ë°ì´íŠ¸: {update_count:,} | "
                                  f"Match: {current_match_rate:.1%} | "
                                  f"Loss: {current_loss:.4f}", end='\r', flush=True)
                
                # ëª¨ë“  ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì™„ë£Œ í›„ ì¤„ë°”ê¿ˆ
                if episode_count == len(episode_indices):
                    print()  # ì¤„ë°”ê¿ˆ
            else:
                # ê¸°ì¡´ ëª¨ë“œ: ì…”í”Œëœ ë…ë¦½ ìƒ˜í”Œ í•™ìŠµ
                states_array = np.array(self.demo_states)  # [N, 256]
                actions_array = np.array(self.demo_actions)  # [N]
                
                # ë°ì´í„° ì…”í”Œ
                indices = np.arange(len(states_array))
                np.random.shuffle(indices)
                shuffled_states = states_array[indices]
                shuffled_actions = actions_array[indices]
                
                # ì—í¬í¬ ì§„í–‰ ìƒí™© í‘œì‹œ (ê¸°ì¡´ ëª¨ë“œ)
                epoch_progress = (epoch + 1) / epochs * 100
                elapsed_time = time.time() - start_time
                if epoch > 0:
                    avg_epoch_time = elapsed_time / epoch
                    remaining_epochs = epochs - epoch - 1
                    eta_seconds = avg_epoch_time * remaining_epochs
                    eta_str = str(timedelta(seconds=int(eta_seconds)))
                else:
                    eta_str = "ê³„ì‚° ì¤‘..."
                
                print(f"\n[{epoch+1}/{epochs}] ì—í¬í¬ ì‹œì‘ ({epoch_progress:.1f}%) | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta_str}")
                print(f"{'='*60}")
                
                num_batches = (len(shuffled_states) + self.batch_size - 1) // self.batch_size
                batch_count = 0
                update_count = 0
                
                # ë°°ì¹˜ë³„ í•™ìŠµ
                for batch_idx, i in enumerate(range(0, len(shuffled_states), self.batch_size)):
                    batch_states = shuffled_states[i:i+self.batch_size]
                    batch_actions = shuffled_actions[i:i+self.batch_size]
                    
                    if len(batch_states) < self.batch_size:
                        continue
                    
                    batch_count += 1
                    
                    # ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸
                    for _ in range(self.update_epochs):
                        stats = self.train_step(batch_states, batch_actions)
                        update_count += 1
                        
                        for key, value in stats.items():
                            epoch_stats[key].append(value)
                    
                    # ë°°ì¹˜ ì§„í–‰ ìƒí™© ì¶œë ¥ (5ê°œ ë°°ì¹˜ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ ë°°ì¹˜)
                    if ((batch_idx + 1) % 5 == 0 or batch_idx == num_batches - 1) and epoch_stats.get('match_rate'):
                        current_match_rate = np.mean(epoch_stats['match_rate'])
                        current_loss = np.mean(epoch_stats['total_loss']) if epoch_stats.get('total_loss') else 0
                        print(f"  ë°°ì¹˜ {batch_idx+1}/{num_batches} | "
                              f"ì—…ë°ì´íŠ¸: {update_count:,} | "
                              f"Match: {current_match_rate:.1%} | "
                              f"Loss: {current_loss:.4f}", end='\r', flush=True)
                
                print()  # ì¤„ë°”ê¿ˆ
            
            # ì—í­ í†µê³„
            epoch_time = time.time() - epoch_start_time
            epoch_start_time = time.time()
            
            if verbose:
                avg_loss = np.mean(epoch_stats['total_loss']) if epoch_stats['total_loss'] else 0
                avg_actor_loss = np.mean(epoch_stats['actor_loss']) if epoch_stats['actor_loss'] else 0
                avg_value_loss = np.mean(epoch_stats['value_loss']) if epoch_stats['value_loss'] else 0
                avg_match_rate = np.mean(epoch_stats['match_rate']) if epoch_stats['match_rate'] else 0
                avg_reward = np.mean(epoch_stats['avg_reward']) if epoch_stats['avg_reward'] else 0
                avg_entropy = np.mean(epoch_stats['entropy']) if epoch_stats['entropy'] else 0
                
                # ì—í¬í¬ë³„ í†µê³„ ì¶œë ¥
                print(f"\n[ì—í¬í¬ {epoch+1}/{epochs} ì™„ë£Œ] ({epoch_time:.1f}ì´ˆ)")
                print(f"  ğŸ“Š í†µê³„:")
                print(f"    - Match Rate: {avg_match_rate:.2%} (ëª©í‘œ: 100%)")
                print(f"    - Avg Reward: {avg_reward:.4f}")
                print(f"    - Total Loss: {avg_loss:.4f}")
                print(f"    - Actor Loss: {avg_actor_loss:.4f}")
                print(f"    - Value Loss: {avg_value_loss:.4f}")
                print(f"    - Entropy: {avg_entropy:.4f}")
                print(f"  ğŸ“ˆ ì—…ë°ì´íŠ¸: {len(epoch_stats['total_loss']):,}ë²ˆ")
                
                # ì „ì²´ ì§„í–‰ ìƒí™©
                total_progress = ((epoch + 1) / epochs) * 100
                elapsed_total = time.time() - start_time
                if epoch > 0:
                    avg_epoch_time = elapsed_total / (epoch + 1)
                    remaining_epochs = epochs - epoch - 1
                    eta_seconds = avg_epoch_time * remaining_epochs
                    eta_str = str(timedelta(seconds=int(eta_seconds)))
                    total_eta_str = str(timedelta(seconds=int(elapsed_total + eta_seconds)))
                    
                    print(f"  â±ï¸  ì‹œê°„: {str(timedelta(seconds=int(elapsed_total)))} / "
                          f"ì˜ˆìƒ ì´ ì‹œê°„: {total_eta_str} (ë‚¨ì€: {eta_str})")
                    print(f"  ğŸ“ ì§„í–‰ë¥ : {total_progress:.1f}% ({'â–ˆ' * int(total_progress / 2)}{'â–‘' * (50 - int(total_progress / 2))})")
                print()
        
        # ëª¨ë¸ ì €ì¥
        self.agent.save(save_path)
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ì´ í•™ìŠµ ì‹œê°„: {str(timedelta(seconds=int(total_time)))}")
        print(f"í‰ê·  ì—í¬í¬ ì‹œê°„: {total_time/epochs:.1f}ì´ˆ")
        print(f"ì´ ì—…ë°ì´íŠ¸ íšŸìˆ˜: {total_updates:,}ë²ˆ")
        print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {save_path}")
        print(f"{'='*60}")
        
        # ìµœì¢… í‰ê°€
        print("\nğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
        final_match_rate = self.evaluate()
        print(f"\nğŸ¯ ìµœì¢… ì¼ì¹˜ìœ¨: {final_match_rate:.2%}")
        print(f"{'='*60}\n")
    
    def evaluate(self, num_samples: int = 1000) -> float:
        """
        ëª¨ë¸ í‰ê°€ (ì¼ì¹˜ìœ¨ ê³„ì‚°)
        
        Args:
            num_samples: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            match_rate: ì¼ì¹˜ìœ¨ (0.0 ~ 1.0)
        """
        self.agent.actor_critic.eval()
        
        indices = np.random.choice(len(self.demo_states), min(num_samples, len(self.demo_states)), replace=False)
        test_states = np.array([self.demo_states[i] for i in indices])
        test_actions = np.array([self.demo_actions[i] for i in indices])
        
        states_tensor = torch.FloatTensor(test_states).to(self.device)
        
        with torch.no_grad():
            # RecurrentActorCritic.get_actionì€ 4ê°œ ê°’ì„ ë°˜í™˜: (action, log_prob, value, new_carry)
            actions, _, _, _ = self.agent.actor_critic.get_action(states_tensor)
            actions_np = actions.cpu().numpy().flatten()
        
        match_rate = np.mean(actions_np == test_actions)
        
        self.agent.actor_critic.train()
        
        return match_rate


def main():
    parser = argparse.ArgumentParser(
        description='Imitation Learning via Reinforcement Learning',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í•™ìŠµ
  python train_imitation_rl.py --demos human_demos.pkl --epochs 100
  
  # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹œì‘
  python train_imitation_rl.py --demos human_demos.pkl --model pretrained.pth --epochs 100
        """
    )
    
    parser.add_argument('--demos', type=str, required=True,
                        help='ì‚¬ìš©ì ë°ëª¨ ë°ì´í„° ê²½ë¡œ (pickle íŒŒì¼)')
    parser.add_argument('--model', type=str, default=None,
                        help='ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒ)')
    parser.add_argument('--epochs', type=int, default=100,
                        help='í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--batch-size', type=int, default=64,
                        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)')
    parser.add_argument('--learning-rate', type=float, default=3e-4,
                        help='í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)')
    parser.add_argument('--save', type=str, default='imitation_rl_model.pth',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: imitation_rl_model.pth)')
    parser.add_argument('--device', type=str, default='cpu',
                        help='ë””ë°”ì´ìŠ¤ (ê¸°ë³¸: cpu)')
    
    args = parser.parse_args()
    
    # Trainer ìƒì„±
    trainer = ImitationRLTrainer(
        demos_path=args.demos,
        model_path=args.model,
        device=args.device,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train(
        epochs=args.epochs,
        save_path=args.save,
        verbose=True
    )


if __name__ == '__main__':
    main()

