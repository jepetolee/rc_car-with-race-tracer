#!/usr/bin/env python3
"""
Imitation Learning via Reinforcement Learning
ì‚¬ìš©ì ë°ëª¨ ë°ì´í„°ì™€ì˜ ì¼ì¹˜ìœ¨ì„ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©í•˜ì—¬ ê°•í™”í•™ìŠµ

Supervised Learningì´ ì•„ë‹Œ ê°•í™”í•™ìŠµìœ¼ë¡œ:
- ëª¨ë¸ì´ ì•¡ì…˜ì„ ì„ íƒ
- ì‚¬ìš©ìê°€ ì„ íƒí•œ ì•¡ì…˜ê³¼ ë¹„êµ
- ì¼ì¹˜ìœ¨ì— ë”°ë¼ ë¦¬ì›Œë“œ ë¶€ì—¬
- PPOë¡œ í•™ìŠµ
"""

import os
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['TORCH_NUM_THREADS'] = '1'

import argparse
import pickle
import numpy as np
import torch
import torch.nn.functional as F
from datetime import datetime
import sys

from ppo_agent import PPOAgent
from train_ppo import train_ppo


class ImitationRLTrainer:
    """
    ì‚¬ìš©ì ë°ëª¨ ë°ì´í„°ì™€ì˜ ì¼ì¹˜ìœ¨ì„ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©í•˜ëŠ” ê°•í™”í•™ìŠµ
    """
    
    def __init__(
        self,
        demos_path: str,
        model_path: str = None,
        device: str = 'cpu',
        learning_rate: float = 3e-4,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_epsilon: float = 0.2,
        value_coef: float = 0.5,
        entropy_coef: float = 0.01,
        max_grad_norm: float = 0.5,
        batch_size: int = 64,
        update_epochs: int = 10
    ):
        """
        Args:
            demos_path: ì‚¬ìš©ì ë°ëª¨ ë°ì´í„° ê²½ë¡œ (pickle íŒŒì¼)
            model_path: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒ)
            device: ë””ë°”ì´ìŠ¤
            learning_rate: í•™ìŠµë¥ 
            gamma: í• ì¸ ê³„ìˆ˜
            gae_lambda: GAE ëŒë‹¤
            clip_epsilon: PPO clip epsilon
            value_coef: Value loss ê³„ìˆ˜
            entropy_coef: Entropy ê³„ìˆ˜
            max_grad_norm: Gradient clipping
            batch_size: ë°°ì¹˜ í¬ê¸°
            update_epochs: ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
        """
        self.demos_path = demos_path
        self.device = device
        self.batch_size = batch_size
        self.update_epochs = update_epochs
        
        # ë°ëª¨ ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“‚ ë°ëª¨ ë°ì´í„° ë¡œë“œ: {demos_path}")
        with open(demos_path, 'rb') as f:
            data = pickle.load(f)
        
        self.demos = data.get('demonstrations', [])
        if len(self.demos) == 0:
            raise ValueError("ë°ëª¨ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… {len(self.demos)}ê°œ ì—í”¼ì†Œë“œ ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë“  (state, action) ìŒ ì¶”ì¶œ
        self.demo_states = []
        self.demo_actions = []
        
        for episode in self.demos:
            states = episode.get('states', [])
            actions = episode.get('actions', [])
            
            if len(states) != len(actions):
                print(f"âš ï¸  ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶ˆì¼ì¹˜: states={len(states)}, actions={len(actions)}")
                min_len = min(len(states), len(actions))
                states = states[:min_len]
                actions = actions[:min_len]
            
            self.demo_states.extend(states)
            self.demo_actions.extend(actions)
        
        print(f"âœ… ì´ {len(self.demo_states)}ê°œ (state, action) ìŒ")
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        print(f"\nğŸ¤– ì—ì´ì „íŠ¸ ìƒì„±...")
        self.agent = PPOAgent(
            state_dim=256,
            action_dim=5,
            discrete_action=True,
            learning_rate=learning_rate,
            gamma=gamma,
            gae_lambda=gae_lambda,
            clip_epsilon=clip_epsilon,
            value_coef=value_coef,
            entropy_coef=entropy_coef,
            max_grad_norm=max_grad_norm,
            device=device,
            use_recurrent=True,  # Deep Supervisionì„ ìœ„í•´ Recurrent í™œì„±í™”
            deep_supervision=True
        )
        
        # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
        if model_path and os.path.exists(model_path):
            print(f"ğŸ“¥ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ: {model_path}")
            try:
                self.agent.load(model_path)
                print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ëœë¤ ì´ˆê¸°í™”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    def compute_imitation_reward(self, predicted_action: int, expert_action: int) -> float:
        """
        ëª¨ë¸ì˜ ì•¡ì…˜ê³¼ ì „ë¬¸ê°€ ì•¡ì…˜ì˜ ì¼ì¹˜ìœ¨ì— ë”°ë¥¸ ë¦¬ì›Œë“œ ê³„ì‚°
        
        Args:
            predicted_action: ëª¨ë¸ì´ ì„ íƒí•œ ì•¡ì…˜
            expert_action: ì‚¬ìš©ìê°€ ì„ íƒí•œ ì•¡ì…˜
        
        Returns:
            reward: ì¼ì¹˜í•˜ë©´ 1.0, ë¶ˆì¼ì¹˜í•˜ë©´ -0.1
        """
        if predicted_action == expert_action:
            return 1.0  # ì™„ì „ ì¼ì¹˜
        else:
            return -0.1  # ë¶ˆì¼ì¹˜ í˜ë„í‹°
    
    def train_step(self, states: np.ndarray, expert_actions: np.ndarray):
        """
        ë‹¨ì¼ í•™ìŠµ ìŠ¤í… (TRM ìŠ¤íƒ€ì¼ Step-wise Update)
        
        Args:
            states: ìƒíƒœ ë°°ì—´ [batch_size, 256]
            expert_actions: ì „ë¬¸ê°€ ì•¡ì…˜ ë°°ì—´ [batch_size]
        """
        states_tensor = torch.FloatTensor(states).to(self.device)
        expert_actions_tensor = torch.LongTensor(expert_actions).to(self.device)
        
        # ì´ˆê¸° ì•¡ì…˜ ì„ íƒ (ë¦¬ì›Œë“œ ê³„ì‚°ìš©)
        actions, log_probs, values = self.agent.actor_critic.get_action(states_tensor)
        actions_np = actions.cpu().numpy().flatten()
        
        # ë¦¬ì›Œë“œ ê³„ì‚° (ì¼ì¹˜ìœ¨ ê¸°ë°˜)
        rewards = np.array([
            self.compute_imitation_reward(pred, expert)
            for pred, expert in zip(actions_np, expert_actions)
        ])
        rewards_tensor = torch.FloatTensor(rewards).to(self.device)
        
        # Advantage ê³„ì‚°
        advantages = rewards_tensor - values.squeeze()
        old_log_probs = log_probs.detach()
        
        # í†µê³„ ëˆ„ì ìš©
        total_loss_sum = 0
        total_actor_loss_sum = 0
        total_value_loss_sum = 0
        total_entropy_sum = 0
        
        # TRM ìŠ¤íƒ€ì¼: Step-wise Update (Kë²ˆ ë°˜ë³µ)
        if self.agent.use_recurrent and self.agent.deep_supervision:
            batch_size = states_tensor.shape[0]
            # Latent ì´ˆê¸°í™”
            latent = self.agent.actor_critic.init_latent.unsqueeze(0).expand(batch_size, -1).clone()
            
            # Kë²ˆì˜ Supervision Loop
            for step in range(self.agent.n_supervision_steps):
                # 1. State Encoding
                state_emb = self.agent.actor_critic.encoder(states_tensor)
                
                # 2. Deep Recursion (One Step of M x N)
                next_latent, latent_grad, value, action_output = self.agent.actor_critic.deep_recursion(
                    state_emb, latent, self.agent.n_deep_loops, self.agent.n_latent_loops
                )
                
                # 3. Loss Calculation for THIS step
                value_pred = value.squeeze(-1)
                value_loss = F.mse_loss(value_pred, rewards_tensor)
                
                # Policy Loss & Entropy
                if self.agent.actor_critic.discrete_action:
                    action_logits = action_output
                    dist = torch.distributions.Categorical(logits=action_logits)
                    new_log_probs = dist.log_prob(expert_actions_tensor.squeeze(-1))
                    entropy = dist.entropy().mean()
                else:
                    action_mean, action_log_std = action_output
                    std = torch.exp(action_log_std)
                    dist = torch.distributions.Normal(action_mean, std)
                    action_inv = torch.atanh(torch.clamp(expert_actions_tensor, -0.999, 0.999))
                    log_prob = dist.log_prob(action_inv).sum(dim=-1, keepdim=True)
                    log_prob -= torch.log(1 - torch.tanh(action_inv).pow(2) + 1e-6).sum(dim=-1, keepdim=True)
                    new_log_probs = log_prob
                    entropy = dist.entropy().sum(dim=-1, keepdim=True).mean()
                
                # Ratio & Surrogate Loss
                ratio = torch.exp(new_log_probs - old_log_probs)
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - self.agent.clip_epsilon, 1 + self.agent.clip_epsilon) * advantages
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # Total Loss for this step
                loss = actor_loss + self.agent.value_coef * value_loss - self.agent.entropy_coef * entropy
                
                # 4. Backward & Update (IMMEDIATELY - TRM Style)
                self.agent.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
                self.agent.optimizer.step()
                
                # í†µê³„ ëˆ„ì 
                total_loss_sum += loss.item()
                total_actor_loss_sum += actor_loss.item()
                total_value_loss_sum += value_loss.item()
                total_entropy_sum += entropy.item()
                
                # 5. Pass detached latent to next step
                latent = next_latent
        else:
            # ê¸°ì¡´ ë°©ì‹ (Non-recurrent ë˜ëŠ” Deep Supervision ë¹„í™œì„±í™”)
            new_log_probs, new_values, entropy = self.agent.actor_critic.evaluate(
                states_tensor, expert_actions_tensor.unsqueeze(-1)
            )
            
            # PPO ì†ì‹¤ ê³„ì‚°
            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.agent.clip_epsilon, 1.0 + self.agent.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # Value loss
            value_loss = F.mse_loss(new_values.squeeze(), rewards_tensor)
            
            # Entropy
            entropy_loss = -entropy.mean()
            
            # ì´ ì†ì‹¤
            total_loss = (
                actor_loss +
                self.agent.value_coef * value_loss +
                self.agent.entropy_coef * entropy_loss
            )
            
            # ì—­ì „íŒŒ
            self.agent.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), self.agent.max_grad_norm)
            self.agent.optimizer.step()
            
            total_loss_sum = total_loss.item()
            total_actor_loss_sum = actor_loss.item()
            total_value_loss_sum = value_loss.item()
            total_entropy_sum = entropy.mean().item()
        
        # í†µê³„ (í‰ê·  ê³„ì‚°)
        if self.agent.use_recurrent and self.agent.deep_supervision:
            n_steps = self.agent.n_supervision_steps
        else:
            n_steps = 1
        
        match_rate = np.mean(actions_np == expert_actions)
        avg_reward = np.mean(rewards)
        
        return {
            'total_loss': total_loss_sum / n_steps,
            'actor_loss': total_actor_loss_sum / n_steps,
            'value_loss': total_value_loss_sum / n_steps,
            'entropy': total_entropy_sum / n_steps,
            'match_rate': match_rate,
            'avg_reward': avg_reward
        }
    
    def train(self, epochs: int = 100, save_path: str = 'imitation_rl_model.pth', verbose: bool = True):
        """
        í•™ìŠµ ì‹¤í–‰
        
        Args:
            epochs: í•™ìŠµ ì—í­ ìˆ˜
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            verbose: ìƒì„¸ ì¶œë ¥
        """
        print(f"\n{'='*60}")
        print("Imitation Learning via Reinforcement Learning ì‹œì‘")
        print(f"{'='*60}")
        print(f"ë°ëª¨ ë°ì´í„°: {len(self.demo_states)}ê°œ ìƒ˜í”Œ")
        print(f"í•™ìŠµ ì—í­: {epochs}")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"{'='*60}\n")
        
        # ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
        states_array = np.array(self.demo_states)  # [N, 256]
        actions_array = np.array(self.demo_actions)  # [N]
        
        # ë°ì´í„° ì…”í”Œ
        indices = np.arange(len(states_array))
        
        for epoch in range(epochs):
            np.random.shuffle(indices)
            shuffled_states = states_array[indices]
            shuffled_actions = actions_array[indices]
            
            epoch_stats = {
                'total_loss': [],
                'actor_loss': [],
                'value_loss': [],
                'entropy': [],
                'match_rate': [],
                'avg_reward': []
            }
            
            # ë°°ì¹˜ë³„ í•™ìŠµ
            for i in range(0, len(shuffled_states), self.batch_size):
                batch_states = shuffled_states[i:i+self.batch_size]
                batch_actions = shuffled_actions[i:i+self.batch_size]
                
                if len(batch_states) < self.batch_size:
                    continue
                
                # ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸
                for _ in range(self.update_epochs):
                    stats = self.train_step(batch_states, batch_actions)
                    
                    for key, value in stats.items():
                        epoch_stats[key].append(value)
            
            # ì—í­ í†µê³„
            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}:")
                print(f"  Loss: {np.mean(epoch_stats['total_loss']):.4f}")
                print(f"  Match Rate: {np.mean(epoch_stats['match_rate']):.2%}")
                print(f"  Avg Reward: {np.mean(epoch_stats['avg_reward']):.4f}")
                print()
        
        # ëª¨ë¸ ì €ì¥
        self.agent.save(save_path)
        print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        
        # ìµœì¢… í‰ê°€
        print("\nìµœì¢… í‰ê°€ ì¤‘...")
        final_match_rate = self.evaluate()
        print(f"ìµœì¢… ì¼ì¹˜ìœ¨: {final_match_rate:.2%}")
    
    def evaluate(self, num_samples: int = 1000) -> float:
        """
        ëª¨ë¸ í‰ê°€ (ì¼ì¹˜ìœ¨ ê³„ì‚°)
        
        Args:
            num_samples: í‰ê°€í•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            match_rate: ì¼ì¹˜ìœ¨ (0.0 ~ 1.0)
        """
        self.agent.actor_critic.eval()
        
        indices = np.random.choice(len(self.demo_states), min(num_samples, len(self.demo_states)), replace=False)
        test_states = np.array([self.demo_states[i] for i in indices])
        test_actions = np.array([self.demo_actions[i] for i in indices])
        
        states_tensor = torch.FloatTensor(test_states).to(self.device)
        
        with torch.no_grad():
            actions, _, _ = self.agent.actor_critic.get_action(states_tensor)
            actions_np = actions.cpu().numpy().flatten()
        
        match_rate = np.mean(actions_np == test_actions)
        
        self.agent.actor_critic.train()
        
        return match_rate


def main():
    parser = argparse.ArgumentParser(
        description='Imitation Learning via Reinforcement Learning',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ í•™ìŠµ
  python train_imitation_rl.py --demos human_demos.pkl --epochs 100
  
  # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹œì‘
  python train_imitation_rl.py --demos human_demos.pkl --model pretrained.pth --epochs 100
        """
    )
    
    parser.add_argument('--demos', type=str, required=True,
                        help='ì‚¬ìš©ì ë°ëª¨ ë°ì´í„° ê²½ë¡œ (pickle íŒŒì¼)')
    parser.add_argument('--model', type=str, default=None,
                        help='ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì„ íƒ)')
    parser.add_argument('--epochs', type=int, default=100,
                        help='í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--batch-size', type=int, default=64,
                        help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)')
    parser.add_argument('--learning-rate', type=float, default=3e-4,
                        help='í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)')
    parser.add_argument('--save', type=str, default='imitation_rl_model.pth',
                        help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: imitation_rl_model.pth)')
    parser.add_argument('--device', type=str, default='cpu',
                        help='ë””ë°”ì´ìŠ¤ (ê¸°ë³¸: cpu)')
    
    args = parser.parse_args()
    
    # Trainer ìƒì„±
    trainer = ImitationRLTrainer(
        demos_path=args.demos,
        model_path=args.model,
        device=args.device,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train(
        epochs=args.epochs,
        save_path=args.save,
        verbose=True
    )


if __name__ == '__main__':
    main()

