#!/usr/bin/env python3
"""
Supervised Learning (Teacher Forcing)ì„ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ
ì‚¬ëŒì´ ì§ì ‘ ì¡°ì‘í•œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ supervised learningìœ¼ë¡œ ì‚¬ì „ í•™ìŠµí•œ í›„ ê°•í™”í•™ìŠµìœ¼ë¡œ fine-tuning

Teacher Forcing = Supervised Learning:
- ì‚¬ëŒì´ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒì„ ì‚¬ìš©
- Maximum Likelihood Estimation (MLE)ìœ¼ë¡œ ì •ì±… í•™ìŠµ
- ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹

ì‚¬ìš©ë²•:
    # 1ë‹¨ê³„: Supervised Learning ì‚¬ì „ í•™ìŠµ
    python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100
    
    # 2ë‹¨ê³„: ê°•í™”í•™ìŠµìœ¼ë¡œ fine-tuning
    python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100 --rl-steps 100000
"""

import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import os
import sys
from datetime import datetime
from collections import deque

# í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„í¬íŠ¸
from rc_car_sim_env import RCCarSimEnv
from car_racing_env import CarRacingEnvWrapper
from ppo_agent import PPOAgent
from train_ppo import train_ppo

# TensorBoard ì§€ì›
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False
    print("âš ï¸  TensorBoard ë¯¸ì„¤ì¹˜ - pip install tensorboard ë¡œ ì„¤ì¹˜í•˜ë©´ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥")


class TeacherForcingTrainer:
    """
    Supervised Learning (Teacher Forcing)ì„ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ í´ë˜ìŠ¤
    ì‚¬ëŒì´ ì§ì ‘ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒìœ¼ë¡œ ì •ì±…ì„ supervised learningìœ¼ë¡œ í•™ìŠµ
    
    í•™ìŠµ ë°©ì‹:
    - Maximum Likelihood Estimation (MLE)
    - Loss = -log P(ì‹¤ì œ_ì•¡ì…˜ | ìƒíƒœ)
    - ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”
    """
    
    def __init__(
        self,
        agent: PPOAgent,
        demonstrations: list,
        device: str = 'cuda',
        lr: float = 3e-4
    ):
        """
        Args:
            agent: PPO ì—ì´ì „íŠ¸
            demonstrations: ìˆ˜ì§‘ëœ ë°ëª¨ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            device: ë””ë°”ì´ìŠ¤
            lr: í•™ìŠµë¥ 
        """
        self.agent = agent
        self.device = device
        self.demonstrations = demonstrations
        
        # ì˜µí‹°ë§ˆì´ì € (Actorë§Œ í•™ìŠµ)
        self.optimizer = optim.Adam(
            self.agent.actor_critic.parameters(),
            lr=lr
        )
        
        # ë°ì´í„° ì¤€ë¹„
        self.states, self.actions = self._prepare_data()
        
        print(f"âœ… Teacher Forcing ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ì´ ìƒíƒœ ìˆ˜: {len(self.states)}")
        print(f"   ì´ ì•¡ì…˜ ìˆ˜: {len(self.actions)}")
    
    def _prepare_data(self):
        """ë°ëª¨ ë°ì´í„°ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ë³€í™˜"""
        all_states = []
        all_actions = []
        
        for episode in self.demonstrations:
            states = episode['states']
            actions = episode['actions']
            
            # ìƒíƒœì™€ ì•¡ì…˜ì„ í…ì„œë¡œ ë³€í™˜
            for state, action in zip(states, actions):
                all_states.append(state)
                all_actions.append(action)
        
        return np.array(all_states), np.array(all_actions)
    
    def train_epoch(self, batch_size: int = 64):
        """
        ë‹¨ì¼ ì—í­ í•™ìŠµ
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
        
        Returns:
            loss: í‰ê·  ì†ì‹¤
        """
        total_loss = 0.0
        num_batches = 0
        
        # ë°ì´í„° ì…”í”Œ
        indices = np.random.permutation(len(self.states))
        
        for i in range(0, len(self.states), batch_size):
            batch_indices = indices[i:i+batch_size]
            batch_states = self.states[batch_indices]
            batch_actions = self.actions[batch_indices]
            
            # í…ì„œë¡œ ë³€í™˜
            states_tensor = torch.FloatTensor(batch_states).to(self.device)
            # ì•¡ì…˜ì€ ë‚˜ì¤‘ì— discrete/continuousì— ë”°ë¼ ë³€í™˜
            actions_tensor = torch.from_numpy(batch_actions).to(self.device)
            
            # TRM-PPO ëª¨ë“œ í™•ì¸
            use_recurrent = getattr(self.agent, 'use_recurrent', False)
            
            # ì´ì‚° ì•¡ì…˜ ì²˜ë¦¬
            if self.agent.actor_critic.discrete_action:
                # ì´ì‚° ì•¡ì…˜: LongTensorë¡œ ë³€í™˜
                actions_tensor = actions_tensor.long()
                if actions_tensor.dim() == 1:
                    actions_tensor = actions_tensor.unsqueeze(-1)
            else:
                # ì—°ì† ì•¡ì…˜: FloatTensorë¡œ ë³€í™˜
                actions_tensor = actions_tensor.float()
                if actions_tensor.dim() == 1:
                    actions_tensor = actions_tensor.unsqueeze(-1)
            
            # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ ì•¡ì…˜ í™•ë¥  ê³„ì‚°
            if use_recurrent:
                # TRM-PPO: evaluate ì‚¬ìš©
                log_probs, _, _ = self.agent.actor_critic.evaluate(
                    states_tensor,
                    actions_tensor,
                    n_cycles=self.agent.n_cycles
                )
            else:
                # ê¸°ì¡´ PPO
                log_probs, _, _ = self.agent.actor_critic.evaluate(
                    states_tensor,
                    actions_tensor
                )
            
            # Supervised Learning: Negative log likelihood loss (ìµœëŒ€ ìš°ë„ ì¶”ì •)
            # ì‚¬ëŒì´ ì¡°ì‘í•œ ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”
            # Loss = -log P(ì‹¤ì œ_ì•¡ì…˜ | ìƒíƒœ) â†’ ìµœì†Œí™”í•˜ë©´ P(ì‹¤ì œ_ì•¡ì…˜ | ìƒíƒœ) ìµœëŒ€í™”
            loss = -log_probs.mean()
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), 0.5)
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def pretrain(
        self,
        epochs: int = 100,
        batch_size: int = 64,
        save_path: str = 'pretrained_model.pth',
        log_dir: str = 'runs',
        verbose: bool = True
    ):
        """
        Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ
        
        ì‚¬ëŒì´ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒì„ ì‚¬ìš©í•˜ì—¬ ì •ì±…ì„ supervised learningìœ¼ë¡œ í•™ìŠµ
        
        Args:
            epochs: í•™ìŠµ ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            final_loss: ìµœì¢… ì†ì‹¤
        """
        print(f"\n{'='*60}")
        print("Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ì—í­ ìˆ˜: {epochs}")
        print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"ë°ì´í„° í¬ê¸°: {len(self.states)}")
        print(f"{'='*60}\n")
        
        # TensorBoard ì„¤ì •
        writer = None
        if HAS_TENSORBOARD:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_path = os.path.join(log_dir, f"teacher_forcing_{timestamp}")
            writer = SummaryWriter(log_path)
            print(f"ğŸ“Š TensorBoard ë¡œê·¸: {log_path}")
            print(f"   ì‹¤í–‰: tensorboard --logdir={log_dir}\n")
        
        best_loss = float('inf')
        
        for epoch in range(epochs):
            # í•™ìŠµ
            loss = self.train_epoch(batch_size)
            
            # ë¡œê¹…
            if verbose and (epoch + 1) % 10 == 0:
                print(f"[Epoch {epoch+1:4d}/{epochs}] Loss: {loss:.6f}")
            
            if writer:
                writer.add_scalar('Train/Loss', loss, epoch)
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if loss < best_loss:
                best_loss = loss
                self.agent.save(save_path)
                if verbose:
                    print(f"  ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {save_path} (Loss: {loss:.6f})")
        
        if writer:
            writer.close()
        
        print(f"\n{'='*60}")
        print("Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"ìµœì¢… ì†ì‹¤: {best_loss:.6f}")
        print(f"ëª¨ë¸ ì €ì¥: {save_path}")
        print(f"{'='*60}\n")
        
        return best_loss


def load_demonstrations(filepath: str):
    """
    ì €ì¥ëœ ë°ëª¨ ë°ì´í„° ë¡œë“œ
    
    Args:
        filepath: ë°ëª¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    
    Returns:
        data: ë¡œë“œëœ ë°ì´í„° (metadata, demonstrations)
    """
    with open(filepath, 'rb') as f:
        data = pickle.load(f)
    
    print(f"âœ… ë°ëª¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filepath}")
    print(f"   ì—í”¼ì†Œë“œ ìˆ˜: {data['metadata']['num_episodes']}")
    print(f"   ì´ ìŠ¤í… ìˆ˜: {data['metadata']['total_steps']}")
    print(f"   í™˜ê²½ íƒ€ì…: {data['metadata']['env_type']}")
    
    return data


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='Supervised Learning (Teacher Forcing)ì„ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ ë° ê°•í™”í•™ìŠµ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # 1ë‹¨ê³„: Supervised Learning ì‚¬ì „ í•™ìŠµë§Œ
  python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100
  
  # 2ë‹¨ê³„: ì‚¬ì „ í•™ìŠµ + ê°•í™”í•™ìŠµ fine-tuning
  python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100 --rl-steps 100000
  
  # 3ë‹¨ê³„: ê¸°ì¡´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë¡œ ê°•í™”í•™ìŠµë§Œ
  python train_with_teacher_forcing.py --load pretrained_model.pth --rl-steps 100000
        """
    )
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument('--demos', type=str, default=None,
                        help='ë°ëª¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (pickle í˜•ì‹)')
    parser.add_argument('--load', type=str, default=None,
                        help='ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì‚¬ì „ í•™ìŠµ ìƒëµ ì‹œ)')
    
    # Supervised Learning (Teacher Forcing) ì„¤ì •
    parser.add_argument('--pretrain-epochs', type=int, default=0,
                        help='Supervised Learning ì‚¬ì „ í•™ìŠµ ì—í­ ìˆ˜ (0ì´ë©´ ìƒëµ)')
    parser.add_argument('--pretrain-batch-size', type=int, default=64,
                        help='ì‚¬ì „ í•™ìŠµ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)')
    parser.add_argument('--pretrain-lr', type=float, default=3e-4,
                        help='ì‚¬ì „ í•™ìŠµ í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)')
    parser.add_argument('--pretrain-save', type=str, default='pretrained_model.pth',
                        help='ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: pretrained_model.pth)')
    
    # ê°•í™”í•™ìŠµ ì„¤ì •
    parser.add_argument('--rl-steps', type=int, default=0,
                        help='ê°•í™”í•™ìŠµ ìŠ¤í… ìˆ˜ (0ì´ë©´ ìƒëµ)')
    parser.add_argument('--rl-env-type', choices=['carracing', 'sim', 'real'],
                        default='carracing',
                        help='ê°•í™”í•™ìŠµ í™˜ê²½ íƒ€ì… (ê¸°ë³¸: carracing)')
    parser.add_argument('--rl-port', type=str, default='/dev/ttyACM0',
                        help='ì‹œë¦¬ì–¼ í¬íŠ¸ (real ëª¨ë“œ ì‚¬ìš© ì‹œ)')
    parser.add_argument('--rl-save', type=str, default='ppo_model.pth',
                        help='ê°•í™”í•™ìŠµ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ppo_model.pth)')
    
    # ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°
    parser.add_argument('--hidden-dim', type=int, default=256,
                        help='íˆë“  ë ˆì´ì–´ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--latent-dim', type=int, default=256,
                        help='TRM-PPO ì ì¬ ìƒíƒœ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--n-cycles', type=int, default=4,
                        help='TRM-PPO ì¬ê·€ ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 4)')
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None,
                        help='ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™ ì„ íƒ)')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = PPOAgent(
        state_dim=256,
        action_dim=5,  # ì´ì‚° ì•¡ì…˜
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=True,
        device=device,
        discrete_action=True,
        num_discrete_actions=5,
        use_recurrent=True
    )
    
    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    if args.load:
        if os.path.exists(args.load):
            agent.load(args.load)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {args.load}")
        else:
            print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.load}")
    
    # Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ
    if args.pretrain_epochs > 0:
        if args.demos is None:
            print("âŒ Supervised Learningì„ ì‚¬ìš©í•˜ë ¤ë©´ --demos ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        if not os.path.exists(args.demos):
            print(f"âŒ ë°ëª¨ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.demos}")
            sys.exit(1)
        
        # ë°ëª¨ ë°ì´í„° ë¡œë“œ
        demo_data = load_demonstrations(args.demos)
        demonstrations = demo_data['demonstrations']
        
        # Supervised Learning í•™ìŠµ
        trainer = TeacherForcingTrainer(
            agent=agent,
            demonstrations=demonstrations,
            device=device,
            lr=args.pretrain_lr
        )
        
        trainer.pretrain(
            epochs=args.pretrain_epochs,
            batch_size=args.pretrain_batch_size,
            save_path=args.pretrain_save,
            verbose=True
        )
    
    # ê°•í™”í•™ìŠµ fine-tuning
    if args.rl_steps > 0:
        print(f"\n{'='*60}")
        print("ê°•í™”í•™ìŠµ Fine-tuning ì‹œì‘")
        print(f"{'='*60}\n")
        
        # í™˜ê²½ ìƒì„±
        if args.rl_env_type == 'carracing':
            try:
                env = CarRacingEnvWrapper(
                    max_steps=1000,
                    use_extended_actions=True,
                    use_discrete_actions=True
                )
            except ImportError as e:
                print(f"âŒ CarRacing í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                sys.exit(1)
        elif args.rl_env_type == 'sim':
            env = RCCarSimEnv(
                max_steps=1000,
                use_extended_actions=True,
                use_discrete_actions=True
            )
        else:  # real
            try:
                from rc_car_env import RCCarEnv
                env = RCCarEnv(
                    max_steps=1000,
                    use_extended_actions=True,
                    use_discrete_actions=True
                )
            except ImportError:
                print("âŒ ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                sys.exit(1)
        
        # ê°•í™”í•™ìŠµ ì‹¤í–‰
        train_ppo(
            env=env,
            agent=agent,
            total_steps=args.rl_steps,
            max_episode_steps=1000,
            update_frequency=2048,
            update_epochs=10,
            save_frequency=10000,
            save_path=args.rl_save,
            use_tensorboard=True,
            log_dir='runs',
            mc_update_on_done=False
        )
        
        env.close()
    
    print("\nâœ… ëª¨ë“  í•™ìŠµ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

