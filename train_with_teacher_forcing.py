#!/usr/bin/env python3
"""
Supervised Learning (Teacher Forcing)ì„ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ
ì‚¬ëŒì´ ì§ì ‘ ì¡°ì‘í•œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ supervised learningìœ¼ë¡œ ì‚¬ì „ í•™ìŠµí•œ í›„ ê°•í™”í•™ìŠµìœ¼ë¡œ fine-tuning

Teacher Forcing = Supervised Learning:
- ì‚¬ëŒì´ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒì„ ì‚¬ìš©
- Maximum Likelihood Estimation (MLE)ìœ¼ë¡œ ì •ì±… í•™ìŠµ
- ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹

ì‚¬ìš©ë²•:
    # 1ë‹¨ê³„: Supervised Learning ì‚¬ì „ í•™ìŠµ
    python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100
    
    # 2ë‹¨ê³„: ê°•í™”í•™ìŠµìœ¼ë¡œ fine-tuning
    python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100 --rl-steps 100000
"""

import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pickle
import os
import sys
from datetime import datetime, timedelta
from collections import deque
import time

# í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì„í¬íŠ¸
from rc_car_sim_env import RCCarSimEnv
from car_racing_env import CarRacingEnvWrapper
from ppo_agent import PPOAgent
from train_ppo import train_ppo

# TensorBoard ì§€ì›
try:
    from torch.utils.tensorboard import SummaryWriter
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False
    print("âš ï¸  TensorBoard ë¯¸ì„¤ì¹˜ - pip install tensorboard ë¡œ ì„¤ì¹˜í•˜ë©´ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥")


class TeacherForcingTrainer:
    """
    Supervised Learning (Teacher Forcing)ì„ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ í´ë˜ìŠ¤
    ì‚¬ëŒì´ ì§ì ‘ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒìœ¼ë¡œ ì •ì±…ì„ supervised learningìœ¼ë¡œ í•™ìŠµ
    
    í•™ìŠµ ë°©ì‹:
    - Maximum Likelihood Estimation (MLE)
    - Loss = -log P(ì‹¤ì œ_ì•¡ì…˜ | ìƒíƒœ)
    - ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”
    """
    
    def __init__(
        self,
        agent: PPOAgent,
        demonstrations: list,
        device: str = 'cuda',
        lr: float = 3e-4
    ):
        """
        Args:
            agent: PPO ì—ì´ì „íŠ¸
            demonstrations: ìˆ˜ì§‘ëœ ë°ëª¨ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            device: ë””ë°”ì´ìŠ¤
            lr: í•™ìŠµë¥ 
        """
        self.agent = agent
        self.device = device
        self.demonstrations = demonstrations
        
        # ì˜µí‹°ë§ˆì´ì € (Actorë§Œ í•™ìŠµ)
        self.optimizer = optim.Adam(
            self.agent.actor_critic.parameters(),
            lr=lr
        )
        
        # ë°ì´í„° ì¤€ë¹„
        self.states, self.actions = self._prepare_data()
        
        print(f"âœ… Teacher Forcing ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ì´ ìƒíƒœ ìˆ˜: {len(self.states)}")
        print(f"   ì´ ì•¡ì…˜ ìˆ˜: {len(self.actions)}")
    
    def _prepare_data(self):
        """ë°ëª¨ ë°ì´í„°ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ë³€í™˜"""
        all_states = []
        all_actions = []
        
        for episode in self.demonstrations:
            states = episode['states']
            actions = episode['actions']
            
            # ìƒíƒœì™€ ì•¡ì…˜ì„ í…ì„œë¡œ ë³€í™˜
            for state, action in zip(states, actions):
                all_states.append(state)
                all_actions.append(action)
        
        return np.array(all_states), np.array(all_actions)
    
    def train_epoch(self, batch_size: int = 64, verbose: bool = False):
        """
        ë‹¨ì¼ ì—í­ í•™ìŠµ
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            loss: í‰ê·  ì†ì‹¤
            accuracy: ì •í™•ë„ (ì¼ì¹˜ìœ¨)
        """
        total_loss = 0.0
        num_batches = 0
        correct_predictions = 0
        total_predictions = 0
        
        # ë°ì´í„° ì…”í”Œ
        indices = np.random.permutation(len(self.states))
        num_batches_total = (len(self.states) + batch_size - 1) // batch_size
        
        for batch_idx, i in enumerate(range(0, len(self.states), batch_size)):
            batch_indices = indices[i:i+batch_size]
            batch_states = self.states[batch_indices]
            batch_actions = self.actions[batch_indices]
            
            # í…ì„œë¡œ ë³€í™˜
            states_tensor = torch.FloatTensor(batch_states).to(self.device)
            # ì•¡ì…˜ì€ ë‚˜ì¤‘ì— discrete/continuousì— ë”°ë¼ ë³€í™˜
            actions_tensor = torch.from_numpy(batch_actions).to(self.device)
            
            # TRM-PPO ëª¨ë“œ í™•ì¸
            use_recurrent = getattr(self.agent, 'use_recurrent', False)
            
            # ì´ì‚° ì•¡ì…˜ ì²˜ë¦¬
            if self.agent.actor_critic.discrete_action:
                # ì´ì‚° ì•¡ì…˜: LongTensorë¡œ ë³€í™˜
                actions_tensor = actions_tensor.long()
                if actions_tensor.dim() == 1:
                    actions_tensor = actions_tensor.unsqueeze(-1)
            else:
                # ì—°ì† ì•¡ì…˜: FloatTensorë¡œ ë³€í™˜
                actions_tensor = actions_tensor.float()
                if actions_tensor.dim() == 1:
                    actions_tensor = actions_tensor.unsqueeze(-1)
            
            # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¡œ ì•¡ì…˜ í™•ë¥  ê³„ì‚°
            if use_recurrent:
                # TRM-PPO: evaluate ì‚¬ìš©
                log_probs, _, _ = self.agent.actor_critic.evaluate(
                    states_tensor,
                    actions_tensor,
                    n_cycles=self.agent.n_cycles
                )
            else:
                # ê¸°ì¡´ PPO
                log_probs, _, _ = self.agent.actor_critic.evaluate(
                    states_tensor,
                    actions_tensor
                )
            
            # Supervised Learning: Negative log likelihood loss (ìµœëŒ€ ìš°ë„ ì¶”ì •)
            # ì‚¬ëŒì´ ì¡°ì‘í•œ ì‹¤ì œ ì•¡ì…˜ì˜ ë¡œê·¸ í™•ë¥ ì„ ìµœëŒ€í™”
            # Loss = -log P(ì‹¤ì œ_ì•¡ì…˜ | ìƒíƒœ) â†’ ìµœì†Œí™”í•˜ë©´ P(ì‹¤ì œ_ì•¡ì…˜ | ìƒíƒœ) ìµœëŒ€í™”
            loss = -log_probs.mean()
            
            # ì •í™•ë„ ê³„ì‚° (ì˜ˆì¸¡ ì•¡ì…˜ê³¼ ì‹¤ì œ ì•¡ì…˜ ë¹„êµ)
            with torch.no_grad():
                try:
                    if use_recurrent:
                        # RecurrentActorCritic.get_actionì€ 4ê°œ ê°’ì„ ë°˜í™˜
                        predicted_actions, _, _, _ = self.agent.actor_critic.get_action(
                            states_tensor, deterministic=True
                        )
                    else:
                        # ActorCritic.get_actionì€ 3ê°œ ê°’ì„ ë°˜í™˜
                        predicted_actions, _, _ = self.agent.actor_critic.get_action(
                            states_tensor, deterministic=True
                        )
                    
                    if self.agent.actor_critic.discrete_action:
                        predicted_actions = predicted_actions.cpu().numpy().flatten()
                        actual_actions = batch_actions.flatten()
                        correct_predictions += np.sum(predicted_actions == actual_actions)
                        total_predictions += len(actual_actions)
                except Exception:
                    # ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
                    pass
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.actor_critic.parameters(), 0.5)
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # ë°°ì¹˜ ì§„í–‰ ìƒí™© ì¶œë ¥ (verbose ëª¨ë“œ)
            if verbose and (batch_idx + 1) % max(1, num_batches_total // 10) == 0:
                current_loss = total_loss / num_batches
                current_acc = correct_predictions / total_predictions if total_predictions > 0 else 0
                print(f"  ë°°ì¹˜ {batch_idx+1}/{num_batches_total} | "
                      f"Loss: {current_loss:.6f} | "
                      f"Acc: {current_acc:.1%}", end='\r', flush=True)
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
        
        if verbose:
            print()  # ì¤„ë°”ê¿ˆ
        
        return avg_loss, accuracy
    
    def pretrain(
        self,
        epochs: int = 100,
        batch_size: int = 64,
        save_path: str = 'pretrained_model.pth',
        log_dir: str = 'runs',
        verbose: bool = True
    ):
        """
        Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ
        
        ì‚¬ëŒì´ ì¡°ì‘í•œ (ìƒíƒœ, ì•¡ì…˜) ìŒì„ ì‚¬ìš©í•˜ì—¬ ì •ì±…ì„ supervised learningìœ¼ë¡œ í•™ìŠµ
        
        Args:
            epochs: í•™ìŠµ ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
        Returns:
            final_loss: ìµœì¢… ì†ì‹¤
        """
        print(f"\n{'='*60}")
        print("Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ì—í­ ìˆ˜: {epochs}")
        print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"ë°ì´í„° í¬ê¸°: {len(self.states):,}ê°œ ìƒ˜í”Œ")
        print(f"ì´ ë°°ì¹˜ ìˆ˜: {(len(self.states) + batch_size - 1) // batch_size}ê°œ/ì—í­")
        print(f"{'='*60}\n")
        
        # TensorBoard ì„¤ì •
        writer = None
        if HAS_TENSORBOARD:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_path = os.path.join(log_dir, f"teacher_forcing_{timestamp}")
            writer = SummaryWriter(log_path)
            print(f"ğŸ“Š TensorBoard ë¡œê·¸: {log_path}")
            print(f"   ì‹¤í–‰: tensorboard --logdir={log_dir}\n")
        
        # ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        epoch_start_time = time.time()
        
        best_loss = float('inf')
        best_accuracy = 0.0
        
        for epoch in range(epochs):
            # í•™ìŠµ
            loss, accuracy = self.train_epoch(batch_size, verbose=verbose)
            
            # ì‹œê°„ ê³„ì‚°
            epoch_time = time.time() - epoch_start_time
            epoch_start_time = time.time()
            elapsed_total = time.time() - start_time
            
            if epoch > 0:
                avg_epoch_time = elapsed_total / (epoch + 1)
                remaining_epochs = epochs - epoch - 1
                eta_seconds = avg_epoch_time * remaining_epochs
                eta_str = str(timedelta(seconds=int(eta_seconds)))
            else:
                eta_str = "ê³„ì‚° ì¤‘..."
            
            # ë¡œê¹… (ë§¤ ì—í¬í¬ë§ˆë‹¤ ì¶œë ¥)
            if verbose:
                epoch_progress = (epoch + 1) / epochs * 100
                print(f"[ì—í¬í¬ {epoch+1}/{epochs}] ({epoch_progress:.1f}%) | "
                      f"Loss: {loss:.6f} | "
                      f"Accuracy: {accuracy:.2%} | "
                      f"ì‹œê°„: {epoch_time:.1f}ì´ˆ | "
                      f"ì˜ˆìƒ ë‚¨ì€: {eta_str}")
            
            if writer:
                writer.add_scalar('Train/Loss', loss, epoch)
                writer.add_scalar('Train/Accuracy', accuracy, epoch)
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if loss < best_loss:
                best_loss = loss
                best_accuracy = accuracy
                self.agent.save(save_path)
                if verbose:
                    print(f"  ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {save_path} (Loss: {loss:.6f}, Acc: {accuracy:.2%})")
        
        if writer:
            writer.close()
        
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print("âœ… Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"ìµœì¢… ì†ì‹¤: {best_loss:.6f}")
        print(f"ìµœì¢… ì •í™•ë„: {best_accuracy:.2%}")
        print(f"ì´ í•™ìŠµ ì‹œê°„: {str(timedelta(seconds=int(total_time)))}")
        print(f"í‰ê·  ì—í¬í¬ ì‹œê°„: {total_time/epochs:.1f}ì´ˆ")
        print(f"ëª¨ë¸ ì €ì¥: {save_path}")
        print(f"{'='*60}\n")
        
        return best_loss


def load_demonstrations(filepath: str):
    """
    ì €ì¥ëœ ë°ëª¨ ë°ì´í„° ë¡œë“œ
    
    Args:
        filepath: ë°ëª¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    
    Returns:
        data: ë¡œë“œëœ ë°ì´í„° (metadata, demonstrations)
    """
    with open(filepath, 'rb') as f:
        data = pickle.load(f)
    
    print(f"âœ… ë°ëª¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {filepath}")
    print(f"   ì—í”¼ì†Œë“œ ìˆ˜: {data['metadata']['num_episodes']}")
    print(f"   ì´ ìŠ¤í… ìˆ˜: {data['metadata']['total_steps']}")
    print(f"   í™˜ê²½ íƒ€ì…: {data['metadata']['env_type']}")
    
    return data


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='Supervised Learning (Teacher Forcing)ì„ ì‚¬ìš©í•œ ì‚¬ì „ í•™ìŠµ ë° ê°•í™”í•™ìŠµ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # 1ë‹¨ê³„: Supervised Learning ì‚¬ì „ í•™ìŠµë§Œ
  python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100
  
  # 2ë‹¨ê³„: ì‚¬ì „ í•™ìŠµ + ê°•í™”í•™ìŠµ fine-tuning
  python train_with_teacher_forcing.py --demos human_demos.pkl --pretrain-epochs 100 --rl-steps 100000
  
  # 3ë‹¨ê³„: ê¸°ì¡´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë¡œ ê°•í™”í•™ìŠµë§Œ
  python train_with_teacher_forcing.py --load pretrained_model.pth --rl-steps 100000
        """
    )
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument('--demos', type=str, default=None,
                        help='ë°ëª¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (pickle í˜•ì‹)')
    parser.add_argument('--load', type=str, default=None,
                        help='ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì‚¬ì „ í•™ìŠµ ìƒëµ ì‹œ)')
    
    # Supervised Learning (Teacher Forcing) ì„¤ì •
    parser.add_argument('--pretrain-epochs', type=int, default=0,
                        help='Supervised Learning ì‚¬ì „ í•™ìŠµ ì—í­ ìˆ˜ (0ì´ë©´ ìƒëµ)')
    parser.add_argument('--pretrain-batch-size', type=int, default=64,
                        help='ì‚¬ì „ í•™ìŠµ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 64)')
    parser.add_argument('--pretrain-lr', type=float, default=3e-4,
                        help='ì‚¬ì „ í•™ìŠµ í•™ìŠµë¥  (ê¸°ë³¸: 3e-4)')
    parser.add_argument('--pretrain-save', type=str, default='pretrained_model.pth',
                        help='ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: pretrained_model.pth)')
    
    # ê°•í™”í•™ìŠµ ì„¤ì •
    parser.add_argument('--rl-steps', type=int, default=0,
                        help='ê°•í™”í•™ìŠµ ìŠ¤í… ìˆ˜ (0ì´ë©´ ìƒëµ)')
    parser.add_argument('--rl-env-type', choices=['carracing', 'sim', 'real'],
                        default='carracing',
                        help='ê°•í™”í•™ìŠµ í™˜ê²½ íƒ€ì… (ê¸°ë³¸: carracing)')
    parser.add_argument('--rl-port', type=str, default='/dev/ttyACM0',
                        help='ì‹œë¦¬ì–¼ í¬íŠ¸ (real ëª¨ë“œ ì‚¬ìš© ì‹œ)')
    parser.add_argument('--rl-save', type=str, default='ppo_model.pth',
                        help='ê°•í™”í•™ìŠµ ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ppo_model.pth)')
    
    # ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°
    parser.add_argument('--hidden-dim', type=int, default=256,
                        help='íˆë“  ë ˆì´ì–´ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--latent-dim', type=int, default=256,
                        help='TRM-PPO ì ì¬ ìƒíƒœ ì°¨ì› (ê¸°ë³¸: 256)')
    parser.add_argument('--n-cycles', type=int, default=4,
                        help='TRM-PPO ì¬ê·€ ì¶”ë¡  ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 4)')
    
    # ë””ë°”ì´ìŠ¤
    parser.add_argument('--device', type=str, default=None,
                        help='ë””ë°”ì´ìŠ¤ (cuda/cpu, ê¸°ë³¸: ìë™ ì„ íƒ)')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = PPOAgent(
        state_dim=256,
        action_dim=5,  # ì´ì‚° ì•¡ì…˜
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
        n_cycles=args.n_cycles,
        carry_latent=True,
        device=device,
        discrete_action=True,
        num_discrete_actions=5,
        use_recurrent=True
    )
    
    # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    if args.load:
        if os.path.exists(args.load):
            agent.load(args.load)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {args.load}")
        else:
            print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.load}")
    
    # Supervised Learning (Teacher Forcing) ì‚¬ì „ í•™ìŠµ
    if args.pretrain_epochs > 0:
        if args.demos is None:
            print("âŒ Supervised Learningì„ ì‚¬ìš©í•˜ë ¤ë©´ --demos ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        if not os.path.exists(args.demos):
            print(f"âŒ ë°ëª¨ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.demos}")
            sys.exit(1)
        
        # ë°ëª¨ ë°ì´í„° ë¡œë“œ
        demo_data = load_demonstrations(args.demos)
        demonstrations = demo_data['demonstrations']
        
        # Supervised Learning í•™ìŠµ
        trainer = TeacherForcingTrainer(
            agent=agent,
            demonstrations=demonstrations,
            device=device,
            lr=args.pretrain_lr
        )
        
        trainer.pretrain(
            epochs=args.pretrain_epochs,
            batch_size=args.pretrain_batch_size,
            save_path=args.pretrain_save,
            verbose=True
        )
    
    # ê°•í™”í•™ìŠµ fine-tuning
    if args.rl_steps > 0:
        print(f"\n{'='*60}")
        print("ê°•í™”í•™ìŠµ Fine-tuning ì‹œì‘")
        print(f"{'='*60}\n")
        
        # í™˜ê²½ ìƒì„±
        if args.rl_env_type == 'carracing':
            try:
                env = CarRacingEnvWrapper(
                    max_steps=1000,
                    use_extended_actions=True,
                    use_discrete_actions=True
                )
            except ImportError as e:
                print(f"âŒ CarRacing í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                sys.exit(1)
        elif args.rl_env_type == 'sim':
            env = RCCarSimEnv(
                max_steps=1000,
                use_extended_actions=True,
                use_discrete_actions=True
            )
        else:  # real
            try:
                from rc_car_env import RCCarEnv
                env = RCCarEnv(
                    max_steps=1000,
                    use_extended_actions=True,
                    use_discrete_actions=True
                )
            except ImportError:
                print("âŒ ì‹¤ì œ í•˜ë“œì›¨ì–´ í™˜ê²½ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                sys.exit(1)
        
        # ê°•í™”í•™ìŠµ ì‹¤í–‰
        train_ppo(
            env=env,
            agent=agent,
            total_steps=args.rl_steps,
            max_episode_steps=1000,
            update_frequency=2048,
            update_epochs=10,
            save_frequency=10000,
            save_path=args.rl_save,
            use_tensorboard=True,
            log_dir='runs',
            mc_update_on_done=False
        )
        
        env.close()
    
    print("\nâœ… ëª¨ë“  í•™ìŠµ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

